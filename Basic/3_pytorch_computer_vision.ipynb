{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Reference : https://github.com/mrdbourke/pytorch-deep-learning/blob/main/03_pytorch_computer_vision.ipynb\n",
        "\n",
        "\n",
        "https://www.learnpytorch.io/03_pytorch_computer_vision/"
      ],
      "metadata": {
        "id": "BENJUmdXJXz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch module\n",
        "\n",
        "# torchvision:\t\n",
        "Contains datasets, model architectures and image transformations often used for computer vision problems.\n",
        "\n",
        "\n",
        "# torchvision.datasets:\t\n",
        "Here you'll find many example computer vision datasets for a range of problems from image classification, object detection, image captioning, video classification and more. It also contains a series of base classes for making custom datasets.\n",
        "# torchvision.models:\t\n",
        "This module contains well-performing and commonly used computer vision model architectures implemented in PyTorch, you can use these with your own problems.\n",
        "# torchvision.transforms:\t\n",
        "Often images need to be transformed (turned into numbers/processed/augmented) before being used with a model, common image transformations are found here.\n",
        "# torch.utils.data.Dataset:\t\n",
        "Base dataset class for PyTorch.\n",
        "#torch.utils.data.DataLoader:\t\n",
        "Creates a Python iteralbe over a dataset (created with torch.utils.data.Dataset)."
      ],
      "metadata": {
        "id": "08MG6YEU7USY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "metadata": {
        "id": "iUnsF_KD72GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Getting a dataset\n",
        "To begin working on a computer vision problem, let's get a computer vision dataset.\n",
        "\n",
        "We're going to start with FashionMNIST.\n",
        "\n",
        "MNIST stands for Modified National Institute of Standards and Technology.\n",
        "\n",
        "The original MNIST dataset contains thousands of examples of handwritten digits (from 0 to 9) and was used to build computer vision models to identify numbers for postal services.\n",
        "\n",
        "FashionMNIST, made by Zalando Research, is a similar setup.\n",
        "\n",
        "Except it contains grayscale images of 10 different kinds of clothing."
      ],
      "metadata": {
        "id": "c3u8fZ7o8Sgb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* root: str - which folder do you want to download the data to?\n",
        "* train: Bool - do you want the training or test split?\n",
        "* download: Bool - should the data be downloaded?\n",
        "* transform: torchvision.transforms - what transformations would you like to do on the data?\n",
        "* target_transform - you can transform the targets (labels) if you like too"
      ],
      "metadata": {
        "id": "SwRT6iAB80Bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets.fakedata import transforms\n",
        "#setup traning data\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root='dahwin',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform = ToTensor(),\n",
        "    target_transform=None\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root='dahwin',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ],
      "metadata": {
        "id": "FpfJAYgh8lV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see first training sample\n",
        "image,label = train_data[0]\n",
        "# image,label\n"
      ],
      "metadata": {
        "id": "NTt1YWdC95vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image.shape"
      ],
      "metadata": {
        "id": "ay4jQlWv-fAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data.data),len(train_data.targets),len(test_data.data),len(test_data.targets)"
      ],
      "metadata": {
        "id": "c7HPpd_x-e9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names= train_data.classes"
      ],
      "metadata": {
        "id": "dYK6_04c-e5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.2 Visualizing our data"
      ],
      "metadata": {
        "id": "qg0CVBIv-e1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Image shape: {image.shape}\")\n",
        "plt.imshow(image.squeeze()) # image shape is [1, 28, 28] (colour channels, height, width)\n",
        "plt.title(label);"
      ],
      "metadata": {
        "id": "-sv_gdan-exx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(image.squeeze(),cmap='gray')"
      ],
      "metadata": {
        "id": "3m5mHYyU-et5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ploting more images\n",
        "torch.manual_seed(42)\n",
        "fig = plt.figure(figsize=(9,30))\n",
        "rows,cols = 10,4\n",
        "for i in range(1,rows*cols+1):\n",
        "  random_idx = torch.randint(0,len(train_data),size=[1]).item()\n",
        "  img,label = train_data[random_idx]\n",
        "  fig.add_subplot(rows,cols,i)\n",
        "  plt.imshow(img.squeeze(),cmap='gray')\n",
        "  plt.title(class_names[label])\n",
        "  plt.axis(False)"
      ],
      "metadata": {
        "id": "v3tlHs-V-eqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Prepare DataLoader\n",
        "Now we've got a dataset ready to go.\n",
        "\n",
        "The next step is to prepare it with a torch.utils.data.DataLoader or DataLoader for short.\n",
        "\n",
        "The DataLoader does what you think it might do.\n",
        "\n",
        "It helps load data into a model.\n",
        "\n",
        "For training and for inference.\n",
        "\n",
        "It turns a large Dataset into a Python iterable of smaller chunks.\n",
        "\n",
        "These smaller chunks are called batches or mini-batches and can be set by the batch_size parameter.\n",
        "\n",
        "Why do this?\n",
        "\n",
        "Because it's more computationally efficient.\n",
        "\n",
        "In an ideal world you could do the forward pass and backward pass across all of your data at once.\n",
        "\n",
        "But once you start using really large datasets, unless you've got infinite computing power, it's easier to break them up into batches.\n",
        "\n",
        "It also gives your model more opportunities to improve.\n",
        "\n",
        "With mini-batches (small portions of the data), gradient descent is performed more often per epoch (once per mini-batch rather than once per epoch).\n",
        "\n",
        "What's a good batch size?\n",
        "\n",
        "32 is a good place to start for a fair amount of problems.\n",
        "\n",
        "But since this is a value you can set (a hyperparameter) you can try all different kinds of values, though generally powers of 2 are used most often (e.g. 32, 64, 128, 256, 512)."
      ],
      "metadata": {
        "id": "qcRwIVbSHU0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "batch_size = 32\n",
        "train_dataloader = DataLoader( train_data,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=True\n",
        "    \n",
        ")\n",
        "test_dataloader = DataLoader(test_data,\n",
        "                             batch_size=batch_size,\n",
        "                             shuffle=False)\n",
        "# Let's check out what we've created\n",
        "print(f\"Dataloaders: {train_dataloader, test_dataloader}\") \n",
        "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {batch_size}\")\n",
        "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {batch_size}\")"
      ],
      "metadata": {
        "id": "mTI3MNLM-emC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features_batch ,train_labels_batch = next(iter(train_dataloader))\n",
        "train_features_batch.shape,train_labels_batch.shape"
      ],
      "metadata": {
        "id": "xe2GroZx-eip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "random_idx = torch.randint(0,len(train_features_batch),size=[1]).item()\n",
        "img,label = train_features_batch[random_idx],train_labels_batch[random_idx]\n",
        "plt.imshow(img.squeeze())\n",
        "plt.title(class_names[label])\n",
        "plt.axis('Off')\n",
        "print(f'Image size: {img.shape}')\n",
        "print(f'Label:{label},label size: {label.shape}')"
      ],
      "metadata": {
        "id": "baKLmFqR-efi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Model 0: Build a baseline model\n",
        "Data loaded and prepared!\n",
        "\n",
        "Time to build a baseline model by subclassing nn.Module.\n",
        "\n",
        "A baseline model is one of the simplest models you can imagine.\n",
        "\n",
        "You use the baseline as a starting point and try to improve upon it with subsequent, more complicated models.\n",
        "\n",
        "Our baseline will consist of two nn.Linear() layers.\n",
        "\n",
        "We've done this in a previous section but there's going to one slight difference.\n",
        "\n",
        "Because we're working with image data, we're going to use a different layer to start things off.\n",
        "\n",
        "And that's the nn.Flatten() layer.\n",
        "\n",
        "nn.Flatten() compresses the dimensions of a tensor into a single vector.\n",
        "\n",
        "This is easier to understand when you see"
      ],
      "metadata": {
        "id": "m_nZrjXOCqwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a flattttten layer\n",
        "flatten_model = nn.Flatten()\n",
        "# Get a single sample\n",
        "x = train_features_batch[0]\n",
        "\n",
        "output = flatten_model(x)\n",
        "# Print out what happened\n",
        "print(f\"Shape before flattening: {x.shape} -> [color_channels, height, width]\")\n",
        "print(f\"Shape after flattening: {output.shape} -> [color_channels, height*width]\")"
      ],
      "metadata": {
        "id": "U9NeJ-kP-ecR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "class DahwinFashionMNISTModelV0(nn.Module):\n",
        "  def __init__(self,input_shape:int,hidden_units:int,output_shape:int):\n",
        "      super().__init__()\n",
        "      self.layer_stack = nn.Sequential(\n",
        "          nn.Flatten(),\n",
        "          nn.Linear(in_features=input_shape,out_features=hidden_units),\n",
        "          nn.Linear(in_features=hidden_units,out_features=output_shape))\n",
        "  def forward(self,x):\n",
        "    return self.layer_stack(x)\n",
        "   "
      ],
      "metadata": {
        "id": "Vmvft68p-eY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "model_0 = DahwinFashionMNISTModelV0(input_shape=784,\n",
        "                                    hidden_units=10,output_shape=len(class_names))\n",
        "model_0"
      ],
      "metadata": {
        "id": "o5jccr-o-eVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# setup loss,optimizer and evaluation metrics \n",
        "since we're working on a classification problem,let's bring in our helper_fuctions.py script and \n",
        "subsquentlyl the accuracy_fn() we defined in notebook"
      ],
      "metadata": {
        "id": "ras92S6F-eR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "if Path('helper_fuctioins.py').is_file():\n",
        "  print('already exists')\n",
        "else:\n",
        "  print('Downloading')\n",
        "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "  with open(\"helper_functions.py\", \"wb\") as f:\n",
        "    f.write(request.content)"
      ],
      "metadata": {
        "id": "kNpVw1Og-eOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import accuracy_fn\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params= model_0.parameters(),lr=0.1)"
      ],
      "metadata": {
        "id": "qqEPJyuH-eK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "HURo9E_zB3AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.2 Creating a function to time our experiments\n",
        "Loss function and optimizer ready!\n",
        "\n",
        "It's time to start training a model.\n",
        "\n",
        "But how about we do a little experiment while we train.\n",
        "\n",
        "I mean, let's make a timing function to measure the time it takes our model to train on CPU versus using a GPU.\n",
        "\n",
        "We'll train this model on the CPU but the next one on the GPU and see what happens.\n",
        "\n",
        "Our timing function will import the timeit.default_timer() function from the Python timeit module. "
      ],
      "metadata": {
        "id": "5j8H2rlbrjYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.3 Creating a training loop and training a model on batches of data\n",
        "Beautiful!\n",
        "\n",
        "Looks like we've got all of the pieces of the puzzle ready to go, a timer, a loss function, an optimizer, a model and most importantly, some data.\n",
        "\n",
        "Let's now create a training loop and a testing loop to train and evaluate our model.\n",
        "\n",
        "We'll be using the same steps as the previous notebook(s), though since our data is now in batch form, we'll add another loop to loop through our data batches.\n",
        "\n",
        "Our data batches are contained within our DataLoaders, train_dataloader and test_dataloader for the training and test data splits respectively.\n",
        "\n",
        "A batch is BATCH_SIZE samples of X (features) and y (labels), since we're using BATCH_SIZE=32, our batches have 32 samples of images and targets.\n",
        "\n",
        "And since we're computing on batches of data, our loss and evaluation metrics will be calculated per batch rather than across the whole dataset.\n",
        "\n",
        "This means we'll have to divide our loss and accuracy values by the number of batches in each dataset's respective dataloader.\n",
        "\n",
        "Let's step through it:\n",
        "\n",
        "Loop through epochs.\n",
        "Loop through training batches, perform training steps, calculate the train loss per batch.\n",
        "Loop through testing batches, perform testing steps, calculate the test loss per batch.\n",
        "Print out what's happening.\n",
        "Time it all (for fun).\n",
        "A fair few steps but..."
      ],
      "metadata": {
        "id": "SOyNwBy55UdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "torch.manual_seed(42)\n",
        "start = time.time()\n",
        "epochs =3\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"Epoch:{epoch}\\n....\")\n",
        "  train_loss = 0\n",
        "  # Add a loop to loop through training batches\n",
        "  for batch ,(x,y) in enumerate(train_dataloader):\n",
        "    model_0.train()\n",
        "    y_pre = model_0(x)\n",
        "    loss = loss_fn(y_pre,y)\n",
        "    # calculate loss (per batch)\n",
        "    train_loss += loss # accumulatively add up the loss per epoch\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch % 400 ==0:\n",
        "      print(f'LOoked at {batch*len(x)}/{len(train_dataloader.dataset)} samples')\n",
        "  # Divide total train loss by lenth of train dataloader\n",
        "  train_loss /= len(train_dataloader)\n",
        "  ### testing \n",
        "  test_loss,test_acc = 0,0\n",
        "  model_0.eval()\n",
        "  with torch.inference_mode():\n",
        "    for x_test,y_test in test_dataloader:\n",
        "      test_pre = model_0(x_test)\n",
        "      # calculate loss (accumulatively)\n",
        "      test_loss = loss_fn(test_pre,y_test)\n",
        "      test_acc += accuracy_fn(y_test,test_pre.argmax(dim=1))\n",
        "    # calculate the loss avarage per batch\n",
        "    test_loss /= len(test_dataloader)\n",
        "    # calculate the test acc avarage per batch\n",
        "    test_acc /= len(test_dataloader)\n",
        "  print(f\"\\n Train loss:{train_loss:.4f}| test loss: {test_loss:.4f},Test acc: {test_acc:.4f}\")\n",
        "  # calculat training time\n",
        "  end = time.time()\n",
        "  time0 = end-start\n",
        "  print(time0)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n5GcuW3V-eHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Make predictions and get Model 0 results"
      ],
      "metadata": {
        "id": "s2h4Pt1GFaTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "def eval_model(model:torch.nn.Module,\n",
        "         data_loader:torch.utils.data.DataLoader,\n",
        "         loss_fn:torch.nn.Module,\n",
        "         accuracy_fn):\n",
        "    loss ,acc = 0 , 0\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "      for x,y in data_loader:\n",
        "       \n",
        "        y_pre = model(x)\n",
        "\n",
        "        #Accumulate the loss and acc values per batch\n",
        "        loss += loss_fn(y_pre,y)\n",
        "        acc += accuracy_fn(y,y_pre.argmax(dim=1))\n",
        "    # scale loss and acc to find the averagae loss/acc per batch\n",
        "      loss /= len(data_loader)\n",
        "      acc /= len(data_loader)\n",
        "    return {\n",
        "        'model_name': model.__class__.__name__,\n",
        "        'model_loss': loss.item(),\n",
        "        'model_acc': acc\n",
        "    }\n",
        "# calculate model 0 results on test dataset\n",
        "model_0_results = eval_model(\n",
        "    model=model_0,\n",
        "    data_loader=test_dataloader,\n",
        "    loss_fn=loss_fn,\n",
        "    accuracy_fn=accuracy_fn,\n",
        "    \n",
        ")\n",
        "model_0_results"
      ],
      "metadata": {
        "id": "l2dPqOmM-eAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Setup device agnostic-code (for using a GPU if there is one)"
      ],
      "metadata": {
        "id": "MK20khqsCxy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device agnostic code\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "fZTY4GnR-d8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model1: Building a better model with non-linearity\n"
      ],
      "metadata": {
        "id": "aIyrjpUdDLGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DahwinFashionMNISTModelV1(nn.Module):\n",
        "  def __init__(self,input_shape:int,hidden_units:int,output_shape:int):\n",
        "    super().__init__()\n",
        "    self.layer_stack = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=input_shape,out_features=hidden_units),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=hidden_units,out_features=output_shape),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "  def forward(self,x:torch.Tensor):\n",
        "  \n",
        "   return self.layer_stack(x)"
      ],
      "metadata": {
        "id": "7tgTl_xn-d4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "model_1 = DahwinFashionMNISTModelV1(input_shape=784,hidden_units=10,output_shape=len(class_names)).to(device)\n",
        "next(model_1.parameters()).device"
      ],
      "metadata": {
        "id": "BtPw6CwKG3NF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.1 Setup los,optimiiiizer and evaluation metrics\n",
        "from helper_functions import accuracy_fn\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_1.parameters(),lr=0.1)"
      ],
      "metadata": {
        "id": "v_rpmynIcRGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.2 Functionizing traning and evaluation loops\n",
        "Let's create a fuction for \n",
        "* training loop \n",
        "* testing loop"
      ],
      "metadata": {
        "id": "uAM-Y6xKdmr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               accuracy_fn,\n",
        "               device: torch.device = device):\n",
        "    model.train()\n",
        "    train_loss, train_acc = 0, 0\n",
        "    for batch, (x, y) in enumerate(data_loader):\n",
        "        # Send data to GPU\n",
        "        X, y = x.to(device), y.to(device)\n",
        "\n",
        "        # 1. Forward pass\n",
        "        y_pred = model(X)\n",
        "\n",
        "        # 2. Calculate loss\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss\n",
        "        train_acc += accuracy_fn(y_true=y,\n",
        "                                 y_pred=y_pred.argmax(dim=1)) # Go from logits -> pred labels\n",
        "\n",
        "        # 3. Optimizer zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. Loss backward\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate loss and accuracy per epoch and print out what's happening\n",
        "    train_loss /= len(data_loader)\n",
        "    train_acc /= len(data_loader)\n",
        "    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n",
        "\n",
        "def test_step(\n",
        "              model: torch.nn.Module,\n",
        "              data_loader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              accuracy_fn,\n",
        "              device: torch.device = device):\n",
        "    test_loss, test_acc = 0, 0\n",
        "    model.eval() # put model in eval mode\n",
        "    # Turn on inference context manager\n",
        "    with torch.inference_mode(): \n",
        "        for x, y in data_loader:\n",
        "            # Send data to GPU\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            \n",
        "            # 1. Forward pass\n",
        "            test_pred = model(x)\n",
        "            \n",
        "            # 2. Calculate loss and accuracy\n",
        "            test_loss += loss_fn(test_pred, y)\n",
        "            test_acc += accuracy_fn(y_true=y,\n",
        "                y_pred=test_pred.argmax(dim=1) # Go from logits -> pred labels\n",
        "            )\n",
        "        \n",
        "        # Adjust metrics and print out\n",
        "        test_loss /= len(data_loader)\n",
        "        test_acc /= len(data_loader)\n",
        "        print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")"
      ],
      "metadata": {
        "id": "ZK01tlCrdyJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "start = time.time()\n",
        "epochs = 3\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"Epoch:{epoch}\\n........\")\n",
        "  train_step(model_1,train_dataloader,loss_fn,optimizer,accuracy_fn,device)\n",
        "  test_step(model_1,test_dataloader,loss_fn,accuracy_fn,device)\n",
        "end = time.time()\n",
        "time1 = end-start\n",
        "print(time1)"
      ],
      "metadata": {
        "id": "4U_yW5FtjqHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Note: The training time on CUDA vs CPU will depend largely on the quality of the CPU/GPU you're using. Read on for a more explained answer.\n",
        "\n",
        "Question: \"I used a a GPU but my model didn't train faster, why might that be?\"\n",
        "\n",
        "Answer: Well, one reason could be because your dataset and model are both so small (like the dataset and model we're working with) the benefits of using a GPU are outweighed by the time it actually takes to transfer the data there.\n",
        "\n",
        "There's a small bottleneck between copying data from the CPU memory (default) to the GPU memory.\n",
        "\n",
        "So for smaller models and datasets, the CPU might actually be the optimal place to compute on.\n",
        "\n",
        "But for larger datasets and models, the speed of computing the GPU can offer usually far outweighs the cost of getting the data there.\n",
        "\n",
        "However, this is largely dependant on the hardware you're using. With practice, you will get used to where the best place to train your models i"
      ],
      "metadata": {
        "id": "U8an_JSlm8yG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move values to device\n",
        "torch.manual_seed(42)\n",
        "def eval_model(model: torch.nn.Module, \n",
        "               data_loader: torch.utils.data.DataLoader, \n",
        "               loss_fn: torch.nn.Module, \n",
        "               accuracy_fn, \n",
        "               device: torch.device = device):\n",
        "    \"\"\"Evaluates a given model on a given dataset.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
        "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
        "        loss_fn (torch.nn.Module): The loss function of model.\n",
        "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
        "        device (str, optional): Target device to compute on. Defaults to device.\n",
        "\n",
        "    Returns:\n",
        "        (dict): Results of model making predictions on data_loader.\n",
        "    \"\"\"\n",
        "    loss, acc = 0, 0\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in data_loader:\n",
        "            # Send data to the target device\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_pred = model(X)\n",
        "            loss += loss_fn(y_pred, y)\n",
        "            acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
        "        \n",
        "        # Scale loss and acc\n",
        "        loss /= len(data_loader)\n",
        "        acc /= len(data_loader)\n",
        "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
        "            \"model_loss\": loss.item(),\n",
        "            \"model_acc\": acc}\n",
        "\n",
        "# Calculate model 1 results with device-agnostic code \n",
        "model_1_results = eval_model(\n",
        "    model=model_1,\n",
        "    data_loader=test_dataloader,\n",
        "    loss_fn=loss_fn,\n",
        "    accuracy_fn=accuracy_fn,\n",
        "    \n",
        ")\n",
        "model_1_results"
      ],
      "metadata": {
        "id": "ASaqNzjeDHb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Model 2 : Building a Convolutional Neural Network (CNN)"
      ],
      "metadata": {
        "id": "lzma2KjWF4W_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules import Conv2d\n",
        "class DahwinFashionMNISTModelCNNV2(nn.Module):\n",
        "  def __init__(self,input_shape:int,hidden_layers:int,output_shape:int):\n",
        "    super().__init__()\n",
        "    self.conv_block = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=input_shape,out_channels=hidden_layers,kernel_size=3,stride=1,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_layers,out_channels=hidden_layers,kernel_size=3,stride=1,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "    self.conv_block_2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=hidden_layers,out_channels=hidden_layers,kernel_size=3,stride=1,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_layers,out_channels=hidden_layers,kernel_size=3,stride=1,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=hidden_layers*7*7,out_features=output_shape)\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    x = self.conv_block(x)\n",
        "    # print(f\"output shape of conv_block1:{x.shape}\")\n",
        "    x = self.conv_block_2(x)\n",
        "    # print(f\"output shape of conv_block2:{x.shape}\")\n",
        "\n",
        "    x = self.classifier(x)\n",
        "    # print(f\"output shape of classifier:{x.shape}\")\n",
        "    return x\n",
        "  "
      ],
      "metadata": {
        "id": "-0isKQZbFpti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image.shape"
      ],
      "metadata": {
        "id": "4xFrePJZlHhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "model_2 = DahwinFashionMNISTModelCNNV2(input_shape=1,hidden_layers=10,output_shape=len(class_names)).to(device)\n",
        "model_2"
      ],
      "metadata": {
        "id": "ocIU_TvwkmtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(image.squeeze())"
      ],
      "metadata": {
        "id": "v1n1IYK4pBt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_image_tensor =torch.randn(1,28,28)"
      ],
      "metadata": {
        "id": "9efeGfcFsTrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2(image.unsqueeze(0).to(device))"
      ],
      "metadata": {
        "id": "Z6BdEEytpf2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.1 Stepping through nn.Conv2d"
      ],
      "metadata": {
        "id": "BedtTRuLmQZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "# create a batch of images\n",
        "images = torch.randn(size=(32,3,64,64))\n",
        "test_images = images[0]\n",
        "print(f\"Image batch shape:{images.shape}\")\n",
        "print(f\"single image shape:{test_images.shape}\")\n",
        "print(f\"Test image:\\n{test_images}\")"
      ],
      "metadata": {
        "id": "Ppvn4ncklkKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Create a convolutional layer with same dimensions as TinyVGG \n",
        "# (try changing any of the parameters and see what happens)\n",
        "conv_layer = nn.Conv2d(in_channels=3,\n",
        "                       out_channels=10,\n",
        "                       kernel_size=3,\n",
        "                       stride=1,\n",
        "                       padding=0) # also try using \"valid\" or \"same\" here \n",
        "\n",
        "# Pass the data through the convolutional layer\n",
        "conv_layer(test_images) # Note: If running PyTorch <1.11.0, this will error because of shape issues (nn.Conv.2d() expects a 4d tensor as input) \n"
      ],
      "metadata": {
        "id": "c0JB8lgLn80g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_images.shape"
      ],
      "metadata": {
        "id": "rePzKnbnSoyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_images.unsqueeze(dim=0).shape"
      ],
      "metadata": {
        "id": "xZs6wXH1TIF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_layer(test_images.unsqueeze(dim=0)).shape"
      ],
      "metadata": {
        "id": "a8Un1owcTQuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "# Create a new conv_layer with different values (try setting these to whatever you like)\n",
        "conv_layer_2 = nn.Conv2d(in_channels=3, # same number of color channels as our input image\n",
        "                         out_channels=64,\n",
        "                         kernel_size=(3, 3), # kernel is usually a square so a tuple also works\n",
        "                         stride=1,\n",
        "                         padding=1)\n",
        "\n",
        "# Pass single image through new conv_layer_2 (this calls nn.Conv2d()'s forward() method on the input)\n",
        "conv_layer_2(test_images.unsqueeze(dim=0)).shape"
      ],
      "metadata": {
        "id": "_ak3VnlQThbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out the conv_layer_2 internal parameters\n",
        "print(conv_layer_2.state_dict())"
      ],
      "metadata": {
        "id": "XiDddlfvTpV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get shapes of weight and bias tensors within conv_layer_2\n",
        "print(f\"conv_layer_2 weight shape: \\n{conv_layer_2.weight.shape} -> [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]\")\n",
        "print(f\"\\nconv_layer_2 bias shape: \\n{conv_layer_2.bias.shape} -> [out_channels=10]\")"
      ],
      "metadata": {
        "id": "q3-hJ8e9Tu86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.2 stepiing through nn.MaxPool2d()"
      ],
      "metadata": {
        "id": "ssqZK8OvT1la"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"test image original shape{test_images.shape}\")\n",
        "print(f\"test image with unsqueezed dimension:{test_images.unsqueeze(0).shape}\")\n",
        "# create a sample of nn.MaxPool2d layer\n",
        "max_pool2d_layer = nn.MaxPool2d(kernel_size=2)\n",
        "# pass data through just the conv_layer\n",
        "test_image_through_conv = conv_layer(test_images.unsqueeze(dim=0))\n",
        "print(f\"shape after going through conv_layer():{test_image_through_conv.shape}\")\n",
        "# pass data through the max pool layer\n",
        "\n",
        "test_image_through_conv_max_pool = max_pool2d_layer(test_image_through_conv)\n",
        "print(f\"shape after going through conv_layer() and maxpool2d() layer: {test_image_through_conv_max_pool.shape}\")"
      ],
      "metadata": {
        "id": "7u2cdM8SUB2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "# create random tensor with a similer number of dimensions to our images\n",
        "random_tensor = torch.randn(1,1,2,2)\n",
        "print(f\"random tensor:{random_tensor}\")\n",
        "print(f\"random tensor shape:{random_tensor.shape}\")\n",
        "# create a maxpool layer\n",
        "max_pool2d_layer = nn.MaxPool2d(2)\n",
        "max_pool_tensor = max_pool2d_layer(random_tensor)\n",
        "print(f\"\\nMax pool tensor:\\n {max_pool_tensor}\")\n",
        "print(f\"Max pool tesnor shape: {max_pool_tensor.shape}\")"
      ],
      "metadata": {
        "id": "3JHZ7SmGUB6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.3 setup a loss fuction and optimizer for model_2"
      ],
      "metadata": {
        "id": "KeFTybPqUB95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup loss fuction /eval metrics/optimizer\n",
        "from helper_functions import accuracy_fn\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_2.parameters(), lr=0.1)\n"
      ],
      "metadata": {
        "id": "4r27pcnKUCBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.4 training and testing model_2 using our training and test functions"
      ],
      "metadata": {
        "id": "UxfsZK0DUCEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "start = time.time()\n",
        "epochs = 3\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"Epoch:{epoch}\\n.....\")\n",
        "  train_step(model_2,train_dataloader,loss_fn,optimizer,accuracy_fn,device)\n",
        "  test_step(model_2,test_dataloader,loss_fn,accuracy_fn,device)\n",
        "\n",
        "end = time.time()\n",
        "time2 = end-start\n",
        "print(time2)\n"
      ],
      "metadata": {
        "id": "E33yf4rqUCHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2_results = eval_model(\n",
        "    model=model_2,\n",
        "    data_loader=test_dataloader,\n",
        "    loss_fn=loss_fn,\n",
        "    accuracy_fn=accuracy_fn,\n",
        "    \n",
        ")\n",
        "model_2_results\n"
      ],
      "metadata": {
        "id": "HTmfBBRc3_Dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Compare model results and training time"
      ],
      "metadata": {
        "id": "-FVnuFG33Am8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas"
      ],
      "metadata": {
        "id": "tvRgzywl0NFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "compare_results = pd.DataFrame([model_0_results, model_1_results, model_2_results])\n",
        "compare_results"
      ],
      "metadata": {
        "id": "enEN0l9O3KiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize our model results\n",
        "compare_results.set_index('model_name') ['model_acc'].plot(kind='barh')\n",
        "plt.xlabel('accuracy(%)')\n",
        "plt.ylabel('model')"
      ],
      "metadata": {
        "id": "PHyUy2Fe4fqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_results['executing time'] = [time0,time1,time2]\n",
        "compare_results"
      ],
      "metadata": {
        "id": "Id82rBv-9JAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_predictions(model:torch.nn.Module,data:list,device:torch.device=device):\n",
        "    pre =[]\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "      for sample in data:\n",
        "        sample = torch.unsqueeze(sample,dim=0).to(device)\n",
        "        pre_logits = model(sample)\n",
        "        pr = torch.softmax(pre_logits.squeeze(),dim=0)\n",
        "        pre.append(pr.cpu())\n",
        "\n",
        "    return torch.stack(pre)"
      ],
      "metadata": {
        "id": "g_qsCVYkcFCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "test_samples = []\n",
        "test_labels = []\n",
        "for sample, label in random.sample(list(test_data), k=9):\n",
        "    test_samples.append(sample)\n",
        "    test_labels.append(label)\n",
        "\n",
        "# View the first test sample shape and label\n",
        "print(f\"Test sample image shape: {test_samples[0].shape}\\nTest sample label: {test_labels[0]} ({class_names[test_labels[0]]})\")\n"
      ],
      "metadata": {
        "id": "Mk08Xknrg5k0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on test samples with model 2\n",
        "pred_probs= make_predictions(model=model_2, \n",
        "                             data=test_samples)\n",
        "\n",
        "# View first two prediction probabilities list\n",
        "pred_probs[:2]"
      ],
      "metadata": {
        "id": "iZgzX6Qaexc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn the prediction probabilities into prediction labels by taking the argmax()\n",
        "pred_classes = pred_probs.argmax(dim=1)\n",
        "pred_classes"
      ],
      "metadata": {
        "id": "13f6sEcYe2GA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Are our predictions in the same form as our test labels? \n",
        "test_labels, pred_classes"
      ],
      "metadata": {
        "id": "3UPt0Yxte5iI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot predictions\n",
        "plt.figure(figsize=(9, 9))\n",
        "nrows = 3\n",
        "ncols = 3\n",
        "for i, sample in enumerate(test_samples):\n",
        "  # Create a subplot\n",
        "  plt.subplot(nrows, ncols, i+1)\n",
        "\n",
        "  # Plot the target image\n",
        "  plt.imshow(sample.squeeze(), cmap=\"gray\")\n",
        "\n",
        "  # Find the prediction label (in text form, e.g. \"Sandal\")\n",
        "  pred_label = class_names[pred_classes[i]]\n",
        "\n",
        "  # Get the truth label (in text form, e.g. \"T-shirt\")\n",
        "  truth_label = class_names[test_labels[i]] \n",
        "\n",
        "  # Create the title text of the plot\n",
        "  title_text = f\"Pred: {pred_label} | Truth: {truth_label}\"\n",
        "  \n",
        "  # Check for equality and change title colour accordingly\n",
        "  if pred_label == truth_label:\n",
        "      plt.title(title_text, fontsize=10, c=\"g\") # green text if correct\n",
        "  else:\n",
        "      plt.title(title_text, fontsize=10, c=\"r\") # red text if wrong\n",
        "  plt.axis(False);"
      ],
      "metadata": {
        "id": "MTaooloidcpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make all test prediction"
      ],
      "metadata": {
        "id": "kTqIE1jYkbnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import tqdm for progress bar\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# 1. Make predictions with trained model\n",
        "y_preds = []\n",
        "model_2.eval()\n",
        "with torch.inference_mode():\n",
        "  for X, y in tqdm(test_dataloader, desc=\"Making predictions\"):\n",
        "    # Send data and targets to target device\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    # Do the forward pass\n",
        "    y_logit = model_2(X)\n",
        "    # Turn predictions from logits -> prediction probabilities -> predictions labels\n",
        "    y_pred = torch.softmax(y_logit, dim=1).argmax(dim=1)\n",
        "    # Put predictions on CPU for evaluation\n",
        "    y_preds.append(y_pred.cpu())\n",
        "# Concatenate list of predictions into a tensor\n",
        "y_pred_tensor = torch.cat(y_preds)\n",
        "y_pred_tensor"
      ],
      "metadata": {
        "id": "3itPqGvKkamP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making confusion matrixcs"
      ],
      "metadata": {
        "id": "UsXulzj7v1O4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# See if torchmetrics exists, if not, install it\n",
        "try:\n",
        "    import torchmetrics, mlxtend\n",
        "    print(f\"mlxtend version: {mlxtend.__version__}\")\n",
        "    assert int(mlxtend.__version__.split(\".\")[1]) >= 19, \"mlxtend verison should be 0.19.0 or higher\"\n",
        "except:\n",
        "    !pip install -q torchmetrics -U mlxtend # <- Note: If you're using Google Colab, this may require restarting the runtime\n",
        "    import torchmetrics, mlxtend\n",
        "    print(f\"mlxtend version: {mlxtend.__version__}\")"
      ],
      "metadata": {
        "id": "Yv01F-WHkrDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import mlxtend upgraded version\n",
        "import mlxtend \n",
        "print(mlxtend.__version__)\n",
        "assert int(mlxtend.__version__.split(\".\")[1]) >= 19 # should be version 0.19.0 or higher"
      ],
      "metadata": {
        "id": "h8I0jAcNk11N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import ConfusionMatrix\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "\n",
        "# 2. Setup confusion matrix instance and compare predictions to targets\n",
        "confmat = ConfusionMatrix(num_classes=len(class_names), task='multiclass')\n",
        "confmat_tensor = confmat(preds=y_pred_tensor,\n",
        "                         target=test_data.targets)\n",
        "\n",
        "# 3. Plot the confusion matrix\n",
        "fig, ax = plot_confusion_matrix(\n",
        "    conf_mat=confmat_tensor.numpy(), # matplotlib likes working with NumPy \n",
        "    class_names=class_names, # turn the row and column labels into class names\n",
        "    figsize=(10, 7)\n",
        ");"
      ],
      "metadata": {
        "id": "Inkl52NylAQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Create models directory (if it doesn't already exist), see: https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir\n",
        "MODEL_PATH = Path(\"models\")\n",
        "MODEL_PATH.mkdir(parents=True, # create parent directories if needed\n",
        "                 exist_ok=True # if models directory already exists, don't error\n",
        ")\n",
        "\n",
        "# Create model save path\n",
        "MODEL_NAME = \"DahwinFashionMNISTModelCNNV2.pth\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "\n",
        "# Save the model state dict\n",
        "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
        "torch.save(obj=model_2.state_dict(), # only saving the state_dict() only saves the learned parameters\n",
        "           f=MODEL_SAVE_PATH)"
      ],
      "metadata": {
        "id": "wRkQJNsflDnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a new instance of FashionMNISTModelV2 (the same class as our saved state_dict())\n",
        "# Note: loading model will error if the shapes here aren't the same as the saved version\n",
        "loaded_model_2 = DahwinFashionMNISTModelCNNV2(input_shape=1, \n",
        "                                    hidden_layers=10, # try changing this to 128 and seeing what happens \n",
        "                                    output_shape=10) \n",
        "\n",
        "# Load in the saved state_dict()\n",
        "loaded_model_2.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n",
        "\n",
        "# Send model to GPU\n",
        "loaded_model_2 = loaded_model_2.to(device)"
      ],
      "metadata": {
        "id": "dpBiJGQ3lSc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate loaded model\n",
        "torch.manual_seed(42)\n",
        "\n",
        "loaded_model_2_results = eval_model(\n",
        "    model=loaded_model_2,\n",
        "    data_loader=test_dataloader,\n",
        "    loss_fn=loss_fn, \n",
        "    accuracy_fn=accuracy_fn\n",
        ")\n",
        "\n",
        "loaded_model_2_results"
      ],
      "metadata": {
        "id": "wxm9AgTClYCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2_results"
      ],
      "metadata": {
        "id": "kkALL8x7liwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check to see if results are close to each other (if they are very far away, there may be an error)\n",
        "torch.isclose(torch.tensor(model_2_results[\"model_loss\"]), \n",
        "              torch.tensor(loaded_model_2_results[\"model_loss\"]),\n",
        "              atol=1e-08, # absolute tolerance\n",
        "              rtol=0.0001) # relative tolerance"
      ],
      "metadata": {
        "id": "tAw9sL95lngT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oyje6_2Jlqms"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}