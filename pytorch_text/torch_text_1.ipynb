{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dahyun+darwin = dahwin'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''dahyun+darwin = dahwin'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jVP1WXbCQMdH"
   },
   "outputs": [],
   "source": [
    "!pip install -U torch==1.8.0 torchtext==0.9.0\n",
    "\n",
    "# Reload environment\n",
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "N1_1PoygPp7d"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import spacy\n",
    "from torchtext.legacy.data import Field , TabularDataset , BucketIterator\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SKRtq-hJHoh9"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vuFuU2EmHR9G"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the 'en_core_web_sm' model\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "def tokenize(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "quote = Field(sequential=True, use_vocab=True, tokenize=tokenize, lower=True)\n",
    "score = Field(sequential=False, use_vocab=False)\n",
    "fields = {\"quote\": (\"q\", quote), \"score\": (\"s\", score)}\n",
    "train_data ,test_data = TabularDataset.splits(\n",
    "    path='' , train='train.json',test='test.json',format = 'json',fields=fields\n",
    ")\n",
    "\n",
    "quote.build_vocab(train_data,max_size=10000,min_freq=1)\n",
    "\n",
    "train_iterator , test_iterator = BucketIterator.splits((train_data,test_data),batch_size=2,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0mOQFsqMTVHM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "is\n",
      "an\n",
      "example\n",
      "sentence\n",
      ".\n",
      "It\n",
      "demonstrates\n",
      "the\n",
      "usage\n",
      "of\n",
      "the\n",
      "spaCy\n",
      "tokenizer\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the 'en_core_web_sm' model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define the text to be tokenized\n",
    "text = \"This is an example sentence. It demonstrates the usage of the spaCy tokenizer.\"\n",
    "\n",
    "# Tokenize the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print each token in the document\n",
    "for token in doc:\n",
    "    print(token.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9EzkoDLCTxDb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:03<00:00, 24.3MB/s]\n",
      ".vector_cache/glove.6B.zip: 862MB [02:39, 5.39MB/s]                               \n",
      "100%|█████████▉| 399999/400000 [00:26<00:00, 14865.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[   0,   11,  390,  ..., 6905,   25,   66],\n",
      "        [4760,  313,  990,  ..., 7958, 2737,   22],\n",
      "        [   8,   16,  542,  ...,  315, 7331,    9],\n",
      "        ...,\n",
      "        [   0,   16,   18,  ...,    1,    1,    1],\n",
      "        [ 205,   24, 7504,  ...,    1,    1,    1],\n",
      "        [  88,    4,    1,  ...,    1,    1,    1]]), tensor([461, 461, 460, 460, 460, 459, 459, 458, 458, 458, 457, 456, 456, 455,\n",
      "        455, 455, 455, 454, 454, 454, 454, 453, 453, 453, 452, 452, 452, 451,\n",
      "        451, 451, 451, 451, 450, 450, 450, 450, 450, 450, 450, 449, 449, 449,\n",
      "        449, 449, 448, 448, 448, 448, 447, 446, 446, 445, 445, 445, 445, 445,\n",
      "        445, 445, 444, 444, 444, 443, 443, 443]))\n",
      "tensor([0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
      "        1., 0., 1., 0., 1., 1., 0., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "from torchtext.legacy.datasets import IMDB\n",
    "from torchtext.legacy.data import Field, LabelField, BucketIterator\n",
    "\n",
    "# Define the fields to preprocess the data\n",
    "TEXT = Field(tokenize='spacy', tokenizer_language='en_core_web_sm', include_lengths=True)\n",
    "LABEL = LabelField(dtype=torch.float)\n",
    "\n",
    "# Load the IMDB dataset\n",
    "train_data, test_data = IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "# Build the vocabulary\n",
    "TEXT.build_vocab(train_data, max_size=10000, vectors='glove.6B.100d')\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "# Create an iterator to iterate over the batches of the data\n",
    "train_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, test_data), \n",
    "    batch_size=64, \n",
    "    sort_within_batch=True, \n",
    "    sort_key=lambda x: len(x.text),\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")\n",
    "\n",
    "# Access the vocabulary and the numerical representation of the first batch of the data\n",
    "batch = next(iter(train_iterator))\n",
    "print(batch.text)\n",
    "print(batch.label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.stoi['love'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNv1Mst4Myi+/rLIV2gLELL",
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
