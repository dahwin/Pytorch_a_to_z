{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-23T09:20:58.828860Z","iopub.execute_input":"2023-04-23T09:20:58.829454Z","iopub.status.idle":"2023-04-23T09:20:58.842326Z","shell.execute_reply.started":"2023-04-23T09:20:58.829389Z","shell.execute_reply":"2023-04-23T09:20:58.841329Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/input/shakespeareonline/t8.shakespeare.txt\n/kaggle/input/us-baby-names/StateNames.csv\n/kaggle/input/us-baby-names/NationalReadMe.pdf\n/kaggle/input/us-baby-names/hashes.txt\n/kaggle/input/us-baby-names/NationalNames.csv\n/kaggle/input/us-baby-names/StateReadMe.pdf\n/kaggle/input/us-baby-names/database.sqlite\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport string\nimport random\nimport sys\nimport unidecode\nimport pandas as pd\nfrom torch.utils.tensorboard import SummaryWriter\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nall_characters = string.printable\nn_characters = len(all_characters)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-23T09:20:22.914291Z","iopub.execute_input":"2023-04-23T09:20:22.914986Z","iopub.status.idle":"2023-04-23T09:20:27.131941Z","shell.execute_reply.started":"2023-04-23T09:20:22.914930Z","shell.execute_reply":"2023-04-23T09:20:27.130802Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.get_device_name(0)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-23T08:20:35.526183Z","iopub.execute_input":"2023-04-23T08:20:35.527288Z","iopub.status.idle":"2023-04-23T08:20:35.536135Z","shell.execute_reply.started":"2023-04-23T08:20:35.527260Z","shell.execute_reply":"2023-04-23T08:20:35.535024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Read the CSV file using pandas\ndf = pd.read_csv('/kaggle/input/us-baby-names/StateNames.csv')\n\n# Extract the \"Name\" column as a list of strings\nnames = df['Name']\n\n# Write the names to a text file\nwith open('names.txt', 'w') as f:\n    f.write('\\n'.join(names))\n","metadata":{"execution":{"iopub.status.busy":"2023-04-23T08:20:35.537701Z","iopub.execute_input":"2023-04-23T08:20:35.538178Z","iopub.status.idle":"2023-04-23T08:20:38.868008Z","shell.execute_reply.started":"2023-04-23T08:20:35.538101Z","shell.execute_reply":"2023-04-23T08:20:38.866478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Read large text file (Note can be any text file: not limited to just names)\nfile = unidecode.unidecode(open(\"/kaggle/input/shakespeareonline/t8.shakespeare.txt\").read())","metadata":{"execution":{"iopub.status.busy":"2023-04-23T09:21:25.098567Z","iopub.execute_input":"2023-04-23T09:21:25.099690Z","iopub.status.idle":"2023-04-23T09:21:25.189516Z","shell.execute_reply.started":"2023-04-23T09:21:25.099634Z","shell.execute_reply":"2023-04-23T09:21:25.188485Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class RNN(nn.Module):\n    def __init__(self,input_size,hidden_size,num_layers,output_size):\n        super(RNN,self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        self.embed = nn.Embedding(input_size,hidden_size)\n        self.lstm = nn.LSTM(hidden_size,hidden_size,num_layers,batch_first=True)\n        self.fc = nn.Linear(hidden_size,output_size)\n    def forward(self,x,hidden,cell):\n        out = self.embed(x)\n        out,(hidden,cell) = self.lstm(out.unsqueeze(1),(hidden,cell))\n        out = self.fc(out.reshape(out.shape[0],-1))\n        return out, (hidden,cell) # add return statement to output the result of the linear layer\n    def init_hidden(self,batch_size):\n        hidden = torch.zeros(self.num_layers,batch_size,self.hidden_size).to(device)\n        cell = torch.zeros(self.num_layers,batch_size,self.hidden_size).to(device)\n        return hidden,cell\n\n        \n\nclass Generator():\n    def __init__(self):\n        self.chunk_len = 250\n        self.num_epochs = 5000\n        self.batch_size = 1\n        self.print_every = 50\n        self.hidden_size = 256\n        self.num_layers = 2\n        self.lr = 0.003\n    def char_tensor(self,string):\n        tensor = torch.zeros(len(string)).long()\n        for c in range(len(string)):\n            tensor[c] = all_characters.index(string[c])\n        return tensor\n    def get_random_batch(self):\n        start_idx = random.randint(0, len(file) - self.chunk_len)\n        end_idx = start_idx + self.chunk_len + 1\n        text_str = file[start_idx:end_idx]\n        text_input = torch.zeros(self.batch_size, self.chunk_len)\n        text_target = torch.zeros(self.batch_size, self.chunk_len)\n\n        for i in range(self.batch_size):\n            text_input[i, :] = self.char_tensor(text_str[:-1])\n            text_target[i, :] = self.char_tensor(text_str[1:])\n\n        return text_input.long(), text_target.long()\n\n\n    \n        \n    \n    def generate(self,initial_str ='A',predic_len=100,temperature=0.85):\n        hidden , cell = self.rnn.init_hidden(batch_size = self.batch_size)\n        initial_input = self.char_tensor(initial_str)\n        predicted = initial_str \n        for p in range(len(initial_str)-1):\n            _, (hidden,cell) = self.rnn(initial_input[p].view(1).to(device),hidden,cell)\n        \n        last_char = initial_input[-1]\n        for p in range(predic_len):\n            output , (hidden,cell) = self.rnn(last_char.view(1).to(device),hidden,cell)\n            output_dist = output.data.view(-1).div(temperature).exp()\n            top_char = torch.multinomial(output_dist,1)[0]\n            \n            predicted_char = all_characters[top_char]\n            predicted += predicted_char\n            last_char = self.char_tensor(predicted_char)\n        return predicted\n            \n\n    def train(self):\n        self.rnn = RNN(n_characters,self.hidden_size,self.num_layers,n_characters).to(device)\n        optimizer = torch.optim.Adam(self.rnn.parameters(),lr= self.lr)\n        criterion = nn.CrossEntropyLoss()\n        writer = SummaryWriter(f'run/names0')\n        print('=> Starting training')\n        \n        for epoch in range(1,self.num_epochs +1):\n            inp , target = self.get_random_batch()\n            hidden , cell = self.rnn.init_hidden(batch_size = self.batch_size)\n            self.rnn.zero_grad()\n            loss = 0\n            inp= inp.to(device)\n            target = target.to(device)\n            for c in range(self.chunk_len):\n                output , (hidden, cell) = self.rnn(inp[:,c],hidden,cell)\n                loss += criterion(output,target[:,c])\n            loss.backward()\n            optimizer.step()\n            loss = loss.item() / self.chunk_len\n            if epoch % self.print_every == 0:\n                \n                print(f'Loss:{loss}')\n                print(self.generate())\n            writer.add_scalar(\"Training loss\",loss,global_step=epoch)\n\n            \ngen = Generator()\ngen.train()","metadata":{"execution":{"iopub.status.busy":"2023-04-23T09:21:25.696139Z","iopub.execute_input":"2023-04-23T09:21:25.696978Z","iopub.status.idle":"2023-04-23T09:40:08.400822Z","shell.execute_reply.started":"2023-04-23T09:21:25.696937Z","shell.execute_reply":"2023-04-23T09:40:08.399371Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"=> Starting training\nLoss:2.561072265625\nA=&f es fssanirl  auimet.       h on thatithe is bell and Srinled fhod Fdus.\n       hy   cilld af ire\nLoss:2.272098876953125\nAGNE. 1gof soose, andy one.\n  Tathrereas that terises in, wuadatilrile isisead hit the be sone healn?\nLoss:2.083132568359375\nA. Hof\n    PRODOIROS. SOMS E. TRNSTOONIEN. Shin, Dome, a lrotrem of Whame ar ale,\n     \n    Ineve my \nLoss:2.30299169921875\nA\t hell a, deenin t then thest, deans angodt foncest meres to mrove Broone.\n                         \nLoss:1.9455101318359376\nAHTARETIE. To wous man thim to in meantend a un thou tell and sellain.\n  weal for the all arited manp\nLoss:1.93996826171875\nAER. A youm son to thy to thsing,\n    Of bemice, a sour conce;\n      Wisith's hine bride siste, besde\nLoss:1.86693701171875\nA+an. O wass sayin!\n    That then'll and neing this him of hit the seet?\n  I vere yous and hamtien's \nLoss:1.867953125\nANN. Pear dake Goice,\n    Nake your and in that witad.\n    Fold true now nower wolf word sour surmanc\nLoss:2.415004638671875\nADUNE OF IIS DUS ANHEPD WHARGE\n\n    TORAS TIRBRIOS TONThink For the serce and have liet anfong mune w\nLoss:1.768645263671875\nAv\n    Shirsemin, lord,\n    hin. Hose is farth thim the a this swentith hill\n    geret.              \nLoss:1.95617333984375\nA3 I'll or herry forthin hink divice you deay. That\n    But not for head, in of whom browilic'd now f\nLoss:1.8723033447265625\nAR. As thou pled.\n  DOSTISDHICTAS OLUS. My the prow a' frike this\n       and heareft olf in on mack w\nLoss:1.6737679443359375\nAS 2        Dececonce the fay, hi taul\n    Cone betteef my ellowow\n      And for I well for queerter,\nLoss:1.7987340087890624\nARD. He are this all sape, and betiing. I a propart bistune that me be the his your let.\n    This all\nLoss:1.8000499267578125\nAd;  I mis here make  \n    As the doth you begtiss. What Frint eyes fill the Romridenm\n    now you!\n \nLoss:1.72664453125\nAS. Whire impent the beseth.\n  GOFTOR. OR. Not king to day fulllartuat in out\n    thou the wish the\n \nLoss:2.486721923828125\nAg. Hoor so sird, and\n       Erponer to thet quison SImICILAL VY DIOCESUS.                           \nLoss:2.0052808837890623\nAS  You canquint?\n  RICENDELLO. She did then the tenct! She-\n    And shall to thenser rest in here Be\nLoss:1.776539794921875\nAR. Do do sI when me in thou,\n    and have a podish meming-\n    Or calf the mistoon now the poets. Ye\nLoss:1.643308837890625\nAN\n        Exit-\n  SLECOS AY I Erpired\n    Belar.\n    Whose speak and graces, as you struce,\n    What\nLoss:1.71979736328125\nARDES CHERINA. Pate in the prove me in you hollandines spubneite tongwave, of Evardiev'd, the be the \nLoss:1.6811368408203125\nAY March marry.\n  YOCTHERK. Wh of Goint to hout that everanty as why frowned spiry lets;\n    Under mo\nLoss:1.6757152099609376\nAMDE. Mad in sild, and she das'd a conness, gaby I's God ruddion to usrandeson.\n  AABESSHERS. Who man\nLoss:1.7407503662109376\nAS COYN COPIES OFER SAOR OENST MARESTIMEMIUS IS OFFUR YOR RIANCT SYRBOHARD. Be my furat'd worth shall\nLoss:1.189399169921875\nARD (2) ARE WORL. Who may trued? Some other wearse answeart head thee.\nEnery.\n  BURC. He vay your hei\nLoss:1.659213134765625\nAFOR LIBRALEDE TOM LOWLLY, ANY\nSERVICE THIME OF YOURLY, ANWLONE COLLLEGEPHERTOLYY.  PISTIBUTH PENEY, \nLoss:1.5459158935546875\nAN. Fand inclords so, th' will speaks a for your bless high you see.\n  TITCER. No, our colmen with wa\nLoss:1.6163397216796875\nANDANGES BERTRINE FFRICEMBERNE. Thy genter pricas,\n    For marry have anfedient the our though's heir\nLoss:1.54196240234375\nA now, and this shall from'd the done,\n    Therefore your shall not a sirt find 'tis all him.\n  SICHL\nLoss:1.3498023681640625\nAGE LeBUR\n\n  LOTNAX. The Duke a ney, and will sinch!\n  VORTIO. Came, lith me fashow should brook'st e\nLoss:1.6915477294921875\nAs good me. I canto see mear, with I queen-]\n    I will so we curnave his hand all weish to throught \nLoss:1.5922740478515625\nAThough;\n    Which his ward, and abinted and shall astes\n    Where batter Hast hath the toth you love\nLoss:1.5472135009765624\nAH. Sinch, let the Dever; who beany. What where you and threin your when by\n    The not it not keet s\nLoss:1.7128118896484374\nALY. What gader had of all some some too.\n    Come, ow, I.\n  WALY. And, godd, I have walk a make you \nLoss:1.6165572509765624\nARDOLY. I, when rusbarrether of the\n    thou are the give to see of much thou doldingrish\n    Upon yo\nLoss:1.7993826904296875\nA\n    Will dusity,                                             'A man end must man, seat desuffiester\nLoss:1.5089700927734375\nARGHRIND\n\n  ENOWNA\n    Thy transt shall him as plust it with viliiors,\n    And here in they the from \nLoss:1.371357421875\nAll bland,\n    To you the pain ane on blood,\n    And stroniong his corite rus my noted.\n    Couce him\nLoss:1.6727615966796876\nATH, S1ERDESES. What sen carr'st speaks you no faith;\n        [               Arm seet the pales best\nLoss:1.319873046875\nAFRERE VIROCE. What equarry, sir, there lord.\n  Sig Havour hack; and horronouns of his done and so.\n \nLoss:1.5872174072265626\nArrant you are is thy\n    lose thee leaves! Alam'd of you, Cornument is all me as brink seent!\n      \nLoss:1.5975076904296874\nAL and a the shall within,\n    But rabell's be caulels up thee, thou traint to glose.\n  CASTIA. Now, \nLoss:1.50263720703125\nATHARD OF WARDIO. He godder make think in you now.\n  BURYLUS. I am this appeak me.\n  Come. While, him\nLoss:1.569758056640625\nATOR\n\n  WINTIUS. Ay, could, lord with that mike and she\n    think thou set,\n    Benest do thou tand. \nLoss:1.493425537109375\nARES, or, sir.\n    my bean not, for the as spot, and mastent.\n  WILLIIVA. Ial I am his a will man sta\nLoss:1.667827392578125\nARD THERLIS excife onf\n    Come hart, that upon my leave an in\n    Prince of you prise there you, and\nLoss:1.92660693359375\nARAY. Do me rue's postectiouson, inswere; you to the Duke Ifore\n  And all the all to harns\n        an\nLoss:1.51114453125\nATRIP.>>\n\n\n\n1resence..\n\nA evertiend man Majesten.\n\nEnter IDRAMIESS ARE IMOLLUDERIUS. My like prophen \nLoss:1.64777099609375\nATION BY BE\nDICE\nWITH BY ARIEG BY ANY\nSERVICE ROMIDED COMMERCIALABLE LOAT DISTRIBUTION COPLEGE\n  SERV\nLoss:1.4997763671875\nARAND!ARE 2Y\nWARG OTHERS\nPERDESSER. Come a find the praiter; and stow'd be not, whones (all so?\n    I\nLoss:1.64342236328125\nA\n    And thank pleasure.\n  KING HENRY. How answies, Sir Tone, Petcess\n\n\n\n\n\n\n\n\n\n<<THIS        Encortu\nLoss:1.5562117919921874\nARDION LONE NETERANS EDWIADARD NENTIAN. May) some and corture.\n  KING HELEMANNE FARD GUNES. I will po\nLoss:1.4985054931640625\nANCO. As farewell, I like heaven that reppless Rollwey?\n  PAMILLO. Peace! I must therews death bea th\nLoss:1.6330145263671876\nAR is marching him which whose,\n    I will not for shiphalved of York till be gord we raster,\n    And\nLoss:1.5955087890625\nAup, assey death, misers, be matcher.\n    I am he was all love shell be with all impossel, sir.      \nLoss:1.64620654296875\nARABURY. Haste of must Rome. I being him like it jage,\n    And let her reforcest that inclouch as is \nLoss:1.6653895263671874\nANTOND SIXTVATED BY VOY THE\nBIVERSION YOUR OF THE WILLIZONG\n  SIR WILLIAM\nSHATHONEY OF AS SULYONTED S\nLoss:1.6617471923828124\nAnding\n    I do many fail much a coorse. I till not shall a true give thee, the lent and faice\n    To\nLoss:1.477804931640625\nAGUS SITR\n\nnot these sains this of my do bon.\n    Thou to speech in wrust me them, and be of\n    How \nLoss:1.5241422119140624\nADABETH COMMERCIAL DIAN COPIES DOT CHARGES BY OR USECOND CHARLES OF FRIMPLER FOR YOUR OR OTHERS\nPERSO\nLoss:1.3722076416015625\nAND MACKINGHAM BENeE I.\nRear. You or bechals the come.\n    Under justides an anglandedoo water?\n  LOS\nLoss:1.5563052978515626\nAND\nSERVACE OTHERS. Sir bearten, her are you worse?\n  CASSANDARD. God hands, and not thou shawn, thos\nLoss:1.6614117431640625\nAs see shealonest, and by to eyes o' here\n    And as though thine bronglest, with a come-\n  ATFIRD. C\nLoss:1.63158154296875\nARY BENEDICTINE REW  Staster, all Cas,                Mark, this straws and temparow'd, not been to t\nLoss:1.533909423828125\nATHUS CLOWNLO COPIES FOR YOR EGENWAR OR FOR VERSONAL USE ROVICE COPIES (1) ARE ANY SOLLINABER. Come n\nLoss:1.4415614013671876\nANCLAND COMMERCTIO INCLAND\n    I leave it of the wild your thile,\n    And her soletand many worth his\nLoss:1.4270260009765625\nAGE, I wench deter we do to have with me;\n    Do know welcome!\n    Which deekety my you cannot guel!\n\nLoss:1.461014892578125\nAEGET. Boy him for hast was of my lew some speech.\n  Pray mistright all Antonblyfore laise of gentle \nLoss:1.4314630126953125\nATARD\nSERVICE THAT BY ANY\nSERVICE THE COMMERCIAL DISTRIBUTED OR UTHUS OF OF FYRIBUTED SO LONG AS SUCH\nLoss:1.4895980224609375\nAND the shall retulked was\n    And with thee batch; cold caurt are with my then, you profors in their\nLoss:1.52343505859375\nAND         Enesces to the with days all be one pray that a rong'd in see not thou show dorned him\n  \nLoss:1.607126220703125\nATUS. He was felt his honoun to his wrongs.\n    Therefore loves his viol that he mensain it.\n  DIOMAN\nLoss:1.4019234619140626\nATHERS BY ANY\nSERVID SCENE JONG DISTRIBUTED OR FIR DOWNLOAD TIME OR FOR MEMBERSHIP.>>\n\n\n\n\nACT VICH 1E\nLoss:1.5694154052734375\nAi.\nANTIPHOLUMNES BY ANY\nSCENE I.\nCangeth. The crose. The tray you grom.\n  KING HENRY. Ay, my lords d\nLoss:1.2973594970703124\nANDS. Thus, I umpreed. Let well make were it that you we have is,\n    Or sorrows of seel offess, we w\nLoss:1.8311397705078125\nAY of ruect'st thee?\n  Hore. Certaly!\n  SILVIA. Who ait guvens that griev'd are shall fair\n    And me\nLoss:1.702376220703125\nAGEN\n\n\n\nCOLSTAFF. Confirm you? Thou shalt iranabed and their brave up,\n    But you and menes as done \nLoss:1.3989613037109374\nAm, for the toom. What whose actes a decrius.\n    I'n mother's recompon these Cassion.\n    How, now, \nLoss:1.472373779296875\nAX. There?\n\n\n\n\n<<THIS ELECTRUS. I sagger this so partent hath a priested.\n  CAMILLO. What with the ne\nLoss:1.4538868408203125\nARD] I think valie; For alm I\n    hear fach the restorn them of Leter there-fork;\n    Four me. A take\nLoss:1.3897593994140625\nANDON, TiBURT, MRINCE THAT CHARGES PEEMILLENA. Fear the Quiturer. 'His brow Ham. If this more,\n    We\nLoss:1.6252283935546874\nADOLA\n\n  WELIS. I will light of the day to so all breath,\n    When in a time, ere the sweer; true hel\nLoss:1.4025625\nATOR\n\n  LADY MARGAMETUS. I cousin your night? But I do would you a concled,\n    And here's good boine\nLoss:1.4105653076171876\nAm IMel you, sorrow\n    The trembreds.\n    When have mend on, I though do with\n    would vionces to c\nLoss:1.560978759765625\nAThe his desparts,  \n    For surre tongue, into done corsely know- Then dride others,\n    And our for\nLoss:1.5180643310546875\nAm such upon now\n     To repade we intend.                                       Exeunt Romenation [w\nLoss:1.60122607421875\nAur, he sent in flatte!\n    Look my lord I pidoran would I fall'd do is myself for his two our sing g\nLoss:1.534252197265625\nAs the entroct themself\n    I would does of heart be fore?\n  ANTONIO. Is be herion to a body wain, at\nLoss:1.5286328125\nANDER. Ay, unto Greed the lash and put it house toys not, that I do, sir, my fair in that he contintc\nLoss:1.544421630859375\nAnty her.\n  Moo. If, thee stand Sails; my paleman and\n    wintabers, but coming dead, leave them. Wha\nLoss:1.524490234375\nAThough from me.\n  SYRS. Ay, as no master'd on heart hell; I please the Frenchmmer.\n    I come my bus\nLoss:1.24141845703125\nAnch hath at that as with full faintly may her, an ege\n     These Anclument; I cap it infour of love.\nLoss:1.5539166259765624\nAT OR USER ONLY, AND (2) ARE FOR MEMBERSHIS BY PERMISSION.>>\n\n\n\n\n\nSCENG III.\nPROVEELL OR OTHERS\nPROVE\nLoss:1.6213929443359374\nA, wrace not this well.\n  RODERIGH. I will keep my other to be some\n    Benowned of all him we poor a\nLoss:1.110271728515625\nAusuld and truth, he good than your love fools;\n    castar him my a hast be lord the eyes of\n    happ\nLoss:1.3264169921875\nAThoughts, as I will good a bemird.\n    Is't had not the our please, both, may so more,\n    That that\nLoss:1.473622802734375\nABROLED 'Sing others]\n    It to the now, the stany painted lies as\n    want be answer-vietch that nam\nLoss:1.3499388427734376\nAll; I'll strange, marry.\n                                                Exeunt\n\n\n\n\nSCENE III.\nRemer\nLoss:1.6494718017578125\nA       Enter SENGARET. My soul since and pourse of Nortruff,\n  That voy to still the earm; Lucide an\nLoss:1.501807373046875\nAbuty of old\n    When for them doth mine and think monius, that I thy unclents;\n     When mut. Shall \n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}