{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNv1Mst4Myi+/rLIV2gLELL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!pip install -U torch==1.8.0 torchtext==0.9.0\n","\n","# Reload environment\n","exit()"],"metadata":{"id":"jVP1WXbCQMdH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N1_1PoygPp7d"},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch import optim\n","import spacy\n","from torchtext.legacy.data import Field , TabularDataset , BucketIterator\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","source":["!python -m spacy download en_core_web_sm"],"metadata":{"id":"SKRtq-hJHoh9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import spacy\n","\n","# Load the 'en_core_web_sm' model\n","spacy_en = spacy.load(\"en_core_web_sm\")\n","def tokenize(text):\n","    return [tok.text for tok in spacy_en.tokenizer(text)]\n","quote = Field(sequential=True, use_vocab=True, tokenize=tokenize, lower=True)\n","score = Field(sequential=False, use_vocab=False)\n","fields = {\"quote\": (\"q\", quote), \"score\": (\"s\", score)}\n","train_data ,test_data = TabularDataset.splits(\n","    path='' , train='train.json',test='test.json',format = 'json',fields=fields\n",")\n","\n","quote.build_vocab(train_data,max_size=10000,min_freq=1)\n","\n","train_iterator , test_iterator = BucketIterator.splits((train_data,test_data),batch_size=2,device=device)"],"metadata":{"id":"vuFuU2EmHR9G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import spacy\n","\n","# Load the 'en_core_web_sm' model\n","nlp = spacy.load('en_core_web_sm')\n","\n","# Define the text to be tokenized\n","text = \"This is an example sentence. It demonstrates the usage of the spaCy tokenizer.\"\n","\n","# Tokenize the text\n","doc = nlp(text)\n","\n","# Print each token in the document\n","for token in doc:\n","    print(token.text)\n"],"metadata":{"id":"0mOQFsqMTVHM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9EzkoDLCTxDb"},"execution_count":null,"outputs":[]}]}