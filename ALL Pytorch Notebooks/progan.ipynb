{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"mount_file_id":"1Vll084W5Wj6AGvdUdzcmz83Foq8eyNXA","authorship_tag":"ABX9TyNLvQ6GD0+120c7W2nAHKWl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ZXMcj_yyvqZT"},"outputs":[],"source":["from torch.nn.modules.activation import LeakyReLU\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from math import log2\n","factors = [1,1,1,1,1/2,1/4,1/8,1/16,1/32]\n","class WSconv2d(nn.Module):\n","  def __init__(self,in_channels,out_channels,kernel_size=3,stride=1,padding=1,gain=2):\n","    super().__init__()\n","    self.conv = nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding)\n","    self.scale= (gain/(in_channels*kernel_size**2))**0.5\n","    self.bias = self.conv.bias\n","    self.conv.bias = None\n","    # initialzie conv layer \n","    nn.init.normal_(self.conv.weight)\n","    nn.init.zeros_(self.bias)\n","  def forward(self, x):\n","          return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)\n","\n","class PixelNorm(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.epsilon = 1e-8\n","  def forward(self, x):\n","        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.epsilon)\n","class ConvBlock(nn.Module):\n","  def __init__(self,in_channles,out_channels,use_pixelnorm=True):\n","    super().__init__()\n","    self.conv1 = WSconv2d(in_channles,out_channels)\n","    self.conv2 = WSconv2d(out_channels,out_channels)\n","    self.leaky = nn.LeakyReLU(0.2)\n","    self.pn = PixelNorm()\n","    self.use_pn = use_pixelnorm\n","  def forward(self,x):\n","    x = self.leaky(self.conv1(x))\n","    x = self.pn(x) if self.use_pn else x\n","    x = self.leaky(self.conv2(x))\n","    x = self.pn(x) if self.use_pn else x\n","\n","    return x \n","\n","class Generator(nn.Module):\n","    def __init__(self, z_dim, in_channels, img_channels=3):\n","        super(Generator, self).__init__()\n","\n","        # initial takes 1x1 -> 4x4\n","        self.initial = nn.Sequential(\n","            PixelNorm(),\n","            nn.ConvTranspose2d(z_dim, in_channels, 4, 1, 0),\n","            nn.LeakyReLU(0.2),\n","            WSconv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n","            nn.LeakyReLU(0.2),\n","            PixelNorm(),\n","        )\n","\n","        self.initial_rgb = WSconv2d(\n","            in_channels, img_channels, kernel_size=1, stride=1, padding=0\n","        )\n","        self.prog_blocks, self.rgb_layers = (\n","            nn.ModuleList([]),\n","            nn.ModuleList([self.initial_rgb]),\n","        )\n","\n","        for i in range(\n","            len(factors) - 1\n","        ):  # -1 to prevent index error because of factors[i+1]\n","            conv_in_c = int(in_channels * factors[i])\n","            conv_out_c = int(in_channels * factors[i + 1])\n","            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c))\n","            self.rgb_layers.append(\n","                WSconv2d(conv_out_c, img_channels, kernel_size=1, stride=1, padding=0)\n","            )\n","\n","    def fade_in(self, alpha, upscaled, generated):\n","        # alpha should be scalar within [0, 1], and upscale.shape == generated.shape\n","        return torch.tanh(alpha * generated + (1 - alpha) * upscaled)\n","\n","    def forward(self, x, alpha, steps):\n","        out = self.initial(x)\n","\n","        if steps == 0:\n","            return self.initial_rgb(out)\n","\n","        for step in range(steps):\n","            upscaled = F.interpolate(out, scale_factor=2, mode=\"nearest\")\n","            out = self.prog_blocks[step](upscaled)\n","\n","        # The number of channels in upscale will stay the same, while\n","        # out which has moved through prog_blocks might change. To ensure\n","        # we can convert both to rgb we use different rgb_layers\n","        # (steps-1) and steps for upscaled, out respectively\n","        final_upscaled = self.rgb_layers[steps - 1](upscaled)\n","        final_out = self.rgb_layers[steps](out)\n","        return self.fade_in(alpha, final_upscaled, final_out)\n","class Discriminator(nn.Module):\n","  def __init__(self,z_dim,in_channels,img_channels=3):\n","    super().__init__()\n","    self.prog_blocks , self.rgb_layers = nn.ModuleList(),nn.ModuleList()\n","    self.leaky = nn.LeakyReLU(0.2)\n","    for i in range(len(factors)-1,0,-1):\n","      conv_in_c = int(in_channels*factors[i])\n","      conv_out_c = int(in_channels*factors[i-1])\n","      self.prog_blocks.append(ConvBlock(conv_in_c,conv_out_c,use_pixelnorm=False))\n","      self.rgb_layers.append(WSconv2d(img_channels,conv_in_c,kernel_size=1,stride=1,padding=0))\n","    self.initial_rgb =WSconv2d(img_channels,in_channels,kernel_size=1,stride=1,padding=0)\n","    self.rgb_layers.append(self.initial_rgb)\n","    self.avg_pool = nn.AvgPool2d(kernel_size=2,stride=2)\n","\n","    self.final_block = nn.Sequential(\n","        WSconv2d(in_channels+1,in_channels,kernel_size=3,stride=1,padding=1),\n","        nn.LeakyReLU(0.2),\n","        WSconv2d(in_channels,in_channels,kernel_size=4,stride=1,padding=0),\n","        nn.LeakyReLU(0.2),\n","        WSconv2d(in_channels,1,kernel_size=1,stride=1,padding=0)\n","    )\n","  def fade_in(self,alpha,downscaled,out):\n","    return alpha*out+(1-alpha)*downscaled\n","  def minibatch_std(self,x):\n","    batch_statistics = torch.std(x,dim=0).mean().repeat(x.shape[0],1,x.shape[2],x.shape[3])\n","    return torch.cat([x,batch_statistics],dim=1)\n","  def forward(self,x,alpha,steps):\n","    cur_step = len(self.prog_blocks)-steps\n","    out = self.leaky(self.rgb_layers[cur_step](x))\n","    if steps ==0:\n","      out = self.minibatch_std(out)\n","      return self.final_block(out).view(out.shape[0],-1)\n","    downscaled = self.leaky(self.rgb_layers[cur_step+1](self.avg_pool(x)))\n","    out = self.avg_pool(self.prog_blocks[cur_step](out))\n","    out = self.fade_in(alpha,downscaled,out)\n","    for step in range(cur_step+1,len(self.prog_blocks)):\n","      out = self.prog_blocks[step](out)\n","      out = self.avg_pool(out)\n","    out = self.minibatch_std(out)\n","    return self.final_block(out).view(out.shape[0],-1)\n","\n","Z_DIM = 50\n","  \n","IN_CHANNELS = 256\n","gen = Generator(Z_DIM, IN_CHANNELS, img_channels=3)\n","critic = Discriminator(Z_DIM, IN_CHANNELS, img_channels=3)\n","\n","for img_size in [4, 8, 16, 32, 64, 128, 256, 512, 1024]:\n","    num_steps = int(log2(img_size / 4))\n","    x = torch.randn((1, Z_DIM, 1, 1))\n","    z = gen(x, 0.5, steps=num_steps)\n","    assert z.shape == (1, 3, img_size, img_size)\n","    out = critic(z, alpha=0.5, steps=num_steps)\n","      \n","    assert out.shape == (1, 1)\n","    print(f\"Success! At img size: {img_size}\")\n"]},{"cell_type":"code","source":["factors = [1,1,1,1,1/2,1/4,1/8,1/16,1/32]\n","for i in range(len(factors)-1):\n","      print(i)"],"metadata":{"id":"w3qoc4dcrYrS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(len(factors) - 1, 0, -1):\n","  print(i)"],"metadata":{"id":"dKX6UxPjGkGg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Config"],"metadata":{"id":"t0J1zVynKCdj"}},{"cell_type":"code","source":["import zipfile\n","from pathlib import Path\n","with zipfile.ZipFile('twice/celeba.zip','r') as zip_ref:\n","  print('Unzipping data')\n","  zip_ref.extractall()"],"metadata":{"id":"WOaZmBLhSH0A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["START_TRAIN_AT_IMG_SIZE = 128\n","DATASET = 'celeb_dataset'\n","CHECKPOINT_GEN = \"generator.pth\"\n","CHECKPOINT_CRITIC = \"critic.pth\"\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","SAVE_MODEL = True\n","LOAD_MODEL = False\n","LEARNING_RATE = 1e-3\n","BATCH_SIZES = [32, 32, 32, 16, 16, 16, 16, 8, 4]\n","CHANNELS_IMG = 3\n","Z_DIM = 256  # should be 512 in original paper\n","IN_CHANNELS = 256  # should be 512 in original paper\n","CRITIC_ITERATIONS = 1\n","LAMBDA_GP = 10\n","PROGRESSIVE_EPOCHS = [30] * len(BATCH_SIZES)\n","FIXED_NOISE = torch.randn(8, Z_DIM, 1, 1).to(DEVICE)\n","NUM_WORKERS = 4"],"metadata":{"id":"8nOis0XJGnPP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Utils"],"metadata":{"id":"JiYYv9-GKM_6"}},{"cell_type":"code","source":["import torch\n","import random\n","import numpy as np\n","import os\n","import torchvision\n","import torch.nn as nn\n","\n","from torchvision.utils import save_image\n","from scipy.stats import truncnorm\n","\n","# Print losses occasionally and print to tensorboard\n","def plot_to_tensorboard(\n","    writer, loss_critic, loss_gen, real, fake, tensorboard_step\n","):\n","    writer.add_scalar(\"Loss Critic\", loss_critic, global_step=tensorboard_step)\n","\n","    with torch.no_grad():\n","        # take out (up to) 8 examples to plot\n","        img_grid_real = torchvision.utils.make_grid(real[:8], normalize=True)\n","        img_grid_fake = torchvision.utils.make_grid(fake[:8], normalize=True)\n","        writer.add_image(\"Real\", img_grid_real, global_step=tensorboard_step)\n","        writer.add_image(\"Fake\", img_grid_fake, global_step=tensorboard_step)\n","\n","\n","def gradient_penalty(critic, real, fake, alpha, train_step, device=\"cpu\"):\n","    BATCH_SIZE, C, H, W = real.shape\n","    beta = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n","    interpolated_images = real * beta + fake.detach() * (1 - beta)\n","    interpolated_images.requires_grad_(True)\n","\n","    # Calculate critic scores\n","    mixed_scores = critic(interpolated_images, alpha, train_step)\n","\n","    # Take the gradient of the scores with respect to the images\n","    gradient = torch.autograd.grad(\n","        inputs=interpolated_images,\n","        outputs=mixed_scores,\n","        grad_outputs=torch.ones_like(mixed_scores),\n","        create_graph=True,\n","        retain_graph=True,\n","    )[0]\n","    gradient = gradient.view(gradient.shape[0], -1)\n","    gradient_norm = gradient.norm(2, dim=1)\n","    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n","    return gradient_penalty\n","\n","\n","def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n","    print(\"=> Saving checkpoint\")\n","    checkpoint = {\n","        \"state_dict\": model.state_dict(),\n","        \"optimizer\": optimizer.state_dict(),\n","    }\n","    torch.save(checkpoint, filename)\n","\n","\n","def load_checkpoint(checkpoint_file, model, optimizer, lr):\n","    print(\"=> Loading checkpoint\")\n","    checkpoint = torch.load(checkpoint_file, map_location=\"cuda\")\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n","\n","    # If we don't do this then it will just have learning rate of old checkpoint\n","    # and it will lead to many hours of debugging \\:\n","    for param_group in optimizer.param_groups:\n","        param_group[\"lr\"] = lr\n","\n","def seed_everything(seed=42):\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","def generate_examples(gen, steps, truncation=0.7, n=100):\n","    \"\"\"\n","    Tried using truncation trick here but not sure it actually helped anything, you can\n","    remove it if you like and just sample from torch.randn\n","    \"\"\"\n","    gen.eval()\n","    alpha = 1.0\n","    for i in range(n):\n","        with torch.no_grad():\n","            noise = torch.tensor(truncnorm.rvs(-truncation, truncation, size=(1, Z_DIM, 1, 1)), device=DEVICE, dtype=torch.float32)\n","            img = gen(noise, alpha, steps)\n","            save_image(img*0.5+0.5, f\"saved_examples/img_{i}.png\")\n","    gen.train()"],"metadata":{"id":"SFQ-vFXxKMn5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train"],"metadata":{"id":"60dxpKiILdQp"}},{"cell_type":"code","source":["import torch.optim as optim\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","from math import log2\n","from tqdm import tqdm\n","torch.backends.cudnn.benchmark = True\n","def get_loader(image_size):\n","    transform = transforms.Compose(\n","        [\n","            transforms.Resize((image_size, image_size)),\n","            transforms.ToTensor(),\n","            transforms.RandomHorizontalFlip(p=0.5),\n","            transforms.Normalize(\n","                [0.5 for _ in range(CHANNELS_IMG)],\n","                [0.5 for _ in range(CHANNELS_IMG)],\n","            ),\n","        ]\n","    )\n","    batch_size = BATCH_SIZES[int(log2(image_size / 4))]\n","    dataset = datasets.ImageFolder(root=DATASET, transform=transform)\n","    loader = DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=NUM_WORKERS,\n","        pin_memory=True,\n","    )\n","    return loader, dataset\n","\n","\n","def train_fn(\n","    critic,\n","    gen,\n","    loader,\n","    dataset,\n","    step,\n","    alpha,\n","    opt_critic,\n","    opt_gen,\n","    tensorboard_step,\n","    writer,\n","    scaler_gen,\n","    scaler_critic,\n","):\n","    loop = tqdm(loader, leave=True)\n","    for batch_idx, (real, _) in enumerate(loop):\n","        real = real.to(DEVICE)\n","        cur_batch_size = real.shape[0]\n","\n","        # Train Critic: max E[critic(real)] - E[critic(fake)] <-> min -E[critic(real)] + E[critic(fake)]\n","        # which is equivalent to minimizing the negative of the expression\n","        noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(DEVICE)\n","\n","        with torch.cuda.amp.autocast():\n","            fake = gen(noise, alpha, step)\n","            critic_real = critic(real, alpha, step)\n","            critic_fake = critic(fake.detach(), alpha, step)\n","            gp = gradient_penalty(critic, real, fake, alpha, step, device=DEVICE)\n","            loss_critic = (\n","                -(torch.mean(critic_real) - torch.mean(critic_fake))\n","                + LAMBDA_GP * gp\n","                + (0.001 * torch.mean(critic_real ** 2))\n","            )\n","\n","        opt_critic.zero_grad()\n","        scaler_critic.scale(loss_critic).backward()\n","        scaler_critic.step(opt_critic)\n","        scaler_critic.update()\n","\n","        # Train Generator: max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n","        with torch.cuda.amp.autocast():\n","            gen_fake = critic(fake, alpha, step)\n","            loss_gen = -torch.mean(gen_fake)\n","\n","        opt_gen.zero_grad()\n","        scaler_gen.scale(loss_gen).backward()\n","        scaler_gen.step(opt_gen)\n","        scaler_gen.update()\n","\n","        # Update alpha and ensure less than 1\n","        alpha += cur_batch_size / (\n","            (PROGRESSIVE_EPOCHS[step] * 0.5) * len(dataset)\n","        )\n","        alpha = min(alpha, 1)\n","\n","        if batch_idx % 500 == 0:\n","            with torch.no_grad():\n","                fixed_fakes = gen(FIXED_NOISE, alpha, step) * 0.5 + 0.5\n","            plot_to_tensorboard(\n","                writer,\n","                loss_critic.item(),\n","                loss_gen.item(),\n","                real.detach(),\n","                fixed_fakes.detach(),\n","                tensorboard_step,\n","            )\n","            tensorboard_step += 1\n","\n","        loop.set_postfix(\n","            gp=gp.item(),\n","            loss_critic=loss_critic.item(),\n","        )\n","\n","    return tensorboard_step, alpha\n","\n","# initialize gen and disc, note: discriminator should be called critic,\n","# according to WGAN paper (since it no longer outputs between [0, 1])\n","# but really who cares..\n","gen = Generator(\n","    Z_DIM, IN_CHANNELS, img_channels=CHANNELS_IMG\n",").to(DEVICE)\n","critic = Discriminator(\n","    Z_DIM, IN_CHANNELS, img_channels=CHANNELS_IMG\n",").to(DEVICE)\n","\n","# initialize optimizers and scalers for FP16 training\n","opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99))\n","opt_critic = optim.Adam(\n","    critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99)\n",")\n","scaler_critic = torch.cuda.amp.GradScaler()\n","scaler_gen = torch.cuda.amp.GradScaler()\n","\n","# for tensorboard plotting\n","writer = SummaryWriter(f\"logs/gan1\")\n","\n","if LOAD_MODEL:\n","    load_checkpoint(\n","        CHECKPOINT_GEN, gen, opt_gen, LEARNING_RATE,\n","    )\n","    load_checkpoint(\n","        CHECKPOINT_CRITIC, critic, opt_critic,LEARNING_RATE,\n","    )\n","\n","gen.train()\n","critic.train()\n","\n","tensorboard_step = 0\n","# start at step that corresponds to img size that we set in config\n","step = int(log2(START_TRAIN_AT_IMG_SIZE / 4))\n","for num_epochs in PROGRESSIVE_EPOCHS[step:]:\n","    alpha = 1e-5  # start with very low alpha\n","    loader, dataset = get_loader(4 * 2 ** step)  # 4->0, 8->1, 16->2, 32->3, 64 -> 4\n","    print(f\"Current image size: {4 * 2 ** step}\")\n","\n","    for epoch in range(num_epochs):\n","        print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n","        tensorboard_step, alpha = train_fn(\n","            critic,\n","            gen,\n","            loader,\n","            dataset,\n","            step,\n","            alpha,\n","            opt_critic,\n","            opt_gen,\n","            tensorboard_step,\n","            writer,\n","            scaler_gen,\n","            scaler_critic,\n","        )\n","\n","        if SAVE_MODEL:\n","            save_checkpoint(gen, opt_gen, filename=CHECKPOINT_GEN)\n","            save_checkpoint(critic, opt_critic, filename=CHECKPOINT_CRITIC)\n","\n","    step += 1  # progress to the next img size\n","\n"],"metadata":{"id":"uuk4feTjJ_lz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"kvySLQUyMAWW"},"execution_count":null,"outputs":[]}]}