{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNY/YeWasURs8V3xofb0Axs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"5bgGDOUiRAF7"},"outputs":[],"source":["!pip install transformers\n","! pip install git+https://github.com/openai/CLIP.git"]},{"cell_type":"code","source":["from PIL import Image\n","import requests\n","\n","from transformers import CLIPProcessor, CLIPModel\n","\n","model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n","processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n","\n","url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n","image = Image.open(requests.get(url, stream=True).raw)\n","\n","inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n","\n","outputs = model(**inputs)\n","logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n","probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n","print(probs)"],"metadata":{"id":"3aEdpu4YRHou"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import clip\n","import torch\n","from torchvision.datasets import CIFAR100\n","\n","# Load the model\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model, preprocess = clip.load('ViT-B/32', device)\n","\n","# Download the dataset\n","cifar100 = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)\n","\n","# Prepare the inputs\n","image, class_id = cifar100[3637]\n","image_input = preprocess(image).unsqueeze(0).to(device)\n","text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)\n","\n","# Calculate features\n","with torch.no_grad():\n","    image_features = model.encode_image(image_input)\n","    text_features = model.encode_text(text_inputs)\n","\n","# Pick the top 5 most similar labels for the image\n","image_features /= image_features.norm(dim=-1, keepdim=True)\n","text_features /= text_features.norm(dim=-1, keepdim=True)\n","similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n","values, indices = similarity[0].topk(5)\n","\n","# Print the result\n","print(\"\\nTop predictions:\\n\")\n","for value, index in zip(values, indices):\n","    print(f\"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%\")"],"metadata":{"id":"CAC6huimRmKm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import clip\n","from PIL import Image\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model, preprocess = clip.load(\"ViT-B/32\", device=device)\n","\n","image = preprocess(Image.open(\"/content/126517320_1493967970813418_7804454736276769211_n.jpg\")).unsqueeze(0).to(device)\n","text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\n","\n","with torch.no_grad():\n","    image_features = model.encode_image(image)\n","    text_features = model.encode_text(text)\n","    \n","    logits_per_image, logits_per_text = model(image, text)\n","    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n","\n","print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"],"metadata":{"id":"1A9tNvJorG8n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import clip\n","from PIL import Image\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model, preprocess = clip.load(\"ViT-B/32\", device=device)\n","\n","image = preprocess(Image.open(\"/content/126517320_1493967970813418_7804454736276769211_n.jpg\")).unsqueeze(0).to(device)\n","text = clip.tokenize([\"a diagram\", \"girl\", \"white girl\"]).to(device)\n","\n","with torch.no_grad():\n","    image_features = model.encode_image(image)\n","    text_features = model.encode_text(text)\n","    \n","    logits_per_image, logits_per_text = model(image, text)\n","    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n","\n","    # Get the predicted labels for the text\n","    text_labels = torch.argmax(logits_per_text, dim=-1)\n","    print(\"Text labels:\", text_labels.tolist())\n","\n","print(\"Label probs:\", probs)\n","\n"],"metadata":{"id":"MwykFu9Jr6-1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"x71GF3assjBb"},"execution_count":null,"outputs":[]}]}