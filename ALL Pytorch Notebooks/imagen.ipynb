{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"mount_file_id":"1A4ibJzbE6wVjpEtRdTdNwglcjEoT7hdB","authorship_tag":"ABX9TyO0Ws6BpZ+NBnOJ5lEJwwtu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"MByxBWFsx2VA"},"outputs":[],"source":["!pip install imagen-pytorch"]},{"cell_type":"code","source":["import torch\n","from imagen_pytorch import Unet, Imagen\n","\n","# unet for imagen\n","\n","unet1 = Unet(\n","    dim = 32,\n","    cond_dim = 512,\n","    dim_mults = (1, 2, 4, 8),\n","    num_resnet_blocks = 3,\n","    layer_attns = (False, True, True, True),\n","    layer_cross_attns = (False, True, True, True)\n",")\n","\n","unet2 = Unet(\n","    dim = 32,\n","    cond_dim = 512,\n","    dim_mults = (1, 2, 4, 8),\n","    num_resnet_blocks = (2, 4, 8, 8),\n","    layer_attns = (False, False, False, True),\n","    layer_cross_attns = (False, False, False, True)\n",")\n","\n","# imagen, which contains the unets above (base unet and super resoluting ones)\n","\n","imagen = Imagen(\n","    unets = (unet1, unet2),\n","    image_sizes = (64, 256),\n","    timesteps = 1000,\n","    cond_drop_prob = 0.1\n",").cuda()\n","\n","# mock images (get a lot of this) and text encodings from large T5\n","\n","text_embeds = torch.randn(4, 256, 768).cuda()\n","images = torch.randn(4, 3, 256, 256).cuda()\n","\n","# feed images into imagen, training each unet in the cascade\n","\n","for i in (1, 2):\n","    loss = imagen(images, text_embeds = text_embeds, unet_number = i)\n","    loss.backward()\n","\n","# do the above for many many many many steps\n","# now you can sample an image based on the text embeddings from the cascading ddpm\n","\n","images = imagen.sample(texts = [\n","    'a whale breaching from afar',\n","    'young girl blowing out candles on her birthday cake',\n","    'fireworks with blue and green sparkles'\n","], cond_scale = 3.)\n","\n","images.shape # (3, 3, 256, 256)"],"metadata":{"id":"jz3KKkeKyDxv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from imagen_pytorch import Unet, Imagen, SRUnet256, ImagenTrainer\n","\n","# unets for unconditional imagen\n","\n","unet1 = Unet(\n","    dim = 32,\n","    dim_mults = (1, 2, 4),\n","    num_resnet_blocks = 3,\n","    layer_attns = (False, True, True),\n","    layer_cross_attns = False,\n","    use_linear_attn = True\n",")\n","\n","unet2 = SRUnet256(\n","    dim = 32,\n","    dim_mults = (1, 2, 4),\n","    num_resnet_blocks = (2, 4, 8),\n","    layer_attns = (False, False, True),\n","    layer_cross_attns = False\n",")\n","\n","# imagen, which contains the unets above (base unet and super resoluting ones)\n","\n","imagen = Imagen(\n","    condition_on_text = False,   # this must be set to False for unconditional Imagen\n","    unets = (unet1, unet2),\n","    image_sizes = (64, 128),\n","    timesteps = 1000\n",")\n","\n","trainer = ImagenTrainer(imagen).cuda()\n","\n","# now get a ton of images and feed it through the Imagen trainer\n","\n","training_images = torch.randn(4, 3, 256, 256).cuda()\n","\n","# train each unet separately\n","# in this example, only training on unet number 1\n","\n","loss = trainer(training_images, unet_number = 1)\n","trainer.update(unet_number = 1)\n","\n","# do the above for many many many many steps\n","# now you can sample images unconditionally from the cascading unet(s)\n","\n","images = trainer.sample(batch_size = 16) # (16, 3, 128, 128)"],"metadata":{"id":"7aMr78ZhyPxF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.save('checkpoint.pt')\n","\n","trainer.load('checkpoint.pt')\n","\n","trainer.steps # (2,) step number for each of the unets, in this case 2"],"metadata":{"id":"6PsO4_j400-a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import zipfile\n","from pathlib import Path\n","with zipfile.ZipFile('twice/dawin_data.zip','r') as zip_ref:\n","  print('Unzipping data')\n","  zip_ref.extractall()"],"metadata":{"id":"7pL5BwWb2qve"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from imagen_pytorch import Unet, Imagen, ImagenTrainer\n","from imagen_pytorch.data import Dataset\n","\n","# unets for unconditional imagen\n","\n","unet = Unet(\n","    dim = 32,\n","    dim_mults = (1, 2, 4, 8),\n","    num_resnet_blocks = 1,\n","    layer_attns = (False, False, False, True),\n","    layer_cross_attns = False\n",")\n","\n","# imagen, which contains the unet above\n","\n","imagen = Imagen(\n","    condition_on_text = False,  # this must be set to False for unconditional Imagen\n","    unets = unet,\n","    image_sizes = 128,\n","    timesteps = 1000\n",")\n","\n","trainer = ImagenTrainer(\n","    imagen = imagen,\n","    split_valid_from_train = True # whether to split the validation dataset from the training\n",").cuda()\n","\n","# instantiate your dataloader, which returns the necessary inputs to the DDPM as tuple in the order of images, text embeddings, then text masks. in this case, only images is returned as it is unconditional training\n","\n","dataset = Dataset('dawin_data', image_size = 128)\n","\n","trainer.add_train_dataset(dataset, batch_size = 16)\n","\n","# working training loop\n","\n","for i in range(200000):\n","    loss = trainer.train_step(unet_number = 1, max_batch_size = 4)\n","    print(f'loss: {loss}')\n","\n","    if not (i % 50):\n","        valid_loss = trainer.valid_step(unet_number = 1, max_batch_size = 4)\n","        print(f'valid loss: {valid_loss}')\n","\n","    if not (i % 100) and trainer.is_main: # is_main makes sure this can run in distributed\n","        images = trainer.sample(batch_size = 1, return_pil_images = True) # returns List[Image]\n","        images[0].save(f'./sample-{i // 100}.png')\n"],"metadata":{"id":"8IDRZg9616fj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gE2GJOkB2Az4"},"execution_count":null,"outputs":[]}]}