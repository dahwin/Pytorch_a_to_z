{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2a5ace8-971b-4764-8db4-a6f612a35655",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dahyun+darwin = dahwin'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''dahyun+darwin = dahwin'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22c6eb63-5e76-483b-a835-65dd8bbfbd08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy==2.3.4 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (2.3.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy==2.3.4) (1.0.9)\n",
      "Requirement already satisfied: setuptools in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy==2.3.4) (67.6.1)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy==2.3.4) (7.4.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy==2.3.4) (1.24.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy==2.3.4) (0.10.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy==2.3.4) (2.29.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy==2.3.4) (1.1.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy==2.3.4) (0.7.9)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy==2.3.4) (1.0.6)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy==2.3.4) (1.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy==2.3.4) (3.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy==2.3.4) (2.0.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy==2.3.4) (4.65.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.4) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.4) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.4) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.4) (3.1.0)\n",
      "\u001b[33mWARNING: Error parsing requirements for transformers: [Errno 2] No such file or directory: '/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers-4.27.0.dev0.dist-info/METADATA'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy==2.3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8240e334-a96b-4678-97bb-99700359581c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 5.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from en_core_web_sm==2.3.1) (2.3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.9)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.29.0)\n",
      "Requirement already satisfied: setuptools in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (67.6.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.24.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.8)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.10.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.7)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.1.0)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-py3-none-any.whl size=12047087 sha256=f29dce5fbaf6e5ff044beea4c41550e60bdf55f0efc89c7a3b37b17b93f74e0e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-tffcbwvk/wheels/19/d6/1c/5484b95647df5d7afaf74abde458c66c1cd427e69e801fe826\n",
      "Successfully built en-core-web-sm\n",
      "\u001b[33mWARNING: Error parsing requirements for transformers: [Errno 2] No such file or directory: '/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers-4.27.0.dev0.dist-info/METADATA'\u001b[0m\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Attempting uninstall: en-core-web-sm\n",
      "    Found existing installation: en-core-web-sm 3.5.0\n",
      "    Uninstalling en-core-web-sm-3.5.0:\n",
      "      Successfully uninstalled en-core-web-sm-3.5.0\n",
      "Successfully installed en-core-web-sm-2.3.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/en_core_web_sm\n",
      "-->\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n",
      "Collecting de_core_news_sm==2.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.3.0/de_core_news_sm-2.3.0.tar.gz (14.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.9 MB 6.2 MB/s eta 0:00:01    |█████████▋                      | 4.5 MB 6.2 MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from de_core_news_sm==2.3.0) (2.3.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.24.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (0.7.9)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.9)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.29.0)\n",
      "Requirement already satisfied: setuptools in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (67.6.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.0.7)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.6)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (4.65.0)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (7.4.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.4)\n",
      "Building wheels for collected packages: de-core-news-sm\n",
      "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.3.0-py3-none-any.whl size=14907564 sha256=4bb63c15bdf5d05947a1b36cd011b4cf14de86ded42dccc40b8c34c0ef150311\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-wulp5krl/wheels/22/f7/f8/6832015ff86c11653b1d5375139cc1abd0ad0214fe107a0ca1\n",
      "Successfully built de-core-news-sm\n",
      "\u001b[33mWARNING: Error parsing requirements for transformers: [Errno 2] No such file or directory: '/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers-4.27.0.dev0.dist-info/METADATA'\u001b[0m\n",
      "Installing collected packages: de-core-news-sm\n",
      "  Attempting uninstall: de-core-news-sm\n",
      "    Found existing installation: de-core-news-sm 3.5.0\n",
      "    Uninstalling de-core-news-sm-3.5.0:\n",
      "      Successfully uninstalled de-core-news-sm-3.5.0\n",
      "Successfully installed de-core-news-sm-2.3.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('de_core_news_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/de_core_news_sm\n",
      "-->\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/spacy/data/de\n",
      "You can now load the model via spacy.load('de')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en\n",
    "!python -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "782c0dae-87af-4211-befe-8c37a267ce4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.de.German at 0x7f3e48387250>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.load('en_core_web_sm')\n",
    "spacy.load('de_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93c7f9b1-29a8-46b4-9849-b8564f9aaa79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spacy_ger = spacy.load(\"de\")\n",
    "spacy_eng = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c0a672-71aa-4871-b3da-5bca38c34be8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en\n",
    "!python -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc243cd2-977c-4059-9f4c-89e85c45bd38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[0/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['web', 'one', 'one', 'holds', 'holds', 'holds', 'holds', 'holds', 'holds', 'holds', 'holds', 'holds', 'holds', 'holds', 'holds', 'holds', 'holds', 'swim', 'holds', 'holds', 'holds', 'holds', 'holds', 'holds', 'holds', 'holds', 'holds', 'holds', 'holds', 'swim', 'holds', 'holds', 'holds', 'holds', 'holds', 'holds', 'holds', 'holds', 'holds', 'holds', 'holds', 'swim', 'holds', 'holds', 'holds', 'holds', 'holds', 'holds', 'holds', 'holds']\n",
      "epoch[1/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'man', 'is', 'a', 'a', 'a', 'shirt', 'is', 'a', 'a', 'a', '.', 'a', '.', '.', '<eos>']\n",
      "epoch[2/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'young', 'dog', 'is', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<eos>']\n",
      "epoch[3/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'couple', 'is', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
      "epoch[4/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', '<unk>', 'is', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
      "epoch[5/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'skateboarder', 'is', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
      "epoch[6/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', '<unk>', 'is', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
      "epoch[7/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', '<unk>', 'player', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
      "epoch[8/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'baseball', 'player', 'is', 'a', 'a', 'a', 'a', 'a', 'a', 'the', 'ball', '.', '<eos>']\n",
      "epoch[9/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'baseball', 'player', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'the', 'ball', '.', '<eos>']\n",
      "epoch[10/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'street', 'of', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
      "epoch[11/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', '<unk>', 'is', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
      "epoch[12/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'construction', 'worker', 'is', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
      "epoch[13/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', '<unk>', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
      "epoch[14/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'worker', 'in', 'a', 'of', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
      "epoch[15/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', '<unk>', 'with', 'a', 'people', 'in', 'the', 'background', ',', 'a', 'a', '.', '<eos>']\n",
      "epoch[16/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'street', 'vendor', 'with', 'a', 'people', 'in', 'the', 'people', 'is', 'being', 'cheered', 'by', 'the', 'distance', '.', '<eos>']\n",
      "epoch[17/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', '<unk>', 'with', 'a', 'heavily', 'damaged', 'written', 'is', 'a', 'the', 'street', '.', '<eos>']\n",
      "epoch[18/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', '<unk>', 'with', 'a', 'word', 'damaged', 'written', 'is', 'a', 'a', 'a', '.', '<eos>']\n",
      "epoch[19/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'race', 'with', 'a', '<unk>', 'is', 'waiting', 'down', 'the', 'street', '.', '<eos>']\n",
      "epoch[20/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'referee', 'with', 'a', 'heavily', 'damaged', 'written', 'is', 'now', '.', '<eos>']\n",
      "epoch[21/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'race', 'with', 'with', 'a', '<unk>', '<unk>', 'the', 'the', 'the', '.', '<eos>']\n",
      "epoch[22/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'car', 'with', 'a', 'heavily', 'being', 'pulled', 'by', 'the', '.', '<eos>']\n",
      "epoch[23/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'firefighter', 'with', 'a', 'heavily', 'damaged', 'is', 'is', 'a', 'the', '.', '<eos>']\n",
      "epoch[24/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'race', 'with', 'a', 'heavily', 'damaged', ',', 'is', 'being', 'pulled', 'by', 'a', '.', '.', '<eos>']\n",
      "epoch[25/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'race', 'with', 'a', 'heavily', 'damaged', '<unk>', 'is', 'now', 'by', 'a', '.', '.', '<eos>']\n",
      "epoch[26/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'race', 'with', 'a', 'heavily', 'damaged', '<unk>', 'is', 'now', 'through', 'a', '.', '<eos>']\n",
      "epoch[27/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'car', 'car', 'with', 'a', 'people', ',', 'several', 'people', 'walking', 'down', 'a', 'street', '.', '<eos>']\n",
      "epoch[28/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'race', 'car', 'with', 'a', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '.', '<eos>']\n",
      "epoch[29/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'race', 'with', 'a', 'heavily', 'damaged', '<unk>', 'is', 'now', 'by', 'a', '<unk>', '.', '<eos>']\n",
      "epoch[30/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'taxi', 'with', 'with', 'a', '<unk>', 'of', 'people', ',', '<unk>', '<unk>', '.', '<eos>']\n",
      "epoch[31/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'car', 'with', 'a', 'heavily', 'damaged', '<unk>', 'is', 'being', 'pushed', 'by', 'a', 'large', '.', '.', '<eos>']\n",
      "epoch[32/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'race', 'with', 'with', 'two', 'people', ',', 'pulled', 'by', 'a', 'race', '.', '<eos>']\n",
      "epoch[33/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'car', 'with', 'with', 'a', '<unk>', 'in', 'a', 'of', 'a', '<unk>', '<unk>', '.', '<eos>']\n",
      "epoch[34/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'race', 'with', 'a', 'heavily', '<unk>', '<unk>', '<unk>', 'on', 'a', '<unk>', 'track', '.', '<eos>']\n",
      "epoch[35/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'referee', 'with', 'a', 'heavily', '<unk>', '<unk>', 'by', 'a', '<unk>', '<unk>', '.', '<eos>']\n",
      "epoch[36/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'race', 'car', 'with', 'a', '<unk>', 'floats', 'splashes', 'by', 'a', 'race', '.', '<eos>']\n",
      "epoch[37/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'car', 'with', 'a', 'heavily', 'damaged', '<unk>', 'splashes', 'by', 'a', '<unk>', '<unk>', '.', '<eos>']\n",
      "epoch[38/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'race', 'with', 'a', 'number', '<unk>', 'pulled', 'by', 'a', 'dirt', '<unk>', '.', '<eos>']\n",
      "epoch[39/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'referee', 'with', 'a', 'heavily', 'damaged', 'pulled', 'to', '<unk>', 'around', 'a', '<unk>', '.', '<eos>']\n",
      "epoch[40/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'in', 'a', 'heavily', 'being', 'pulled', 'by', 'a', '<unk>', 'of', 'spectators', '.', '<eos>']\n",
      "epoch[41/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'race', 'with', 'a', 'number', 'being', 'pulled', 'by', 'a', 'dirt', '<unk>', 'of', 'a', 'race', '.', '<eos>']\n",
      "epoch[42/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'race', 'with', 'a', 'number', '<unk>', 'pulled', 'by', 'a', '<unk>', '<unk>', '<unk>', '.', '<eos>']\n",
      "epoch[43/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'a', 'number', 'pulled', 'pulled', 'by', 'a', 'race', 'of', 'a', 'fighter', '.', '<eos>']\n",
      "epoch[44/ 60]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'a', 'men', 'being', 'pulled', 'by', 'horses', 'from', 'a', 'livestock', '<unk>', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils import translate_sentence, bleu, save_checkpoint, load_checkpoint\n",
    "\n",
    "spacy_ger = spacy.load(\"de_core_news_sm\")\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def tokenize_ger(text):\n",
    "    return [tok.text for tok in spacy_ger.tokenizer(text)]\n",
    "\n",
    "\n",
    "def tokenize_eng(text):\n",
    "    return [tok.text for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "\n",
    "german = Field(tokenize=tokenize_ger, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "\n",
    "english = Field(\n",
    "    tokenize=tokenize_eng, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\"\n",
    ")\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(\n",
    "    exts=(\".de\", \".en\"), fields=(german, english)\n",
    ")\n",
    "\n",
    "german.build_vocab(train_data, max_size=10000, min_freq=2)\n",
    "english.build_vocab(train_data, max_size=10000, min_freq=2)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        return hidden, cell\n",
    "\n",
    "        \n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,input_size,embedding_size,hidden_size,\n",
    "                output_size,num_layers,dropout):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embedding = nn.Embedding(input_size,embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size,hidden_size,num_layers,dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size,output_size)\n",
    "    def forward(self,x,hidden,cell):\n",
    "        x = x.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        outputs,(hidden,cell) = self.rnn(embedded,(hidden,cell))\n",
    "        predictions = self.fc(outputs)\n",
    "        predictions = predictions.squeeze(0)\n",
    "        return predictions,hidden,cell\n",
    "    \n",
    "        \n",
    "        \n",
    "class SeqSeq(nn.Module):\n",
    "    \n",
    "    def __init__(self,encoder,decoder):\n",
    "        super(SeqSeq,self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    def forward(self,source,target,teacher_force_ratio=0.5):\n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = len(english.vocab)\n",
    "        outputs = torch.zeros(target_len,batch_size,target_vocab_size).to(device)\n",
    "        hidden,cell = self.encoder(source)\n",
    "        # grab the start token\n",
    "        \n",
    "        x = target[0]\n",
    "        for t in range(1,target_len):\n",
    "            output, hidden,cell = self.decoder(x,hidden,cell)\n",
    "            outputs[t] = output\n",
    "            best_guess = output.argmax(1)\n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "        return outputs\n",
    "\n",
    "num_epochs = 60\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "load_model = False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_size_encoder = len(german.vocab)\n",
    "input_size_decoder = len(english.vocab)\n",
    "output_size = len(english.vocab)\n",
    "encoder_embedding_size = 300\n",
    "decoder_embedding_size = 300\n",
    "hidden_size = 1024\n",
    "num_layers = 4\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data,valid_data,test_data),\n",
    "    batch_size = batch_size,\n",
    "    sort_within_batch= True,\n",
    "    sort_key = lambda x : len(x.src),\n",
    "    device= device)\n",
    "\n",
    "encoder_net = Encoder(input_size_encoder,encoder_embedding_size,\n",
    "                      hidden_size,num_layers,enc_dropout).to(device)\n",
    "decoder_net = Decoder(input_size_decoder,decoder_embedding_size,\n",
    "                      hidden_size,output_size,num_layers,dec_dropout).to(device)\n",
    "\n",
    "model = SeqSeq(encoder_net,decoder_net).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "pad_idx = english.vocab.stoi['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "step = 0\n",
    "if load_model :\n",
    "    load_checkpoint(torch.load('my_checkpoint.pth.ptar'),model,optimizer)\n",
    "sentence = \"ein boot mit mehreren männern darauf wird von einem großen pferdegespann ans ufer gezogen.\"\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'epoch[{epoch}/ {num_epochs}]')\n",
    "    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "    save_checkpoint(checkpoint)\n",
    "    model.eval()\n",
    "\n",
    "    translated_sentence = translate_sentence(\n",
    "        model, sentence, german, english, device, max_length=50\n",
    "    )\n",
    "\n",
    "    print(f\"Translated example sentence: \\n {translated_sentence}\")\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        inp_data = batch.src.to(device)\n",
    "        target = batch.trg.to(device)\n",
    "        output = model(inp_data,target)\n",
    "        output = output[1:].reshape(-1,output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output,target)\n",
    "        # Back prop\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip to avoid exploding gradient issues, makes sure grads are\n",
    "        # within a healthy range\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Gradient descent step\n",
    "        optimizer.step()\n",
    "        step +=1\n",
    "\n",
    "score = bleu(test_data[1:100], model, german, english, device)\n",
    "print(f\"Bleu score {score*100:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b0afad-676e-4ec7-bb74-a7debd36bc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c036ca82-0d65-4f7c-b5e5-2868340e196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5160ed-9e09-4d4f-a2e7-3d182a6df484",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
