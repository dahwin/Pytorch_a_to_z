{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"mount_file_id":"1Fe-J-5o6m77wbQg2cjsc0uZXvx1ZiuvK","authorship_tag":"ABX9TyOVOXN/SbMUdoCowYmTXIha"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"KmluBFnnwJHP"},"outputs":[],"source":["'dahyun+darwin = dahwin'\n"]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"TvZe50dzwQM8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","import torch\n","\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","model = GPT2LMHeadModel.from_pretrained(\"model_state.pt\")\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model.to(device)\n","\n","def generate_conversation(inp):\n","    inp = \"<startofstring> \"+inp+\" <bot>: \"\n","    inp = tokenizer(inp, return_tensors=\"pt\")\n","    input_ids = inp[\"input_ids\"].to(device)\n","    attention_mask = inp[\"attention_mask\"].to(device)\n","    output = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n","    return tokenizer.decode(output[0], skip_special_tokens=True)\n","\n","print(generate_conversation(\"hello\"))\n","\n"],"metadata":{"id":"EECL1vkVwmCj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"IExzihPfw28C"},"execution_count":null,"outputs":[]}]}