{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"mount_file_id":"1tsykwHM4zVL6BN0vcC3zEGl-qSUS7Btc","authorship_tag":"ABX9TyOwPDOiwdZAerPgmcCoVxi4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","source":["'dahyun+darwin = dahwin'"],"metadata":{"id":"kow0gxjPpgIP"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iUtOsnBnlA5E"},"outputs":[],"source":["!pip install transformers accelerate"]},{"cell_type":"code","source":["from transformers import GPT2LMHeadModel,GPT2Tokenizer\n","from torch.optim import Adam\n","from torch.utils.data import DataLoader\n","import tqdm\n","import torch"],"metadata":{"id":"UfgYCPZ0lpyK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","# model = GPT2LMHeadModel.from_pretrained('gpt2')\n","# print(model.generate(tokenizer('I love dahyun',return_tensors='pt')))"],"metadata":{"id":"ZYP9GgqCmIuf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","model = GPT2LMHeadModel.from_pretrained('gpt2')\n","input_text = \"I love to eat\"\n","input_ids = tokenizer.encode(input_text, return_tensors='pt')\n","output = model.generate(input_ids)\n","generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n","print(generated_text)\n"],"metadata":{"id":"pQ2u6tEDpnUW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time \n","s = time.time()\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","model = GPT2LMHeadModel.from_pretrained('gpt2')\n","input_text = \"I love to eat\"\n","input_ids = tokenizer.encode(input_text, return_tensors='pt')\n","output = model.generate(\n","    input_ids,\n","    max_length=100,\n","    num_return_sequences=3,\n","    no_repeat_ngram_size=2,\n","    repetition_penalty=1.5,\n","    do_sample=True,\n","    top_k=50,\n","    top_p=0.95,\n","    temperature=0.9\n",")\n","generated_text = []\n","for seq in output:\n","    generated_text.append(tokenizer.decode(seq, skip_special_tokens=True))\n","    \n","generated_text_cleaned = []\n","for text in generated_text:\n","    cleaned_text = text.replace('\\n', '').replace('\\r', '').replace('\\t', '')\n","    generated_text_cleaned.append(cleaned_text)\n","    \n","print(generated_text_cleaned)\n","\n","e = time.time()\n","print(e-s)\n"],"metadata":{"id":"wml80zLTp-yF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time \n","s = time.time()\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","model = GPT2LMHeadModel.from_pretrained('gpt2')\n","input_text = \"I love to eat\"\n","import torch\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","input_ids = input_ids.to(device)\n","output = model.generate(\n","    input_ids,\n","    max_length=100,\n","    num_return_sequences=3,\n","    no_repeat_ngram_size=2,\n","    repetition_penalty=1.5,\n","    do_sample=True,\n","    top_k=50,\n","    top_p=0.95,\n","    temperature=0.9\n",")\n","\n","generated_text = []\n","for seq in output:\n","    generated_text.append(tokenizer.decode(seq, skip_special_tokens=True))\n","    \n","generated_text_cleaned = []\n","for text in generated_text:\n","    cleaned_text = text.replace('\\n', '').replace('\\r', '').replace('\\t', '')\n","    generated_text_cleaned.append(cleaned_text)\n","    \n","print(generated_text_cleaned)\n","e = time.time()\n","print(e-s)"],"metadata":{"id":"6URZfNz7rMPD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","import json\n","\n","class ChatData(Dataset):\n","    def __init__(self, path:str, tokenizer):\n","        self.data = json.load(open(path, \"r\"))\n","\n","        self.X = []\n","        for i in self.data:\n","            for j in i['dialog']:\n","                self.X.append(j['text'])\n","\n","        for idx, i in enumerate(self.X):\n","            try:\n","                self.X[idx] = \"<startofstring> \"+i+\" <bot>: \"+self.X[idx+1]+\" <endofstring>\"\n","            except:\n","                break\n","\n","        self.X = self.X[:5000]\n","        \n","        print(self.X[0])\n","\n","        self.X_encoded = tokenizer(self.X,max_length=40, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n","        self.input_ids = self.X_encoded['input_ids']\n","        self.attention_mask = self.X_encoded['attention_mask']\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        return (self.input_ids[idx], self.attention_mask[idx])"],"metadata":{"id":"H_hyAxu9ujXd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.add_special_tokens({\"pad_token\": \"<pad>\", \n","                                \"bos_token\": \"<startofstring>\",\n","                                \"eos_token\": \"<endofstring>\"})\n","tokenizer.add_tokens([\"<bot>:\"])\n","chatdata = ChatData('chat_data.json',tokenizer)\n"],"metadata":{"id":"_USouEQ8-NkO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","\n","from torch.optim import Adam\n","from torch.utils.data import DataLoader\n","import tqdm\n","import torch\n","\n","def train(chatData, model, optim):\n","\n","    epochs = 12\n","\n","    for i in tqdm.tqdm(range(epochs)):\n","        for X, a in chatData:\n","            X = X.to(device)\n","            a = a.to(device)\n","            optim.zero_grad()\n","            loss = model(X, attention_mask=a, labels=X).loss\n","            loss.backward()\n","            optim.step()\n","        torch.save(model.state_dict(), \"model_state.pt\")\n","        print(infer(\"hello how are you\"))\n","\n","def infer(inp):\n","    inp = \"<startofstring> \"+inp+\" <bot>: \"\n","    inp = tokenizer(inp, return_tensors=\"pt\")\n","    X = inp[\"input_ids\"].to(device)\n","    a = inp[\"attention_mask\"].to(device)\n","    output = model.generate(X, attention_mask=a )\n","    output = tokenizer.decode(output[0])\n","    return output\n","\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n","\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","tokenizer.add_special_tokens({\"pad_token\": \"<pad>\", \n","                                \"bos_token\": \"<startofstring>\",\n","                                \"eos_token\": \"<endofstring>\"})\n","tokenizer.add_tokens([\"<bot>:\"])\n","\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","model.resize_token_embeddings(len(tokenizer))\n","\n","model = model.to(device)\n","\n","# print(tokenizer.decode(model.generate(**tokenizer(\"hey i was good at basketball but \",\n","#                          return_tensors=\"pt\"))[0]))\n","\n","chatData = ChatData(\"chat_data.json\", tokenizer)\n","chatData =  DataLoader(chatData, batch_size=64)\n","\n","model.train()\n","\n","optim = Adam(model.parameters(), lr=1e-3)\n","\n","print(\"training .... \")\n","train(chatData, model, optim)\n","\n","print(\"infer from model : \")\n","while True:\n","  inp = input()\n","  print(infer(inp))"],"metadata":{"id":"JUULF-c_u0dO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# <startofstring> I Love Dahyun! More than my life! She is my girlfriend! <bot>: It's great to hear \n","# that you love your girlfriend Dahyun so much! It's important to cherish and appreciate our loved ones.<endofstring>\n"],"metadata":{"id":"Pu0qJazE4QnP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.optim import Adam\n","from torch.utils.data import DataLoader\n","import tqdm\n","import torch\n","\n","def train(chatData, model, optim):\n","\n","    epochs = 12\n","\n","    for i in tqdm.tqdm(range(epochs)):\n","        for X, a in chatData:\n","            X = X.to(device)\n","            a = a.to(device)\n","            optim.zero_grad()\n","            loss = model(X, attention_mask=a, labels=X).loss\n","            loss.backward()\n","            optim.step()\n","        model.save_pretrained(\"model\")\n","        print(infer(\"hello how are you\"))\n","\n","def infer(inp):\n","    inp = \"<startofstring> \"+inp+\" <bot>: \"\n","    inp = tokenizer(inp, return_tensors=\"pt\")\n","    X = inp[\"input_ids\"].to(device)\n","    a = inp[\"attention_mask\"].to(device)\n","    output = model.generate(X, attention_mask=a )\n","    output = tokenizer.decode(output[0])\n","    return output\n","\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n","\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","tokenizer.add_special_tokens({\"pad_token\": \"<pad>\", \n","                                \"bos_token\": \"<startofstring>\",\n","                                \"eos_token\": \"<endofstring>\"})\n","tokenizer.add_tokens([\"<bot>:\"])\n","\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","model.resize_token_embeddings(len(tokenizer))\n","\n","model = model.to(device)\n","\n","# print(tokenizer.decode(model.generate(**tokenizer(\"hey i was good at basketball but \",\n","#                          return_tensors=\"pt\"))[0]))\n","\n","chatData = ChatData(\"chat_data.json\", tokenizer)\n","chatData =  DataLoader(chatData, batch_size=64)\n","\n","model.train()\n","\n","optim = Adam(model.parameters(), lr=1e-3)\n","\n","print(\"training .... \")\n","train(chatData, model, optim)\n","\n","print(\"infer from model : \")\n","while True:\n","  inp = input()\n","  print(infer(inp))\n"],"metadata":{"id":"f-SfkDx70kx6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","# replace with your file path\n","file_path = \"model/pytorch_model.bin\"\n","\n","# get size of file in bytes\n","size_bytes = os.path.getsize(file_path)\n","\n","# convert to megabytes\n","size_mb = size_bytes / (1024 * 1024)\n","\n","print(f\"File size: {size_mb:.2f} MB\")\n"],"metadata":{"id":"qMv4BmWr1FPp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","import torch\n","from transformers import GPT2Config\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","model_config_path = \"model/config.json\"\n","model_weights_path = \"model/pytorch_model.bin\"\n","\n","model_config = GPT2Config.from_json_file(model_config_path)\n","model = GPT2LMHeadModel(config=model_config)\n","model.load_state_dict(torch.load(model_weights_path, map_location=torch.device('cpu')))\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model.to(device)\n","\n","def generate_conversation(inp):\n","    inp = \"<startofstring> \"+inp+\" <bot>: \"\n","    inp = tokenizer(inp, return_tensors=\"pt\")\n","    input_ids = inp[\"input_ids\"].to(device)\n","    attention_mask = inp[\"attention_mask\"].to(device)\n","    output = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n","    output = output.flatten().tolist()\n","    output = [t for t in output if t is not None and tokenizer.decode([t]).strip() not in tokenizer.all_special_tokens]\n","    return tokenizer.decode(output, skip_special_tokens=True)\n","\n","\n","print(generate_conversation(\"what you like to do?\"))"],"metadata":{"id":"-MxeemlLpDPK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","import torch\n","from transformers import GPT2Config\n","\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","model_config_path = \"model/config.json\"\n","model_weights_path = \"model/pytorch_model.bin\"\n","\n","model_config = GPT2Config.from_json_file(model_config_path)\n","model = GPT2LMHeadModel(config=model_config)\n","model.load_state_dict(torch.load(model_weights_path, map_location=torch.device('cpu')))\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model.to(device)\n","\n","def generate_conversation(prompt, max_length):\n","    prompt = \"<startofstring> \" + prompt + \": \"\n","    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n","\n","    mask = [1 for _ in range(max_length)] * len(input_ids)\n","    input_ids = input_ids[:max_length - 1].unsqueeze(-1)  # remove eos token & pad\n","    attention_mask = torch.cat((1 - torch.tensor(mask)[:max_length - 1], torch.zeros(len(input_ids)).bool()), dim=-1).float().unsqueeze(-1)\n","\n","    generated = \"\"\n","    while True:\n","        all_outs = []\n","        try:\n","            logits, _, _ = model(input_ids, attention_mask=attention_mask)\n","            predictions = logits[:, :-1, :]\n","            top_preds, top_scores = [], []\n","            for pred_index in range(len(predictions)):\n","                score = abs(predictions[:, pred_index, :])\n","                top_preds.append(list(range(num_choices)[::-1][score > thresh]))\n","                top_scores.append(max(score[-2:][top_preds[pred_index]]))\n","            chosen_pred = top_preds[0][sample(*top_scores[0])]\n","            generated += tokenizer.convert_ids_to_str([generated[:-1]] + [next(sorted(set(predicted_toks)), key=(lambda x: random()))])\n","            tokenized = tokenizer.batch_encode([generated])[0]\n","\n","            input_ids = tokenized[:max_length - 1]\n","            attention_mask = torch.tensor([tokenizer.convert_tokens_to_ids(gen) == tokenizer.convert_tokens_to_ids(encoded_prompts) for gen in generated for encoded_prompts in generated[:generations]*2+1], dtype=torch.int64)\n","            attention_mask = attention_mask / 2**len(input_ids)*2 - 0.9\n","        except RuntimeError as e:\n","            break\n","    return generated\n","\n","\n","print(generate_conversation(\"what you like to do?\", max_length=50))\n","\n"],"metadata":{"id":"olayZb1hqjLQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_conversation(prompt, max_length):\n","    ...\n","    # Generate responses up to the specified max_length or until the end-of-sentence token\n","    response = []\n","    while len(response) < max_length and '<unk>' not in input_ids:\n","        batched_inputs, _ = model. generator(input_ids=input_ids[:max_length], attention_mask=attention_mask[:max_length])\n","        response += batched_inputs[:, :, -1].numpy()\n","        input_ids, attention_mask = zip(*batched_inputs)\n","\n","    ..."],"metadata":{"id":"iJPuXDGer2Dd"},"execution_count":null,"outputs":[]}]}