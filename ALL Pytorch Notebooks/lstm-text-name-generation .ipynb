{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-23T08:20:35.504459Z","iopub.execute_input":"2023-04-23T08:20:35.505473Z","iopub.status.idle":"2023-04-23T08:20:35.513923Z","shell.execute_reply.started":"2023-04-23T08:20:35.505416Z","shell.execute_reply":"2023-04-23T08:20:35.512651Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"/kaggle/input/us-baby-names/StateNames.csv\n/kaggle/input/us-baby-names/NationalReadMe.pdf\n/kaggle/input/us-baby-names/hashes.txt\n/kaggle/input/us-baby-names/NationalNames.csv\n/kaggle/input/us-baby-names/StateReadMe.pdf\n/kaggle/input/us-baby-names/database.sqlite\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport string\nimport random\nimport sys\nimport unidecode\nimport pandas as pd\nfrom torch.utils.tensorboard import SummaryWriter\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nall_characters = string.printable\nn_characters = len(all_characters)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-23T08:20:35.515416Z","iopub.execute_input":"2023-04-23T08:20:35.516331Z","iopub.status.idle":"2023-04-23T08:20:35.524852Z","shell.execute_reply.started":"2023-04-23T08:20:35.516290Z","shell.execute_reply":"2023-04-23T08:20:35.524059Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.get_device_name(0)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-23T08:20:35.526183Z","iopub.execute_input":"2023-04-23T08:20:35.527288Z","iopub.status.idle":"2023-04-23T08:20:35.536135Z","shell.execute_reply.started":"2023-04-23T08:20:35.527260Z","shell.execute_reply":"2023-04-23T08:20:35.535024Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'Tesla P100-PCIE-16GB'"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\n\n# Read the CSV file using pandas\ndf = pd.read_csv('/kaggle/input/us-baby-names/StateNames.csv')\n\n# Extract the \"Name\" column as a list of strings\nnames = df['Name']\n\n# Write the names to a text file\nwith open('names.txt', 'w') as f:\n    f.write('\\n'.join(names))\n","metadata":{"execution":{"iopub.status.busy":"2023-04-23T08:20:35.537701Z","iopub.execute_input":"2023-04-23T08:20:35.538178Z","iopub.status.idle":"2023-04-23T08:20:38.868008Z","shell.execute_reply.started":"2023-04-23T08:20:35.538101Z","shell.execute_reply":"2023-04-23T08:20:38.866478Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"\n\n# Read large text file (Note can be any text file: not limited to just names)\nfile = unidecode.unidecode(open(\"names.txt\").read())","metadata":{"execution":{"iopub.status.busy":"2023-04-23T08:20:38.870050Z","iopub.execute_input":"2023-04-23T08:20:38.870746Z","iopub.status.idle":"2023-04-23T08:20:38.962240Z","shell.execute_reply.started":"2023-04-23T08:20:38.870705Z","shell.execute_reply":"2023-04-23T08:20:38.961212Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class RNN(nn.Module):\n    def __init__(self,input_size,hidden_size,num_layers,output_size):\n        super(RNN,self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        self.embed = nn.Embedding(input_size,hidden_size)\n        self.lstm = nn.LSTM(hidden_size,hidden_size,num_layers,batch_first=True)\n        self.fc = nn.Linear(hidden_size,output_size)\n    def forward(self,x,hidden,cell):\n        out = self.embed(x)\n        out,(hidden,cell) = self.lstm(out.unsqueeze(1),(hidden,cell))\n        out = self.fc(out.reshape(out.shape[0],-1))\n        return out, (hidden,cell) # add return statement to output the result of the linear layer\n    def init_hidden(self,batch_size):\n        hidden = torch.zeros(self.num_layers,batch_size,self.hidden_size).to(device)\n        cell = torch.zeros(self.num_layers,batch_size,self.hidden_size).to(device)\n        return hidden,cell\n\n        \n\nclass Generator():\n    def __init__(self):\n        self.chunk_len = 250\n        self.num_epochs = 5000\n        self.batch_size = 1\n        self.print_every = 50\n        self.hidden_size = 256\n        self.num_layers = 2\n        self.lr = 0.003\n    def char_tensor(self,string):\n        tensor = torch.zeros(len(string)).long()\n        for c in range(len(string)):\n            tensor[c] = all_characters.index(string[c])\n        return tensor\n    def get_random_batch(self):\n        start_idx = random.randint(0, len(file) - self.chunk_len)\n        end_idx = start_idx + self.chunk_len + 1\n        text_str = file[start_idx:end_idx]\n        text_input = torch.zeros(self.batch_size, self.chunk_len)\n        text_target = torch.zeros(self.batch_size, self.chunk_len)\n\n        for i in range(self.batch_size):\n            text_input[i, :] = self.char_tensor(text_str[:-1])\n            text_target[i, :] = self.char_tensor(text_str[1:])\n\n        return text_input.long(), text_target.long()\n\n\n    \n        \n    \n    def generate(self,initial_str ='A',predic_len=100,temperature=0.85):\n        hidden , cell = self.rnn.init_hidden(batch_size = self.batch_size)\n        initial_input = self.char_tensor(initial_str)\n        predicted = initial_str \n        for p in range(len(initial_str)-1):\n            _, (hidden,cell) = self.rnn(initial_input[p].view(1).to(device),hidden,cell)\n        \n        last_char = initial_input[-1]\n        for p in range(predic_len):\n            output , (hidden,cell) = self.rnn(last_char.view(1).to(device),hidden,cell)\n            output_dist = output.data.view(-1).div(temperature).exp()\n            top_char = torch.multinomial(output_dist,1)[0]\n            \n            predicted_char = all_characters[top_char]\n            predicted += predicted_char\n            last_char = self.char_tensor(predicted_char)\n        return predicted\n            \n\n    def train(self):\n        self.rnn = RNN(n_characters,self.hidden_size,self.num_layers,n_characters).to(device)\n        optimizer = torch.optim.Adam(self.rnn.parameters(),lr= self.lr)\n        criterion = nn.CrossEntropyLoss()\n        writer = SummaryWriter(f'run/names0')\n        print('=> Starting training')\n        \n        for epoch in range(1,self.num_epochs +1):\n            inp , target = self.get_random_batch()\n            hidden , cell = self.rnn.init_hidden(batch_size = self.batch_size)\n            self.rnn.zero_grad()\n            loss = 0\n            inp= inp.to(device)\n            target = target.to(device)\n            for c in range(self.chunk_len):\n                output , (hidden, cell) = self.rnn(inp[:,c],hidden,cell)\n                loss += criterion(output,target[:,c])\n            loss.backward()\n            optimizer.step()\n            loss = loss.item() / self.chunk_len\n            if epoch % self.print_every == 0:\n                \n                print(f'Loss:{loss}')\n                print(self.generate())\n            writer.add_scalar(\"Training loss\",loss,global_step=epoch)\n\n            \ngen = Generator()\ngen.train()","metadata":{"execution":{"iopub.status.busy":"2023-04-23T08:20:38.963943Z","iopub.execute_input":"2023-04-23T08:20:38.964307Z","iopub.status.idle":"2023-04-23T08:21:30.675709Z","shell.execute_reply.started":"2023-04-23T08:20:38.964270Z","shell.execute_reply":"2023-04-23T08:21:30.673430Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"=> Starting training\nLoss:2.508753173828125\nAMetim\nArinia\nHardha\nRetretta\nKlan\nKrrene\nCatebina\nHlire\nMasthal\ntanee\nMeri\nSonay\nMilalo\nFane\nAngey\nD\nLoss:2.262809814453125\nAmon\nCuna\nEmy\nCard\nAeben\nRepQin\nKify\nKacina\nMana\nShya\nJoliy\nJuston\nKevin\nJaris\nAlevan\nSarid\nErine\nGka\nLoss:2.36035498046875\nArxe\nBrelin\nBaranie\nJavrinne\nMansan\nJooni\nKarik\nJan\nJary\nLewer\nMarvey\nClather\nEdres\nIgesa\nDawian\nVay\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_24/1886845233.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_24/1886845233.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[1;32m    487\u001b[0m         torch.autograd.backward(\n\u001b[0;32m--> 488\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m         )\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m def grad(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}