{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNOSKUC/yh65rDT/mhZZXLm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# dahyun+darwin = dahwin"],"metadata":{"id":"x1T7SGwGJFR_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZgTHJw32I_uN"},"outputs":[],"source":["import torch\n","from torch import *\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import *\n","from torch import nn\n","from sklearn import datasets\n","# x = dir(datasets)\n","# print(\"\\n\".join(x))"]},{"cell_type":"markdown","source":["# Make calssifiction data and get it ready"],"metadata":{"id":"ewhWg9bULiJz"}},{"cell_type":"code","source":["# make 1000 saples\n","n_sample = 1000\n","\n","# create circles\n","x,y = make_circles(n_sample,\n","                    noise=0.03,\n","                   random_state=42\n","                    )"],"metadata":{"id":"hmaHPHd8LYq-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(x),len(y)"],"metadata":{"id":"zaxYRwJGL-cK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'First 5 samples of x:\\n{x[:10]}')\n","print(f\"First 5 samples of y:\\n{y[:10]}\")"],"metadata":{"id":"g_0FyPihMI45"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Make DataFarame of circle data\n","import pandas as pd\n","circles = pd.DataFrame({'x1':x[:,0],\n","                       'x2':x[:,1],\n","                       'label':y})\n","circles.head(10)"],"metadata":{"id":"NQJCPE3JNjXg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.scatter(x=x[:,0],\n","              y=x[:,1],\n","              c= y,\n","              cmap=plt.cm.RdYlBu)"],"metadata":{"id":"4rCv7FhANvlX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **1.1 Check input and output shapes**"],"metadata":{"id":"Aw4kF_ckRPl4"}},{"cell_type":"code","source":["x.shape,y.shape"],"metadata":{"id":"g8Uugcv5PTie"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y[0]"],"metadata":{"id":"3ZVgF8OPgjDl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x"],"metadata":{"id":"YdKy7QAkRhKn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# view the first example of features and labels\n","x_sample = x[0]\n","y_sample = y[0]\n","print(f\"values for one sample of x:{x_sample} and the same for y:{y_sample}\")\n","print(f\"shapes for one sample of x:{x_sample.shape} and the same for y:{y_sample.shape}\")"],"metadata":{"id":"R6QZ8HZZRmoo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1.2 Turn data into tensors and create train and test splits"],"metadata":{"id":"1k-pyabPTmX6"}},{"cell_type":"code","source":["type(x)"],"metadata":{"id":"GeVDURJHTHG3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = torch.from_numpy(x).type(torch.float)\n","y = torch.from_numpy(y).type(torch.float)\n","x[:5] , y[:5]"],"metadata":{"id":"bi25u7IAUiC6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(x)"],"metadata":{"id":"hRLzBXm1VFlD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split data into train and test sets\n","from sklearn.model_selection import train_test_split\n","\n","x_train, x_test, y_train, y_test = train_test_split(x, \n","                                                    y, \n","                                                    test_size=0.2, # 20% test, 80% train\n","                                                    random_state=42) # make the random split reproducible\n","\n","len(x_train), len(x_test), len(y_train), len(y_test)"],"metadata":{"id":"4MBn-J36VWUo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **2. Building a model**\n","\n","Let's build a model to clssify our blue and red dots.\n","To do so , we wan to :\n","1. Setup device agnoistic\n","2. Construct a model (by subclassing nn.Module)\n","3. Define a loss fuction and optimizer\n","4. Create a trainng and test loop"],"metadata":{"id":"rciWw6wFYEfz"}},{"cell_type":"code","source":["# make device agnostic code\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device"],"metadata":{"id":"kMPZm08cXqus"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Now we've setup device agonstic code , let's create a model that:\n","1. subclass nn.Modeule(almost all models in pytorch subclass nn.modeule)\n","2. create 2 nn.Linear() layers that are capable of handiling the shapes of our data\n","3. defines a forward() method that outlilnes the foirward pass (or forwrd computation) 0f the model\n","4. Instatite an instance of our model calss and send it to the target device"],"metadata":{"id":"IYJuzr8ieDhf"}},{"cell_type":"code","source":["x_train.shape"],"metadata":{"id":"8nGall-ldZKi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train[0]"],"metadata":{"id":"aSh514owe4wH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. costruct a model that subclass nn.Module\n","class CircleModelV0(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.layer_1= nn.Linear(in_features=2,out_features=5)\n","    self.layer_2 = nn.Linear(in_features=5,out_features=1)\n","  def forward(self,x):\n","    return self.layer_2(self.layer_1(x)) # x-> layer_1 -> output\n","\n","model_0 = CircleModelV0().to(device)\n","model_0"],"metadata":{"id":"9M_HabxofcFw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's replicate the model above using nn.Sequential()\n","model_0 = nn.Sequential(\n","    nn.Linear(in_features= 2,out_features=5,),\n","    nn.Linear(in_features=5,out_features=1)\n",").to(device)\n","model_0"],"metadata":{"id":"8Mgbkui48-zE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_0.state_dict()"],"metadata":{"id":"ED0N0LPTCvrZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.inference_mode():\n","\n","    untrained_pre = model_0(x_test.to(device))\n","print(f'Length of predictions:{len(untrained_pre)},shape:{untrained_pre.shape}')\n","print(f\"Length of test samples:{len(x_test)},shape:{x_test.shape}\")\n","print(f\"\\nFirst 10 predictions:\\n{torch.round(untrained_pre[:10])}\")\n","print(f\"\\nFirst 10 labels:\\n{y_test[:10]}\")"],"metadata":{"id":"ZyOhoK69DhIH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_test[:10],y_test[:10]"],"metadata":{"id":"BvkdP6VTF8km"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# set up\n","but which one should we use?\n","For example for regression you might want to use MAE or MSE ( mean absolute error or mean squard error)\n","\n","For example for classicfication you might want to use binary cross entropy or categorical cross entropy (cross entropy)\n","\n","As a reminder , the loss fucion mesures how wrong you model's predictions are\n","\n","\n","\n","*   For some common choices of loss fuction and optimizer : https://www.learnpytorch.io/02_pytorch_classification/#21-setup-loss-function-and-optimizer\n","\n","*   For the loss fuction we're going to use torch.nn.BECWithLogitaLoss(),for more on what binary coross entropy(BCE) is\n","*  For a definition of logit in deep learning\n","\n","\n","*   For different optimizers see torch.optim\n","\n","\n","\n","\n"],"metadata":{"id":"mBwT5yOLIKwq"}},{"cell_type":"code","source":["# Create a loss function\n","# loss_fn = nn.BCELoss() # BCELoss = no sigmoid built-in\n","loss_fn = nn.BCEWithLogitsLoss() # BCEWithLogitsLoss = sigmoid built-in\n","\n","# Create an optimizer\n","optimizer = torch.optim.SGD(params=model_0.parameters(), \n","                            lr=0.1)\n"],"metadata":{"id":"Sl-hKsaUGSVc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_0.state_dict()"],"metadata":{"id":"67QBcw66Mtgz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate accuracy (a classification metric)\n","def accuracy_fn(y_true, y_pred):\n","    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n","    acc = (correct / len(y_pred)) * 100 \n","    return acc"],"metadata":{"id":"mljHRxRyMxCR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. Train Model\n","To train our model , we're going to need to build a training lop with the following steps\n","1. Forward\n","2. Calulate the loss\n","3. Optimizer zero grad\n","4. Loss backword (backpropagation)\n","5. Opimizer stp (gradient desent)\n"],"metadata":{"id":"HirFmuplGi80"}},{"cell_type":"markdown","source":["### 3.1 Going from raw logits-> prediction probabilities -> prediction labels\n","our model ouputs are going to be raw **logits**\n","\n","we can convert these **logits** into prediction probabilites by passing them \n","to some kinde of activation fuction(e.g  sigmoid for binary classificaiton and softmax for multicalss classification)\n","Then we can convert our model's prediction probabilities to prediction labels by either rounding them or taking the argmax()"],"metadata":{"id":"f-wzJp5TJcAP"}},{"cell_type":"code","source":["# View the first 5 ouputs of the forward pas on the test data\n","model_0.eval()\n","with torch.inference_mode():\n","  y_logits = model_0(x_test.to(device))[:5]\n","y_logits"],"metadata":{"id":"IQ63GuW8Nrsf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_test[:5]"],"metadata":{"id":"cgYUU_BkJ82A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use the sigmoid activation fuction on our model logits to turn them into prediction probabilities\n","y_pred_pro = torch.sigmoid(y_logits)\n","y_pred_pro"],"metadata":{"id":"V5FHyo4PLl8t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# For our prediction probability values, we need to perform a range style rounding on them:\n","\n","\n","*   y_pred_pro >=0.5 y=1(class 1)\n","*   y_pred_pro <=0 , y=0 (class 0)\n","\n","\n"],"metadata":{"id":"lVtcFoNkMJyK"}},{"cell_type":"code","source":["# Find the predicted labels \n","y_pre = torch.round(y_pred_pro)\n","y_pre_labels = torch.round(torch.sigmoid(model_0(x_test.to(device))[:5]))\n","# check for equalitiy\n","print(torch.eq(y_pre.squeeze(),y_pre_labels.squeeze()))\n","y_pre.squeeze()"],"metadata":{"id":"R56EdPY_L_gd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(42)\n","\n","# Set the number of epochs\n","\n","\n","epochs = 10  # Define the number of epochs to run\n","\n","# Put data to target device\n","X_train, y_train = x_train.to(device), y_train.to(device)\n","X_test, y_test = x_test.to(device), y_test.to(device)\n","\n","# Build training and evaluation loop\n","for epoch in range(0,100):\n","    ### Training\n","    model_0.train()\n","\n","    # 1. Forward pass (model outputs raw logits)\n","    y_logits = model_0(X_train).squeeze() # squeeze to remove extra `1` dimensions, this won't work unless model and data are on same device \n","    y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -> pred probs -> pred labls\n","  \n","    # 2. Calculate loss/accuracy\n","    # loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()\n","    #                y_train) \n","    loss = loss_fn(y_logits, # Using nn.BCEWithLogitsLoss works with raw logits\n","                   y_train) \n","    acc = accuracy_fn(y_true=y_train, \n","                      y_pred=y_pred) \n","\n","    # 3. Optimizer zero grad\n","    optimizer.zero_grad()\n","\n","    # 4. Loss backwards\n","    loss.backward()\n","\n","    # 5. Optimizer step\n","    optimizer.step()\n","\n","    ### Testing\n","    model_0.eval()\n","    with torch.inference_mode():\n","        # 1. Forward pass\n","        test_logits = model_0(X_test).squeeze() \n","        test_pred = torch.round(torch.sigmoid(test_logits))\n","        # 2. Caculate loss/accuracy\n","        test_loss = loss_fn(test_logits,\n","                            y_test)\n","        test_acc = accuracy_fn(y_true=y_test,\n","                               y_pred=test_pred)\n","\n","    # Print out what's happening every 10 epochs\n","    if epoch % 10 == 0:\n","        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n","\n"],"metadata":{"id":"y_7aAy2dSIq7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4 Make predictions and evalutae the model\n","From the metrics it looks like our model is random guessing.\n","\n","\n","How could we investigate this futher?\n","\n","I've got an idea.\n","\n","The data explorer's motto!\n","\n","'Visualize , visualize, visualize!'\n","\n","Let's make a plot of our model's predictions, the data it's tryring to predict on and the decision boundary\n","\n","it's creating for whether something is class 0 or class 1.\n","To do so, we'll write some code to download and import the helper_fuctions.py from the Learn pytorch for deep learning repo.\n","\n","it contains a helful fuction called plot_decision_boundary() which creates a numpy meshgrid to visually plot the different points where our model is predictiong cretain classes\n","\n","We'll also import plot_predictions() which we wrote in notebook 01 to use later"],"metadata":{"id":"lMfWnTqcjASw"}},{"cell_type":"code","source":["import requests\n","from pathlib import Path\n","# Dowload helper fuctions from learn PYtorch repo(if not already downloded)\n","\n","if Path('helper_functions.py').is_file():\n","  print('helper_functions.py already exists')\n","else:\n","  print('Downloading helper_fuctions.py')\n","  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n","  with open('helper_functions.py','wb') as f:\n","    f.write(request.content)\n","from helper_functions import plot_predictions, plot_decision_boundary\n"],"metadata":{"id":"T579FCRo4yVB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(12,6))\n","plt.subplot(1,2,1)\n","plt.title('Train')\n","plot_decision_boundary(model_0,x_train,y_train)\n","plt.subplot(1,2,2)\n","plt.title('test')\n","plot_decision_boundary(model_0,x_test,y_test)"],"metadata":{"id":"2xdGd__smy7-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5. Improving a model (from a model perpective)"],"metadata":{"id":"4mKB8Wq9qihM"}},{"cell_type":"code","source":["model_0.state_dict()"],"metadata":{"id":"hXrXF8p-oPXR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5. Improving a moel ( from a model persprective)\n","* Add more layers = give the model more chances to learn about patterns in the data\n","* Add more hidden units - go from 5 hidden units to 10 hidden units\n","* Fit for longer\n","* Changing the activation functions\n","* Change the learning rate\n","* Change the loss fuction\n","These options are all from a model's perpective because they deal dirctly with the model, rather than the data.\n","And because these options are all values we (as machine learning engineers and data scientists) can change, they are referred as **hyperparameters**\n","\n","Let's try and improve our model by:\n","* Adding more hidden units: 5->10\n","* Increase the numbmer of layers: 2 ->3\n","* Increase the number of epochs: 100 ->1000"],"metadata":{"id":"6hlrZzXlrZvK"}},{"cell_type":"code","source":["class CircleModelV1(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.layer_1 = nn.Linear(in_features=2,out_features=10)\n","    self.layer_2 = nn.Linear(in_features=10,out_features=10)\n","    self.layer_3 = nn.Linear(in_features=10,out_features=1)\n","  def forward(self,x):\n","      # z = self.layer_1(x)\n","      # z = self.layer_2(x)\n","      # z = self.layer_3(x)\n","      return self.layer_3(self.layer_2(self.layer_1(x))) # this way of wiriting operarations levarages speed ups where possible behind the)\n","model_1 = CircleModelV1().to(device)\n","model_1"],"metadata":{"id":"fM-Ts3AEqzx9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_1.state_dict()"],"metadata":{"id":"qFZX2zHZxpsk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss_fn = nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.SGD(params= model_1.parameters(),lr=0.1)"],"metadata":{"id":"aoTojdmn1Dkc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4njb2G4vstwE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(42)\n","torch.cuda.manual_seed(42)\n","epoch = 1000\n","x_train,y_train = x_train.to(device),y_train.to(device)\n","for epoch in range(0,epochs):\n","  model_1.train()\n","  y_logits = model_1(x_train).squeeze()\n","  y_pre = torch.round(torch.sigmoid(y_logits))\n","  loss = loss_fn(y_logits,y_train)\n","  acc = accuracy_fn(y_train,y_pre)\n","  optimizer.zero_grad()\n","  loss.backward()\n","  optimizer.step()\n","  model_1.eval()\n","  with torch.inference_mode():\n","    test_logits = model_1(x_test).squeeze()\n","    test_pre = torch.round(torch.sigmoid(test_logits))\n","    test_loss = loss_fn(test_logits,y_test)\n","    test_acc = accuracy_fn(y_test,test_pre)\n","  # Print out what's happening every 10 epochs\n","  if epoch % 100 == 0:\n","     print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")"],"metadata":{"id":"ljx180xr4jPZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# First decision boundary of the model\n","plt.figure(figsize=(12,6))\n","plt.subplot(1,2,1)\n","plt.title('Treain')\n","plot_decision_boundary(model_1,x_train,y_train)\n","plt.subplot(1,2,2)\n","plt.title('Test')\n","plot_decision_boundary(model_1,x_test,y_test)"],"metadata":{"id":"3YyEPnR26CSc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["weight = 0.7\n","bias = 0.3\n","start = 0\n","end = 1\n","step = 0.01\n","# crate daa \n","x_regression = torch.arange(start,end,step).unsqueeze(dim=1)\n","y_regression = weight * x_regression + bias\n","print(len(x_regression))\n","x_regression[:5],y_regression[:5]"],"metadata":{"id":"n5Q8UTDDFQ_C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# create train and test splits"],"metadata":{"id":"VoMXmKyx4MG4"}},{"cell_type":"code","source":["# Create train and test splits\n","train_split = int(0.8 * len(x_regression)) # 80% of data used for training set\n","X_train_regression, y_train_regression = x_regression[:train_split], y_regression[:train_split]\n","X_test_regression, y_test_regression = x_regression[train_split:], y_regression[train_split:]\n","\n","# Check the lengths of each split\n","print(len(X_train_regression), \n","    len(y_train_regression), \n","    len(X_test_regression), \n","    len(y_test_regression))"],"metadata":{"id":"llTY07-rJc9Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_predictions(X_train_regression,y_train_regression,X_test_regression,y_test_regression)"],"metadata":{"id":"ALU--3UZ4vJB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5.2 Adjusting model_1 to fit a straight line\n","Now we've got some data , let's recreate modelL_1 but with a loss fucntion suted to our regression data"],"metadata":{"id":"EHVmv51D0K4-"}},{"cell_type":"code","source":["# ame architecture as model_1 (but using nn.Sequential)\n","model_2 = nn.Sequential(\n","    nn.Linear(in_features=1,out_features=10),\n","    nn.Linear(in_features=10,out_features=10),\n","    nn.Linear(in_features=10,out_features=1)\n",").to(device)\n","model_2"],"metadata":{"id":"sMHH8UvG5Pg4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# We'll setup the loss function to be nn.L1Loss() (the same as mean absolute error) and the optimizer to be torch.optim.SGD()."],"metadata":{"id":"v8X0wffE1Pkk"}},{"cell_type":"code","source":["loss_fn = nn.L1Loss()\n","optimizer = torch.optim.SGD(model_2.parameters(),lr=0.1)\n"],"metadata":{"id":"k2ZRaQfZ1Eyu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the model\n","torch.manual_seed(42)\n","\n","# Set the number of epochs\n","epochs = 1000\n","\n","# Put data to target device\n","X_train_regression, y_train_regression = X_train_regression.to(device), y_train_regression.to(device)\n","X_test_regression, y_test_regression = X_test_regression.to(device), y_test_regression.to(device)\n","\n","for epoch in range(0,epochs):\n","    ### Training \n","    # 1. Forward pass\n","    y_pred = model_2(X_train_regression)\n","    \n","    # 2. Calculate loss (no accuracy since it's a regression problem, not classification)\n","    loss = loss_fn(y_pred, y_train_regression)\n","\n","    # 3. Optimizer zero grad\n","    optimizer.zero_grad()\n","\n","    # 4. Loss backwards\n","    loss.backward()\n","\n","    # 5. Optimizer step\n","    optimizer.step()\n","\n","    ### Testing\n","    model_2.eval()\n","    with torch.inference_mode():\n","      # 1. Forward pass\n","      test_pred = model_2(X_test_regression)\n","      # 2. Calculate the loss \n","      test_loss = loss_fn(test_pred, y_test_regression)\n","\n","    # Print out what's happening\n","    if epoch % 100 == 0: \n","        print(f\"Epoch: {epoch} | Train loss: {loss:.5f}, Test loss: {test_loss:.5f}\")"],"metadata":{"id":"NCLnRyX01fb1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Okay, unlike model_1 on the classification data, it looks like model_2's loss is actually going down.\n","\n","Let's plot its predictions to see if that's so.\n","\n","And remember, since our model and data are using the target device, and this device may be a GPU, however, our plotting function uses matplotlib and matplotlib can't handle data on the GPU.\n","\n","To handle that, we'll send all of our data to the CPU using .cpu() when we pass it to"],"metadata":{"id":"bfC8Ssvj3vrf"}},{"cell_type":"code","source":["# Turn on evaluation mode\n","model_2.eval()\n","\n","# Make predictions (inference)\n","with torch.inference_mode():\n","    y_preds = model_2(X_test_regression)\n","\n","# Plot data and predictions with data on the CPU (matplotlib can't handle data on the GPU)\n","# (try removing .cpu() from one of the below and see what happens)\n","plot_predictions(train_data=X_train_regression.cpu(),\n","                 train_labels=y_train_regression.cpu(),\n","                 test_data=X_test_regression.cpu(),\n","                 test_labels=y_test_regression.cpu(),\n","                 predictions=y_preds.cpu());"],"metadata":{"id":"eVjyB6cA124s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6. The missing piece: non-linearity\n","We've seen our model can draw straight (linear) lines, thanks to its linear layers.\n","\n","But how about we give it the capacity to draw non-straight (non-linear) lines?\n","\n","How?\n","\n","Let's find out."],"metadata":{"id":"1wBDRmEm4PDH"}},{"cell_type":"markdown","source":["# 6.1 Recreating non-linear data (red and blue circles)\n","First, let's recreate the data to start off fresh. We'll use the same setup as before."],"metadata":{"id":"r7vux0NH4Vi-"}},{"cell_type":"code","source":["# Make and plot data\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import make_circles\n","\n","n_samples = 1000\n","\n","X, y = make_circles(n_samples=1000,\n","    noise=0.03,\n","    random_state=42,\n",")\n","\n","plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu);"],"metadata":{"id":"zAXYKicZ30iL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert to tensors and split into train and test sets\n","import torch\n","from sklearn.model_selection import train_test_split\n","\n","# Turn data into tensors\n","X = torch.from_numpy(X).type(torch.float)\n","y = torch.from_numpy(y).type(torch.float)\n","\n","# Split into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, \n","                                                    y, \n","                                                    test_size=0.2,\n","                                                    random_state=42\n",")\n","\n","X_train[:5], y_train[:5]"],"metadata":{"id":"G2BAtm285HjZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6.2 Building a model with non -linearity\n","Now here comes the fun part\n","\n","\n","*   Linear = straight line\n","*   non-linear = non-straight lines\n","# Artificial nerural network are a large combinaation of lineawr and non-linear fuctions which are potentially abel to find patterns in data\n"],"metadata":{"id":"w6DHhkj854jF"}},{"cell_type":"code","source":["# Build model with non-linear activation function\n","from torch import nn\n","class CircleModelV2(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.layer_1 = nn.Linear(in_features=2, out_features=10)\n","        self.layer_2 = nn.Linear(in_features=10, out_features=10)\n","        self.layer_3 = nn.Linear(in_features=10, out_features=1)\n","        self.relu = nn.ReLU() # <- add in ReLU activation function\n","        # Can also put sigmoid in the model \n","        # This would mean you don't need to use it on the predictions\n","        # self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","      # Intersperse the ReLU activation function between layers\n","       return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))\n","\n","model_3 = CircleModelV2().to(device)\n","print(model_3)"],"metadata":{"id":"glzFfubH52r2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Setup loss and optimizer \n","loss_fn = nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.SGD(model_3.parameters(), lr=0.1)"],"metadata":{"id":"Dn3Nhb6FiSmf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#  6.3 train model in non-linearity"],"metadata":{"id":"fIh-iPbnlhgt"}},{"cell_type":"code","source":["# random seeds\n","torch.manual_seed(42)\n","torch.cuda.manual_seed(42)\n","x_train,y_train = x_train.to(device),y_train.to(device)\n","x_test,y_test = x_test.to(device),y_test.to(device)\n","epochs = 1000\n","for epoch in range(0,epochs):\n","  # train\n","  model_3.train()\\\n","  # 1.Forward pass\n","  y_logits = model_3(x_train).squeeze()\n","  y_pre = torch.round(torch.sigmoid(y_logits))\n","  # 2. cALCULATEL THE LOSS\n","  loss = loss_fn(y_logits,y_train)\n","  acc = accuracy_fn(y_train,y_pre)\n","  # 3. Optimizer zero grad\n","  optimizer.zero_grad()\n","  # 4.Loss backward\n","  loss.backward()\n","  # 5. step the optimizer\n","  optimizer.step()\n","  ### Testing \n","  model_3.eval()\n","  with torch.inference_mode():\n","    test_logits = model_3(x_test).squeeze()\n","    test_pre = torch.round(torch.sigmoid(test_logits))\n","    test_loss = loss_fn(test_logits,y_test)\n","    test_acc = accuracy_fn(y_test,test_pre)\n","  if epoch % 100 == 0:\n","    \n","    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Accuracy: {test_acc:.2f}%\")\n"],"metadata":{"id":"nce4Xoz5lVQL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Make predictions \n","model_3.eval()\n","with torch.inference_mode():\n","  y_pre = torch.round(torch.sigmoid(model_3(x_test))).squeeze()\n","y_pre[:10],y_test[:10]"],"metadata":{"id":"gCkcBKJvmGwP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot decision boundaries \n","plt.figure(figsize=(12,6))\n","plt.subplot(1,2,1)\n","plt.title('Train')\n","plot_decision_boundary(model_1,x_train,y_train)\n","plt.subplot(1,2,2)\n","plt.title('Test')\n","plot_decision_boundary(model_3,x_test,y_test)"],"metadata":{"id":"SbhkojgHygaw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 7. Replication non-linear activation functioins\n","Neural netwoks , rather than us telllilng the model what to learn we give it the tools to discovr patterns in data and it tries to figure out the patters on its own\n","An these tools are linear and non-linear fuctions"],"metadata":{"id":"wUeuo5nnzetO"}},{"cell_type":"code","source":["# create a tensor\n","a = torch.arange(-10,10,1)\n","a.dtype"],"metadata":{"id":"7fSy3T1vHHqM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a"],"metadata":{"id":"vSYdxXbhHJQa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#visualize the tensor\n","plt.plot(a)"],"metadata":{"id":"7uEyN170HJ14"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(torch.relu(a))"],"metadata":{"id":"YsPCwIphHKPA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def myrelu(a):\n","  return torch.maximum(torch.tensor(0),x) \n","myrelu(a)"],"metadata":{"id":"NEgcWeUvIVFM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot myrelu activation fuction\n","plt.plot(torch.relu(a))"],"metadata":{"id":"ditQhOlBIh4j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now let's do the same for sigmoid \n","def mysigmoid(x):\n","  return 1/1 + torch.exp(-x)"],"metadata":{"id":"LoGoRczPIs0j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(torch.sigmoid(a))"],"metadata":{"id":"PZiSrf07JVCC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(sigmoid(a))"],"metadata":{"id":"kjbuZC6HJaqh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 8. Putting it all together with a multiclass classificaation problem\n","\n","\n","*   Binary classification = one thing or another ( cat vs dog, spam or not , fraud or not froud )\n","*   Multi-class  classification = more than thing or another (cat vs dog vs chicken)\n","\n"],"metadata":{"id":"Aa_2Y34UKZiW"}},{"cell_type":"markdown","source":["# Creating a toy multi-class detasets\n"],"metadata":{"id":"qA7afwv-MiI7"}},{"cell_type":"code","source":["from sklearn.datasets import make_blobs\n","# set hyperparameter for data   creations\n","num_classes = 4 \n","num_features = 2\n","random_seed = 42\n","# create multiclass data\n","x_blob ,y_blob = make_blobs(n_samples=1000,n_features=num_features,centers=num_classes, cluster_std=1.5,random_state=random_seed)\n","x_blob = torch.from_numpy(x_blob).type(torch.float)\n","y_blob = torch.from_numpy(y_blob).type(torch.LongTensor)\n","x_blob_train,x_blob_test,y_blob_train,y_blob_test = train_test_split(x_blob,y_blob,test_size=0.2,random_state=random_seed)\n","# plot\n","plt.figure(figsize=(10, 7))\n","plt.scatter(x_blob[:, 0], x_blob[:, 1], c=y_blob, cmap=plt.cm.RdYlBu);"],"metadata":{"id":"B9Jl_dsoJgVB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Building a multi class classification model in Pytorch"],"metadata":{"id":"IjSfUJYtSTT6"}},{"cell_type":"code","source":["class DawinModel(nn.Module):\n","  def __init__(self, input_features, output_features, hidden_units=8):\n","    super().__init__()\n","    self.linear_layer_stack = nn.Sequential(\n","        nn.Linear(in_features=input_features, out_features=hidden_units),\n","        nn.ReLU(),\n","        nn.Linear(in_features=hidden_units, out_features=hidden_units),\n","        nn.ReLU(),\n","        nn.Linear(in_features=hidden_units, out_features=output_features)\n","    )\n","    \n","  def forward(self, x):\n","    return self.linear_layer_stack(x)\n","# Create an instance of DawinModel and send it to teh target device\n","model_4 = DawinModel(input_features=2,output_features=4,hidden_units=8).to(device)\n","model_4"],"metadata":{"id":"J1K6ROhyZPHF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# create loss_fn , optimizer and traning loop for this multi class classification model"],"metadata":{"id":"H103Gf2WeaNZ"}},{"cell_type":"code","source":["loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(params= model_4.parameters(),lr=0.1)\n"],"metadata":{"id":"qELFVBVPa1jz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Getting prediction prbabilities for a multi-class pytorch model \n","In oder to evaluate and  train and test our model, w eeee need to convert our model's outputs (logits) to prediction prebabilities and then to prediction labels .\n","Logits -pre - pre labels"],"metadata":{"id":"ppzsf1c2gt3N"}},{"cell_type":"code","source":["model_4.eval()\n","with torch.inference_mode():\n","    y_logits = model_4(x_blob_test.to(device))\n","y_logits[:10]"],"metadata":{"id":"vIuWZyz1evDS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# converts our model's logit outputs to prediction probabilities\n","y_pre = torch.softmax(y_logits,dim=1)\n","print(y_logits[:5])\n","print(y_pre[:5])"],"metadata":{"id":"Yui3IIDMf1JR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.sum(y_pre[0])"],"metadata":{"id":"rLQoX0AAiNGc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.argmax(y_pre[0])"],"metadata":{"id":"Kq9M-qYOjm-J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pre1 = torch.argmax(y_pre,dim=1)\n","y_pre1"],"metadata":{"id":"kJx-8E9Mjuvp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_blob_train.shape"],"metadata":{"id":"sHZoS8Voqfok"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Creating  a traning loop and testing loop for multi class classifiction model"],"metadata":{"id":"SRC-oWxVnUi5"}},{"cell_type":"code","source":["# FIt the multiclass model to the data\n","torch.manual_seed(42)\n","torch.cuda.manual_seed(42)\n","epochs = 100\n","# Put data to target device\n","x_blob_train, y_blob_train = x_blob_train.to(device), y_blob_train.to(device)\n","x_blob_test, y_blob_test = x_blob_test.to(device), y_blob_test.to(device)\n","for epoch in range(0,epochs):\n","  model_4.train()\n","  # 1. Forward pass\n","  y_logits = model_4(x_blob_train)\n","  y_pre = torch.softmax(y_logits,dim=1).argmax(dim=1)\n","  loss = loss_fn(y_logits,y_blob_train)\n","  acc = accuracy_fn(y_blob_train,y_pre)\n","  optimizer.zero_grad()\n","  loss.backward()\n","  optimizer.step()\n","  ### Testing\n","  model_4.eval()\n","  with torch.inference_mode():\n","    test_logits = model_4(x_blob_test)\n","    test_pre = torch.softmax(test_logits,dim=1).argmax(dim=1)\n","    test_loss = loss_fn(test_logits,y_blob_test)\n","    test_acc = accuracy_fn(y_blob_test,test_pre)\n","  if epoch % 10 == 0:\n","    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\")\n"],"metadata":{"id":"8fmX0RZhnEyr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Make predictions \n","model_4.eval()\n","with torch.inference_mode():\n","  y_logits = model_4(x_blob_test)\n","y_logits[:10]"],"metadata":{"id":"w3VrZBovq7F8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lpAdeJRMNHjG"},"source":["Alright, looks like our model's predictions are still in logit form.\n","\n","Though to evaluate them, they'll have to be in the same form as our labels (`y_blob_test`) which are in integer form.\n","\n","Let's convert our model's prediction logits to prediction probabilities (using `torch.softmax()`) then to prediction labels (by taking the `argmax()` of each sample).\n","\n","> **Note:** It's possible to skip the `torch.softmax()` function and go straight from `predicted logits -> predicted labels` by calling `torch.argmax()` directly on the logits.\n",">\n","> For example, `y_preds = torch.argmax(y_logits, dim=1)`, this saves a computation step (no `torch.softmax()`) but results in no prediction probabilities being available to use. "]},{"cell_type":"code","source":["y_pre_pro = torch.softmax(y_logits,dim=1)\n","y_pre = y_pre_pro.argmax(dim=1)\n","print(f\"Predictions: {y_pre[:10]}\\nLabels: {y_blob_test[:10]}\")\n","print(f\"Test accuracy: {accuracy_fn(y_true=y_blob_test, y_pred=y_preds)}%\")"],"metadata":{"id":"r_DVBWQhk0C0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Nice! Our model predictions are now in the same form as our test labels.\n","\n","Let's visualize them with plot_decision_boundary(), remember because our data is on the GPU, we'll have to move it to the CPU for use with matplotlib (plot_decision_boundary() does this automatically for us)."],"metadata":{"id":"db4Xdaoel5r3"}},{"cell_type":"code","source":["plt.figure(figsize=(12,6))\n","plt.subplot(1,2,1)\n","plt.title(\"Train\")\n","plot_decision_boundary(model_4,x_blob_train,y_blob_train)\n","plt.subplot(1,2,2)\n","plot_decision_boundary(model_4,x_blob_test,y_blob_test)"],"metadata":{"id":"7-rxolDElqiD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["try:\n","    from torchmetrics import Accuracy\n","except:\n","    !pip install torchmetrics # this is the version we're using in this notebook (later versions exist here: https://torchmetrics.readthedocs.io/en/stable/generated/CHANGELOG.html#changelog)\n","    from torchmetrics import Accuracy\n","\n","# Setup metric and make sure it's on the target device\n","torchmetrics_accuracy = Accuracy(task='multiclass', num_classes=4).to(device)\n","\n","# Calculate accuracy\n","torchmetrics_accuracy(y_pre, y_blob_test)"],"metadata":{"id":"NAx_s8Rgmi1x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"zaT57PEnIM1d"},"execution_count":null,"outputs":[]}]}