{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyM/BdTKf5Nx6DmGTv3iDk97"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"4vSrdg-KUAbk"},"outputs":[],"source":["!pip install -U torch==1.8.0 torchtext==0.9.0\n","\n","# Reload environment\n","exit()"]},{"cell_type":"code","source":["!python -m spacy download en_core_web_sm\n","!python -m spacy download de_core_news_sm"],"metadata":{"id":"onGaAKx4Uhnw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch import optim\n","from torchtext.legacy.datasets import Multi30k\n","import spacy\n","from torchtext.legacy.data import Field , TabularDataset , BucketIterator\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","spacy_eng = spacy.load('en_core_web_sm')\n","\n","spacy_ger = spacy.load('de_core_news_sm')"],"metadata":{"id":"eL7GLhcpUKPq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tokenize_eng(text):\n","    return [tok.text for tok in spacy_eng.tokenizer(text)]\n","\n","\n","def tokenize_ger(text):\n","    return [tok.text for tok in spacy_ger.tokenizer(text)]\n","english = Field(sequential=True, use_vocab=True, tokenize=tokenize_eng, lower=True)\n","german = Field(sequential=True, use_vocab=True, tokenize=tokenize_ger, lower=True)\n","\n","train_data , validation_data , test_data = Multi30k.splits(exts=('.de','.en'),\n","                                                           fields=(german,english))\n","\n","english.build_vocab(train_data,max_size=10000,min_freq=2)\n","german.build_vocab(train_data,max_size=10000,min_freq=2)\n","\n","train_iterator, validation_iterator, test_iterator = BucketIterator.splits(\n","    (train_data, validation_data, test_data), batch_size=64, device=\"cuda\"\n",")\n","\n","for batch in train_iterator:\n","  print(batch)\n"],"metadata":{"id":"_ft9ZEPwUvRY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# string to integer (stoi)\n","print(f'Index of the word (the) is: {english.vocab.stoi[\"the\"]}')\n","\n","# print integer to string (itos)\n","print(f\"Word of the index (1612) is: {english.vocab.itos[1612]}\")\n","print(f\"Word of the index (0) is: {english.vocab.itos[0]}\")\n"],"metadata":{"id":"HvVhCJKaeuLk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the text to be tokenized\n","text = \"This is an example sentence. It demonstrates the usage of the spaCy tokenizer.\"\n","def tokenize_eng(text):\n","    return [tok.text for tok in spacy_eng.tokenizer(text)]\n","# Tokenize the text\n","tokens = []\n","for token in spacy_eng.tokenizer(text):\n","    tokens.append(token.text)\n","print(tokenize_eng(text))\n","# Print the tokens\n","print(tokens)"],"metadata":{"id":"wKNiUI5UV_lG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["english.vocab.stoi['love']"],"metadata":{"id":"rW2dTnVuWG6N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["english.vocab.itos[2167]"],"metadata":{"id":"LBp70xoy6unY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"eTDZRqwT7e5-"},"execution_count":null,"outputs":[]}]}