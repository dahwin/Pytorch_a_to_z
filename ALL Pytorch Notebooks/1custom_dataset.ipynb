{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyM7otDQQIH27+bSix5aj/dx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"I86JXoWMQ1il"},"outputs":[],"source":["import torch\n","from torch import nn\n","from PIL import Image\n","from torch.utils.data import Dataset\n","from torchvision import transforms\n","from typing import Tuple, Dict,List\n","# Setup device-agnostic code\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"]},{"cell_type":"code","source":["import requests\n","import zipfile\n","from pathlib import Path\n","\n","# Setup path to data folder\n","data_path = Path(\"data/\")\n","image_path = data_path / \"pizza_steak_sushi\"\n","\n","# If the image folder doesn't exist, download it and prepare it... \n","if image_path.is_dir():\n","    print(f\"{image_path} directory exists.\")\n","else:\n","    print(f\"Did not find {image_path} directory, creating one...\")\n","    image_path.mkdir(parents=True, exist_ok=True)\n","    \n","    # Download pizza, steak, sushi data\n","    with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n","        request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n","        print(\"Downloading pizza, steak, sushi data...\")\n","        f.write(request.content)\n","\n","    # Unzip pizza, steak, sushi data\n","    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n","        print(\"Unzipping pizza, steak, sushi data...\") \n","        zip_ref.extractall(image_path)"],"metadata":{"id":"JoiA76a0O_mJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_path"],"metadata":{"id":"wl7lmKKfPEoH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Setup train and testing paths\n","train_dir = image_path / \"train\"\n","test_dir = image_path / \"test\"\n","\n","train_dir, test_dir"],"metadata":{"id":"uHI0M0TzPK2n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","from PIL import Image\n","\n","# Set seed\n","# random.seed(42) # <- try changing this and see what happens\n","\n","# 1. Get all image paths (* means \"any combination\")\n","\n","image_path_list = list(image_path.glob(\"*/*/*.jpg\")) + list(image_path.glob(\"*/*/*.png\"))\n","\n","\n","# 2. Get random image path\n","random_image_path = random.choice(image_path_list)\n","\n","# 3. Get image class from path name (the image class is the name of the directory where the image is stored)\n","image_class = random_image_path.parent.stem\n","\n","# 4. Open image\n","img = Image.open(random_image_path)\n","\n","# 5. Print metadata\n","print(f\"Random image path: {random_image_path}\")\n","print(f\"Image class: {image_class}\")\n","print(f\"Image height: {img.height}\") \n","\n","print(f\"Image width: {img.width}\")\n","img"],"metadata":{"id":"citNr3U8PaY-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import matplotlib.pyplot as plt\n","\n","from torch.utils.data import DataLoader\n","from torchvision import datasets,transforms"],"metadata":{"id":"tgvcoNbxPdZP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_transform = transforms.Compose([\n","    transforms.Resize(size=(224,224)),\n","    \n","    transforms.ToTensor()\n","])\n","data_transform_test = transforms.Compose([\n","    transforms.Resize(size=(224,224)),\n","    \n","    transforms.ToTensor()\n","])"],"metadata":{"id":"YZwIyUlnPjrG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_transformed_images(image_path_list, transformed_data, n, seed=32):\n","    \"\"\"Plots a series of random images from image_paths.\n","\n","    Will open n image paths from image_paths, transform them\n","    with transform and plot them side by side.\n","\n","    Args:\n","        image_paths (list): List of target image paths. \n","        transform (PyTorch Transforms): Transforms to apply to images.\n","        n (int, optional): Number of images to plot. Defaults to 3.\n","        seed (int, optional): Random seed for the random generator. Defaults to 42.\n","    \"\"\"\n","    random.seed(seed)\n","    random_image_paths = random.sample(image_path_list, k=n)\n","    for image_path in random_image_paths:\n","        with Image.open(image_path) as f:\n","            fig, ax = plt.subplots(1, 2)\n","            ax[0].imshow(f) \n","            ax[0].set_title(f\"Original \\nSize: {f.size}\")\n","            ax[0].axis(\"off\")\n","\n","            # Transform and plot image\n","            # Note: permute() will change shape of image to suit matplotlib \n","            # (PyTorch default is [C, H, W] but Matplotlib is [H, W, C])\n","            transformed_image = transformed_data(f).permute(1, 2, 0) \n","            ax[1].imshow(transformed_image) \n","            ax[1].set_title(f\"Transformed \\nSize: {transformed_image.shape}\")\n","            ax[1].axis(\"off\")\n","\n","            fig.suptitle(f\"Class: {image_path.parent.stem}\", fontsize=16)"],"metadata":{"id":"rMsPyKy7Pm_W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_transformed_images(image_path_list,data_transform,n=2)"],"metadata":{"id":"ggXycYD7PtkV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchvision import datasets\n","train_data = datasets.ImageFolder(root=train_dir,transform=data_transform,target_transform=None)\n","test_data = datasets.ImageFolder(root=test_dir,transform=data_transform)\n","print(f\"Train data:\\n{train_data} Test data \\n{test_data}\")"],"metadata":{"id":"60JUzSoFPwft"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from torch.utils.data import DataLoader\n","NUM_WORKERS = os.cpu_count()\n","train_dataloader = DataLoader(dataset=train_data,batch_size=1,num_workers=NUM_WORKERS,shuffle=False)\n","test_dataloader = DataLoader(dataset=test_data,batch_size=1,num_workers=NUM_WORKERS,shuffle=False)\n","train_dataloader,test_dataloader"],"metadata":{"id":"hgLoeznzP91N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DahwinTinyVGG(nn.Module):\n","    \"\"\"\n","    Model architecture copying TinyVGG from: \n","    https://poloclub.github.io/cnn-explainer/\n","    \"\"\"\n","    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n","        super().__init__()\n","        self.conv_block_1 = nn.Sequential(\n","            nn.Conv2d(in_channels=input_shape, \n","                      out_channels=hidden_units, \n","                      kernel_size=3, # how big is the square that's going over the image?\n","                      stride=1, # default\n","                      padding=1), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n","            nn.ReLU(),\n","            nn.Conv2d(in_channels=hidden_units, \n","                      out_channels=hidden_units,\n","                      kernel_size=3,\n","                      stride=1,\n","                      padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2,\n","                         stride=2) # default stride value is same as kernel_size\n","        )\n","        self.conv_block_2 = nn.Sequential(\n","            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2)\n","        )\n","        self.classifier = nn.Sequential(\n","            nn.Flatten(),\n","            # Where did this in_features shape come from? \n","            # It's because each layer of our network compresses and changes the shape of our inputs data.\n","            nn.Linear(in_features=hidden_units*56*56,\n","                      out_features=output_shape)\n","        )\n","    \n","    def forward(self, x: torch.Tensor):\n","        x = self.conv_block_1(x)\n","        # print(x.shape)\n","        x = self.conv_block_2(x)\n","        # print(x.shape)\n","        x = self.classifier(x)\n","        # print(x.shape)\n","        return x\n","        # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # <- leverage the benefits of operator fusion\n","\n","torch.manual_seed(42)\n","model_0 = DahwinTinyVGG(input_shape=3, # number of color channels (3 for RGB) \n","                  hidden_units=10, \n","                  output_shape=len(train_data.classes)).to(device)\n","model_0"],"metadata":{"id":"1vMpXvhcWENK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install torchinfo"],"metadata":{"id":"3ZmD1fvcmvVM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchinfo import summary\n","summary(model_0,input_size = [1,3,224,224])"],"metadata":{"id":"lESewFIcWN7i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_step(model:torch.nn.Module,dataloader:torch.utils.data.DataLoader,loss_fn:torch.nn.Module,optimizer:torch.optim.Optimizer):\n","  model.train()\n","  train_loss,train_acc = 0,0\n","  for batch ,(x,y) in enumerate(dataloader):\n","    x,y = x.to(device),y.to(device)\n","    y_pred = model(x)\n","    loss = loss_fn(y_pred,y)\n","    train_loss += loss.item()\n","    loss.backward()\n","    optimizer.step()\n","    y_pred_class = torch.argmax(torch.softmax(y_pred,dim=1),dim=1)\n","    train_acc += (y_pred_class==y).sum().item()/len(y_pred)\n","\n","  train_loss = train_loss/len(dataloader)\n","  train_acc = train_acc/len(dataloader)\n","  return train_loss,train_acc"],"metadata":{"id":"VrYLhtj_mRDP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test_step(model:torch.nn.Module,\n","              dataloader:torch.utils.data.DataLoader,\n","              loss_fn:torch.nn.Module):\n","  model.eval()\n","  test_loss,test_acc =0,0\n","  with torch.inference_mode():\n","    for batch ,(X,y) in enumerate(dataloader):\n","      X,y = X.to(device),y.to(device)\n","      test_pred_logits = model(X)\n","      loss = loss_fn(test_pred_logits,y)\n","      test_loss += loss.item()\n","      test_pred_labels = test_pred_logits.argmax(dim=1)\n","      test_acc += ((test_pred_labels==y).sum().item()/len(test_pred_labels))\n","  test_loss = test_loss/len(dataloader)\n","  test_acc = test_acc/len(dataloader)\n","  return test_loss,test_acc\n","     "],"metadata":{"id":"ujvc3uyxm_DV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm.auto import tqdm\n","def train(model:torch.nn.Module,train_dataloader:torch.utils.data.DataLoader,test_dataloader:torch.utils.data.DataLoader,optimizer:torch.optim.Optimizer,loss_fn:torch.nn.Module=nn.CrossEntropyLoss(),epochs:int=5):\n","  results = {'train_loss':[],\n","             'train_acc':[],\n","             'test_loss':[],\n","             'test_acc':[]}\n","  for epoch in tqdm(range(epochs)):\n","    train_loss ,train_acc = train_step(model=model,dataloader=train_dataloader,loss_fn=loss_fn,optimizer=optimizer)\n","    test_loss,test_acc = test_step(model=model,dataloader=test_dataloader,loss_fn=loss_fn)\n","    print(f'{epoch} | Train loss:{train_loss:.4f} |Train acc:{train_acc:.4f} | Test loss: {test_loss:.4f} | Test acc: {test_acc:.4f}')\n","    results['train_loss'].append(train_loss)\n","    results['train_acc'].append(train_acc)\n","    results['test_loss'].append(test_loss)\n","    results['test_acc'].append(test_acc)\n","  return results"],"metadata":{"id":"uLqJ44mSnD4F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set random seeds\n","torch.manual_seed(42) \n","torch.cuda.manual_seed(42)\n","\n","# Set number of epochs\n","NUM_EPOCHS = 5\n","\n","# Recreate an instance of TinyVGG\n","model_0 = DahwinTinyVGG(input_shape=3, # number of color channels (3 for RGB) \n","                  hidden_units=10, \n","                  \n","                  output_shape=len(train_data.classes)).to(device)\n","\n","# Setup loss function and optimizer\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.001)\n","\n","# Start the timer\n","from timeit import default_timer as timer \n","start_time = timer()\n","\n","# Train model_0 \n","model_0_results = train(model=model_0, \n","                        train_dataloader=train_dataloader,\n","                        test_dataloader=test_dataloader,\n","                        optimizer=optimizer,\n","                        loss_fn=loss_fn, \n","                        epochs=NUM_EPOCHS,\n","                        )\n","\n","# End the timer and print out how long it took\n","end_time = timer()\n","print(f\"Total training time: {end_time-start_time:.3f} seconds\")"],"metadata":{"id":"787foPTrnG2F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Download custom image\n","import requests\n","\n","# Setup custom image path\n","custom_image_path = data_path / \"dahyun_pizza.png\"\n","\n","# Download the image if it doesn't already exist\n","if not custom_image_path.is_file():\n","    with open(custom_image_path, \"wb\") as f:\n","        # When downloading from GitHub, need to use the \"raw\" file link\n","        request = requests.get(\"https://thumbs.gfycat.com/AcademicCalmClam-mobile.jpg\")\n","        print(f\"Downloading {custom_image_path}...\")\n","        f.write(request.content)\n","else:\n","    print(f\"{custom_image_path} already exists, skipping download.\")"],"metadata":{"id":"KlVejWWsnXPj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torchvision\n","custom_image_uint8 = torchvision.io.read_image(str(custom_image_path)).type(torch.float32)\n","custom_image_uint8 = custom_image_uint8 / 255. \n","# Print out image data\n","print(f\"Custom image tensor:\\n{custom_image_uint8}\\n\")\n","print(f\"Custom image shape: {custom_image_uint8.shape}\\n\")\n","print(f\"Custom image dtype: {custom_image_uint8.dtype}\")"],"metadata":{"id":"cQR_vSMJoD17"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load in custom image and convert the tensor values to float32\n","custom_image = torchvision.io.read_image(str(custom_image_path)).type(torch.float32)\n","\n","# Divide the image pixel values by 255 to get them between [0, 1]\n","custom_image = custom_image / 255. \n","\n","# Print out image data\n","print(f\"Custom image tensor:\\n{custom_image}\\n\")\n","print(f\"Custom image shape: {custom_image.shape}\\n\")\n","print(f\"Custom image dtype: {custom_image.dtype}\")"],"metadata":{"id":"MZT52Q6boHDz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.imshow(custom_image.permute(1, 2, 0))"],"metadata":{"id":"fQdSGoV7oJVz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["custom_image_transform =  transforms.Compose([\n","    transforms.Resize((224,224))\n","])\n","custom_image_transformed = custom_image_transform(custom_image)\n","print(f\"Original shape:{custom_image.shape}\")\n","print(f\"New shape:{custom_image_transformed.shape}\")"],"metadata":{"id":"mjuemSVBoL1r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.imshow(custom_image_transformed.permute(1, 2, 0))"],"metadata":{"id":"J8nG0Wr_oPKr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_0.eval()\n","with torch.inference_mode():\n","  custom_image_pred = model_0(custom_image_transformed.unsqueeze(0).to(device))\n","custom_image_pred"],"metadata":{"id":"BSQZPLgcoUba"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pred_and_plot_image(model: torch.nn.Module, \n","                        image_path: str, \n","                        class_names: List[str] = None, \n","                        transform=None,\n","                        device: torch.device = device):\n","    \"\"\"Makes a prediction on a target image and plots the image with its prediction.\"\"\"\n","    \n","    # 1. Load in image and convert the tensor values to float32\n","    target_image = torchvision.io.read_image(str(image_path)).type(torch.float32)\n","    \n","    # 2. Divide the image pixel values by 255 to get them between [0, 1]\n","    target_image = target_image / 255. \n","    \n","    # 3. Transform if necessary\n","    if transform:\n","        target_image = transform(target_image)\n","    \n","    # 4. Make sure the model is on the target device\n","    model.to(device)\n","    \n","    # 5. Turn on model evaluation mode and inference mode\n","    model.eval()\n","    with torch.inference_mode():\n","        # Add an extra dimension to the image\n","        target_image = target_image.unsqueeze(dim=0)\n","    \n","        # Make a prediction on image with an extra dimension and send it to the target device\n","        target_image_pred = model(target_image.to(device))\n","        \n","    # 6. Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n","    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n","\n","    # 7. Convert prediction probabilities -> prediction labels\n","    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n","    \n","    # 8. Plot the image alongside the prediction and prediction probability\n","    plt.imshow(target_image.squeeze().permute(1, 2, 0)) # make sure it's the right size for matplotlib\n","    if class_names:\n","        title = f\"Pred: {class_names[target_image_pred_label.cpu()]} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n","    else: \n","        title = f\"Pred: {target_image_pred_label} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n","    plt.title(title)\n","    plt.axis(False);"],"metadata":{"id":"w3K6gdkYofby"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Pred on our custom image\n","pred_and_plot_image(model=model_0,\n","                    image_path=custom_image_path,\n","                    class_names=train_data.classes,\n","                    transform=custom_image_transform,\n","                    device=device)"],"metadata":{"id":"F3koBfsUov9R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Rhn0FnrtpLeT"},"execution_count":null,"outputs":[]}]}