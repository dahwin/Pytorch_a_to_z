{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyO/yjZNztJuUUxhUXhZ3TlE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_BpDDORWph9A"},"outputs":[],"source":["\"\"\"\n","A simple walkthrough of how to code a fully connected neural network\n","using the PyTorch library. For demonstration we train it on the very\n","common MNIST dataset of handwritten digits. In this code we go through\n","how to create the network as well as initialize a loss function, optimizer,\n","check accuracy and more.\n","Programmed by Aladdin Persson\n","* 2020-04-08: Initial coding\n","* 2021-03-24: Added more detailed comments also removed part of\n","              check_accuracy which would only work specifically on MNIST.\n","* 2022-09-23: Updated with more detailed comments, docstrings to functions, and checked code still functions as intended.\n","\"\"\"\n","\n","# Imports\n","import torch\n","import torch.nn.functional as F  # Parameterless functions, like (some) activation functions\n","import torchvision.datasets as datasets  # Standard datasets\n","import torchvision.transforms as transforms  # Transformations we can perform on our dataset for augmentation\n","from torch import optim  # For optimizers like SGD, Adam, etc.\n","from torch import nn  # All neural network modules\n","from torch.utils.data import (\n","    DataLoader,\n",")  # Gives easier dataset managment by creating mini batches etc.\n","from tqdm import tqdm  # For nice progress bar!\n","\n","\n","\n","# Set device cuda for GPU if it's available otherwise run on the CPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Hyperparameters\n","input_size = 28\n","sequence_length = 28\n","num_layers = 2\n","hidden_size = 256\n","num_classes = 10\n","learning_rate = 0.001\n","batch_size = 64\n","num_epochs = 3\n","# Here we create our simple neural network. For more details here we are subclassing and\n","# inheriting from nn.Module, this is the most general way to create your networks and\n","# allows for more flexibility. I encourage you to also check out nn.Sequential which\n","# would be easier to use in this scenario but I wanted to show you something that\n","# \"always\" works and is a general approach.\n","# class RNN(nn.Module):\n","#   def __init__(self,input_size,hidden_size,num_layers,num_classes):\n","#     super(RNN,self).__init__()\n","#     self.hidden_size = hidden_size\n","#     self.num_layers = num_layers\n","#     self.RNN = nn.RNN(input_size,hidden_size,num_layers,batch_first=True)\n","#     self.fc = nn.Linear(hidden_size*sequence_length,num_classes)\n","class RNN(nn.Module):\n","   def __init__(self,input_size,hidden_size,num_layers,num_classes):\n","     super(RNN,self).__init__()\n","     self.hidden_size = hidden_size\n","     self.num_layers = num_layers\n","     self.RNN = nn.RNN(input_size,hidden_size,num_layers,batch_first=True)\n","     self.fc = nn.Linear(hidden_size*sequence_length,num_classes)\n","\n","   def forward(self,x):\n","     h0 = torch.zeros(self.num_layers,x.size(0),self.hidden_size).to(device)\n","     out , _ = self.RNN(x,h0)\n","     out = out.reshape(out.shape[0],-1)\n","     out = self.fc(out)\n","     return out\n","\n","\n","# Load Data\n","train_dataset = datasets.MNIST(\n","    root=\"dataset/\", train=True, transform=transforms.ToTensor(), download=True\n",")\n","test_dataset = datasets.MNIST(\n","    root=\"dataset/\", train=False, transform=transforms.ToTensor(), download=True\n",")\n","train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n","\n","# Initialize network\n","model = RNN(input_size,hidden_size,num_layers,num_classes).to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Train Network\n","for epoch in range(num_epochs):\n","    for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):\n","        # Get data to cuda if possible\n","        data = data.to(device=device).squeeze(1)\n","        targets = targets.to(device=device)\n","\n","\n","        # Forward\n","        scores = model(data)\n","        loss = criterion(scores, targets)\n","\n","        # Backward\n","        optimizer.zero_grad()\n","        loss.backward()\n","\n","        # Gradient descent or adam step\n","        optimizer.step()\n","\n","\n","# Check accuracy on training & test to see how good our model\n","def check_accuracy(loader, model):\n","    \"\"\"\n","    Check accuracy of our trained model given a loader and a model\n","    Parameters:\n","        loader: torch.utils.data.DataLoader\n","            A loader for the dataset you want to check accuracy on\n","        model: nn.Module\n","            The model you want to check accuracy on\n","    Returns:\n","        acc: float\n","            The accuracy of the model on the dataset given by the loader\n","    \"\"\"\n","\n","    num_correct = 0\n","    num_samples = 0\n","    model.eval()\n","\n","    # We don't need to keep track of gradients here so we wrap it in torch.no_grad()\n","    with torch.no_grad():\n","        # Loop through the data\n","        for x, y in loader:\n","\n","            # Move data to device\n","            x = x.to(device=device)\n","            y = y.to(device=device)\n","\n","            # Get to correct shape\n","            x = x.reshape(x.shape[0], -1)\n","\n","            # Forward pass\n","            scores = model(x)\n","            _, predictions = scores.max(1)\n","\n","            # Check how many we got correct\n","            num_correct += (predictions == y).sum()\n","\n","            # Keep track of number of samples\n","            num_samples += predictions.size(0)\n","\n","    model.train()\n","    return num_correct / num_samples\n","\n","\n","# Check accuracy on training & test to see how good our model\n","print(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:.2f}\")\n","print(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")\n"]},{"cell_type":"code","source":["\"\"\"\n","A simple walkthrough of how to code a fully connected neural network\n","using the PyTorch library. For demonstration we train it on the very\n","common MNIST dataset of handwritten digits. In this code we go through\n","how to create the network as well as initialize a loss function, optimizer,\n","check accuracy and more.\n","Programmed by Aladdin Persson\n","* 2020-04-08: Initial coding\n","* 2021-03-24: Added more detailed comments also removed part of\n","              check_accuracy which would only work specifically on MNIST.\n","* 2022-09-23: Updated with more detailed comments, docstrings to functions, and checked code still functions as intended.\n","\"\"\"\n","\n","# Imports\n","import torch\n","import torch.nn.functional as F  # Parameterless functions, like (some) activation functions\n","import torchvision.datasets as datasets  # Standard datasets\n","import torchvision.transforms as transforms  # Transformations we can perform on our dataset for augmentation\n","from torch import optim  # For optimizers like SGD, Adam, etc.\n","from torch import nn  # All neural network modules\n","from torch.utils.data import (\n","    DataLoader,\n",")  # Gives easier dataset managment by creating mini batches etc.\n","from tqdm import tqdm  # For nice progress bar!\n","\n","\n","\n","# Set device cuda for GPU if it's available otherwise run on the CPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Hyperparameters\n","input_size = 28\n","sequence_length = 28\n","num_layers = 2\n","hidden_size = 256\n","num_classes = 10\n","learning_rate = 0.001\n","batch_size = 64\n","num_epochs = 3\n","# Here we create our simple neural network. For more details here we are subclassing and\n","# inheriting from nn.Module, this is the most general way to create your networks and\n","# allows for more flexibility. I encourage you to also check out nn.Sequential which\n","# would be easier to use in this scenario but I wanted to show you something that\n","# \"always\" works and is a general approach.\n","# class RNN(nn.Module):\n","#   def __init__(self,input_size,hidden_size,num_layers,num_classes):\n","#     super(RNN,self).__init__()\n","#     self.hidden_size = hidden_size\n","#     self.num_layers = num_layers\n","#     self.RNN = nn.RNN(input_size,hidden_size,num_layers,batch_first=True)\n","#     self.fc = nn.Linear(hidden_size*sequence_length,num_classes)\n","class RNN(nn.Module):\n","   def __init__(self,input_size,hidden_size,num_layers,num_classes):\n","     super(RNN,self).__init__()\n","     self.hidden_size = hidden_size\n","     self.num_layers = num_layers\n","     self.RNN = nn.RNN(input_size,hidden_size,num_layers,batch_first=True)\n","     self.fc = nn.Linear(hidden_size*sequence_length,num_classes)\n","\n","   def forward(self,x):\n","     h0 = torch.zeros(self.num_layers,x.size(0),self.hidden_size).to(device)\n","     out , _ = self.RNN(x,h0)\n","     out = out.reshape(out.shape[0],-1)\n","     out = self.fc(out)\n","     return out\n","\n","\n","# Load Data\n","train_dataset = datasets.MNIST(\n","    root=\"dataset/\", train=True, transform=transforms.ToTensor(), download=True\n",")\n","test_dataset = datasets.MNIST(\n","    root=\"dataset/\", train=False, transform=transforms.ToTensor(), download=True\n",")\n","train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n","\n","# Initialize network (try out just using simple RNN, or GRU, and then compare with LSTM)\n","model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Train Network\n","for epoch in range(num_epochs):\n","    for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):\n","        # Get data to cuda if possible\n","        data = data.to(device=device).squeeze(1)\n","        targets = targets.to(device=device)\n","\n","        # forward\n","        scores = model(data)\n","        loss = criterion(scores, targets)\n","\n","        # backward\n","        optimizer.zero_grad()\n","        loss.backward()\n","\n","        # gradient descent update step/adam step\n","        optimizer.step()\n","\n","# Check accuracy on training & test to see how good our model\n","def check_accuracy(loader, model):\n","    num_correct = 0\n","    num_samples = 0\n","\n","    # Set model to eval\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x = x.to(device=device).squeeze(1)\n","            y = y.to(device=device)\n","\n","            scores = model(x)\n","            _, predictions = scores.max(1)\n","            num_correct += (predictions == y).sum()\n","            num_samples += predictions.size(0)\n","\n","    # Toggle model back to train\n","    model.train()\n","    return num_correct / num_samples\n","\n","\n","print(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")\n","print(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")"],"metadata":{"id":"8-ggeKVPCMYL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","A simple walkthrough of how to code a fully connected neural network\n","using the PyTorch library. For demonstration we train it on the very\n","common MNIST dataset of handwritten digits. In this code we go through\n","how to create the network as well as initialize a loss function, optimizer,\n","check accuracy and more.\n","Programmed by Aladdin Persson\n","* 2020-04-08: Initial coding\n","* 2021-03-24: Added more detailed comments also removed part of\n","              check_accuracy which would only work specifically on MNIST.\n","* 2022-09-23: Updated with more detailed comments, docstrings to functions, and checked code still functions as intended.\n","\"\"\"\n","\n","# Imports\n","import torch\n","import torch.nn.functional as F  # Parameterless functions, like (some) activation functions\n","import torchvision.datasets as datasets  # Standard datasets\n","import torchvision.transforms as transforms  # Transformations we can perform on our dataset for augmentation\n","from torch import optim  # For optimizers like SGD, Adam, etc.\n","from torch import nn  # All neural network modules\n","from torch.utils.data import (\n","    DataLoader,\n",")  # Gives easier dataset managment by creating mini batches etc.\n","from tqdm import tqdm  # For nice progress bar!\n","\n","\n","\n","# Set device cuda for GPU if it's available otherwise run on the CPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Hyperparameters\n","input_size = 28\n","sequence_length = 28\n","num_layers = 2\n","hidden_size = 256\n","num_classes = 10\n","learning_rate = 0.001\n","batch_size = 64\n","num_epochs = 3\n","# Here we create our simple neural network. For more details here we are subclassing and\n","# inheriting from nn.Module, this is the most general way to create your networks and\n","# allows for more flexibility. I encourage you to also check out nn.Sequential which\n","# would be easier to use in this scenario but I wanted to show you something that\n","# \"always\" works and is a general approach.\n","# class RNN(nn.Module):\n","#   def __init__(self,input_size,hidden_size,num_layers,num_classes):\n","#     super(RNN,self).__init__()\n","#     self.hidden_size = hidden_size\n","#     self.num_layers = num_layers\n","#     self.RNN = nn.RNN(input_size,hidden_size,num_layers,batch_first=True)\n","#     self.fc = nn.Linear(hidden_size*sequence_length,num_classes)\n","class GRU(nn.Module):\n","   def __init__(self,input_size,hidden_size,num_layers,num_classes):\n","     super(GRU,self).__init__()\n","     self.hidden_size = hidden_size\n","     self.num_layers = num_layers\n","     self.GRU = nn.GRU(input_size,hidden_size,num_layers,batch_first=True)\n","     self.fc = nn.Linear(hidden_size*sequence_length,num_classes)\n","\n","   def forward(self,x):\n","     h0 = torch.zeros(self.num_layers,x.size(0),self.hidden_size).to(device)\n","     out , _ = self.GRU(x,h0)\n","     out = out.reshape(out.shape[0],-1)\n","     out = self.fc(out)\n","     return out\n","\n","\n","# Load Data\n","train_dataset = datasets.MNIST(\n","    root=\"dataset/\", train=True, transform=transforms.ToTensor(), download=True\n",")\n","test_dataset = datasets.MNIST(\n","    root=\"dataset/\", train=False, transform=transforms.ToTensor(), download=True\n",")\n","train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n","\n","# Initialize network (try out just using simple RNN, or GRU, and then compare with LSTM)\n","model = GRU(input_size, hidden_size, num_layers, num_classes).to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Train Network\n","for epoch in range(num_epochs):\n","    for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):\n","        # Get data to cuda if possible\n","        data = data.to(device=device).squeeze(1)\n","        targets = targets.to(device=device)\n","\n","        # forward\n","        scores = model(data)\n","        loss = criterion(scores, targets)\n","\n","        # backward\n","        optimizer.zero_grad()\n","        loss.backward()\n","\n","        # gradient descent update step/adam step\n","        optimizer.step()\n","\n","# Check accuracy on training & test to see how good our model\n","def check_accuracy(loader, model):\n","    num_correct = 0\n","    num_samples = 0\n","\n","    # Set model to eval\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x = x.to(device=device).squeeze(1)\n","            y = y.to(device=device)\n","\n","            scores = model(x)\n","            _, predictions = scores.max(1)\n","            num_correct += (predictions == y).sum()\n","            num_samples += predictions.size(0)\n","\n","    # Toggle model back to train\n","    model.train()\n","    return num_correct / num_samples\n","\n","\n","print(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")\n","print(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")"],"metadata":{"id":"WFVowyPLHA2g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","A simple walkthrough of how to code a fully connected neural network\n","using the PyTorch library. For demonstration we train it on the very\n","common MNIST dataset of handwritten digits. In this code we go through\n","how to create the network as well as initialize a loss function, optimizer,\n","check accuracy and more.\n","Programmed by Aladdin Persson\n","* 2020-04-08: Initial coding\n","* 2021-03-24: Added more detailed comments also removed part of\n","              check_accuracy which would only work specifically on MNIST.\n","* 2022-09-23: Updated with more detailed comments, docstrings to functions, and checked code still functions as intended.\n","\"\"\"\n","\n","# Imports\n","import torch\n","import torch.nn.functional as F  # Parameterless functions, like (some) activation functions\n","import torchvision.datasets as datasets  # Standard datasets\n","import torchvision.transforms as transforms  # Transformations we can perform on our dataset for augmentation\n","from torch import optim  # For optimizers like SGD, Adam, etc.\n","from torch import nn  # All neural network modules\n","from torch.utils.data import (\n","    DataLoader,\n",")  # Gives easier dataset managment by creating mini batches etc.\n","from tqdm import tqdm  # For nice progress bar!\n","\n","\n","\n","# Set device cuda for GPU if it's available otherwise run on the CPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Hyperparameters\n","input_size = 28\n","sequence_length = 28\n","num_layers = 2\n","hidden_size = 256\n","num_classes = 10\n","learning_rate = 0.001\n","batch_size = 64\n","num_epochs = 3\n","# Here we create our simple neural network. For more details here we are subclassing and\n","# inheriting from nn.Module, this is the most general way to create your networks and\n","# allows for more flexibility. I encourage you to also check out nn.Sequential which\n","# would be easier to use in this scenario but I wanted to show you something that\n","# \"always\" works and is a general approach.\n","# class RNN(nn.Module):\n","#   def __init__(self,input_size,hidden_size,num_layers,num_classes):\n","#     super(RNN,self).__init__()\n","#     self.hidden_size = hidden_size\n","#     self.num_layers = num_layers\n","#     self.RNN = nn.RNN(input_size,hidden_size,num_layers,batch_first=True)\n","#     self.fc = nn.Linear(hidden_size*sequence_length,num_classes)\n","class LSTM(nn.Module):\n","   def __init__(self,input_size,hidden_size,num_layers,num_classes):\n","     super(LSTM,self).__init__()\n","     self.hidden_size = hidden_size\n","     self.num_layers = num_layers\n","     self.LSTM = nn.LSTM(input_size,hidden_size,num_layers,batch_first=True)\n","     self.fc = nn.Linear(hidden_size*sequence_length,num_classes)\n","\n","   def forward(self,x):\n","     h0 = torch.zeros(self.num_layers,x.size(0),self.hidden_size).to(device)\n","     c0 = torch.zeros(self.num_laylers,x.size(0),self.hidden_size).to(device)\n","     out , _ = self.LSTM(x,(h0,c0))\n","     (x,h0)\n","     out = out.reshape(out.shape[0],-1)\n","     out = self.fc(out)\n","     return out\n","\n","\n","# Load Data\n","train_dataset = datasets.MNIST(\n","    root=\"dataset/\", train=True, transform=transforms.ToTensor(), download=True\n",")\n","test_dataset = datasets.MNIST(\n","    root=\"dataset/\", train=False, transform=transforms.ToTensor(), download=True\n",")\n","train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n","\n","# Initialize network (try out just using simple RNN, or GRU, and then compare with LSTM)\n","model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Train Network\n","for epoch in range(num_epochs):\n","    for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):\n","        # Get data to cuda if possible\n","        data = data.to(device=device).squeeze(1)\n","        targets = targets.to(device=device)\n","\n","        # forward\n","        scores = model(data)\n","        loss = criterion(scores, targets)\n","\n","        # backward\n","        optimizer.zero_grad()\n","        loss.backward()\n","\n","        # gradient descent update step/adam step\n","        optimizer.step()\n","\n","# Check accuracy on training & test to see how good our model\n","def check_accuracy(loader, model):\n","    num_correct = 0\n","    num_samples = 0\n","\n","    # Set model to eval\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x = x.to(device=device).squeeze(1)\n","            y = y.to(device=device)\n","\n","            scores = model(x)\n","            _, predictions = scores.max(1)\n","            num_correct += (predictions == y).sum()\n","            num_samples += predictions.size(0)\n","\n","    # Toggle model back to train\n","    model.train()\n","    return num_correct / num_samples\n","\n","\n","print(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")\n","print(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")"],"metadata":{"id":"vmpa_PhtIf7u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jpgEBJarKu0w"},"execution_count":null,"outputs":[]}]}