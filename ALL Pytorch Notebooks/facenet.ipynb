{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyPPZY0WbBBBFOMDuRuerWBG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision.transforms import transforms\n","\n","\n","# Define dataset and transform\n","transform = transforms.Compose([\n","    transforms.Resize((128, 128)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n","train_dataset = FaceDataset('train', transform=transform)\n","test_dataset = FaceDataset('test', transform=transform)\n","\n","# Define data loader\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32)"],"metadata":{"id":"_PHQw3jdKEOX"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bzXSLqthJ37K"},"outputs":[],"source":["\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class DahwinFaceNet(nn.Module):\n","    def __init__(self):\n","        super(self).__init__()\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n","        self.bn2 = nn.BatchNorm2d(64)\n","        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n","        self.bn3 = nn.BatchNorm2d(128)\n","        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n","        self.bn4 = nn.BatchNorm2d(128)\n","        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n","        self.bn5 = nn.BatchNorm2d(256)\n","        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n","        self.bn6 = nn.BatchNorm2d(256)\n","        self.conv7 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n","        self.bn7 = nn.BatchNorm2d(512)\n","        self.conv8_1 = nn.Conv2d(512, 128, kernel_size=1, stride=1, padding=0)\n","        self.conv8_2 = nn.Conv2d(128, 128, kernel_size=7, stride=1, padding=0)\n","        self.conv9_1 = nn.Conv2d(128, 512, kernel_size=1, stride=1, padding=0)\n","        self.conv9_2 = nn.Conv2d(512, 512, kernel_size=7, stride=1, padding=0)\n","        self.conv10_1 = nn.Conv2d(512, 128, kernel_size=1, stride=1, padding=0)\n","        self.conv10_2 = nn.Conv2d(128, 128, kernel_size=7, stride=1, padding=0)\n","        self.conv11_1 = nn.Conv2d(128, 512, kernel_size=1, stride=1, padding=0)\n","        self.conv11_2 = nn.Conv2d(512, 512, kernel_size=7, stride=1, padding=0)\n","        self.fc = nn.Linear(512 * 7 * 7, 128)\n","        self.bn8 = nn.BatchNorm1d(128)\n","\n","    def forward(self, x):\n","      # Feature extraction\n","      x = F.relu(self.bn1(self.conv1(x)))\n","      x = F.max_pool2d(F.relu(self.bn2(self.conv2(x))), 2)\n","      x = F.relu(self.bn3(self.conv3(x)))\n","      x = F.max_pool2d(F.relu(self.bn4(self.conv4(x))), 2)\n","      x = F.relu(self.bn5(self.conv5(x)))\n","      x = F.max_pool2d(F.relu(self.bn6(self.conv6(x))), 2)\n","      x = F.relu(self.bn7(self.conv7(x)))\n","      x = F.max_pool2d(x, 2)\n","      x = F.relu(self.conv8_1(self.conv8_2(x)))\n","      x = F.relu(self.conv9_1(self.conv9_2(x)))\n","      x = F.relu(self.conv10_1(self.conv10_2(x)))\n","      x = F.relu(self.conv11_1(self.conv11_2(x)))\n","      # Flatten\n","      x = x.view(-1, 512 * 7 * 7)\n","      # Fully connected layer and L2 normalization\n","      x = self.fc(x)\n","      x = F.normalize(self.bn8(x), p=2, dim=1)\n","      return x\n","torch.manual_seed(42)\n","model = DahwinFaceNet()\n","\n","\n","\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Train model\n","for epoch in range(10):\n","    model.train()\n","    for inputs, labels in train_loader:\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Evaluate model\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    accuracy = 100 * correct / total\n","    print('Epoch: {}, Test Accuracy: {}%'.format(epoch, accuracy))\n","\n","# Save model\n","torch.save(model.state_dict(), 'model.pth')\n"]},{"cell_type":"code","source":["    # Evaluate model\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    test_loss = 0  # initialize test loss\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            test_loss += loss.item() * labels.size(0)  # accumulate test loss\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","        accuracy = 100 * correct / total\n","        test_loss /= total  # calculate average test loss\n","        print('Epoch: {}, Test Accuracy: {}%, Test Loss: {:.4f}'.format(epoch, accuracy, test_loss))"],"metadata":{"id":"EfFe4JRK4k1w"},"execution_count":null,"outputs":[]}]}