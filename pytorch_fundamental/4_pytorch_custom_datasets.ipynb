{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ig-pK2aRs3fc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "# Setup device-agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Get data\n",
        "First thing's first we need some data.\n",
        "\n",
        "And like any good cooking show, some data has already been prepared for us.\n",
        "\n",
        "We're going to start small.\n",
        "\n",
        "Because we're not looking to train the biggest model or use the biggest dataset yet.\n",
        "\n",
        "Machine learning is an iterative process, start small, get something working and increase when necessary.\n",
        "\n",
        "The data we're going to be using is a subset of the Food101 dataset.\n",
        "\n",
        "Food101 is popular computer vision benchmark as it contains 1000 images of 101 different kinds of foods, totaling 101,000 images (75,750 train and 25,250 test).\n",
        "\n",
        "Can you think of 101 different foods?\n",
        "\n",
        "Can you think of a computer program to classify 101 foods?\n",
        "\n",
        "I can.\n",
        "\n",
        "A machine learning model!\n",
        "\n",
        "Specifically, a PyTorch computer vision model like we covered in notebook 03.\n",
        "\n",
        "Instead of 101 food classes though, we're going to start with 3: pizza, steak and sushi.\n",
        "\n",
        "And instead of 1,000 images per class, we're going to start with a random 10% (start small, increase when necessary)."
      ],
      "metadata": {
        "id": "W-UhNPXJwyFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup path to data folder\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"pizza_steak_sushi\"\n",
        "\n",
        "# If the image folder doesn't exist, download it and prepare it... \n",
        "if image_path.is_dir():\n",
        "    print(f\"{image_path} directory exists.\")\n",
        "else:\n",
        "    print(f\"Did not find {image_path} directory, creating one...\")\n",
        "    image_path.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Download pizza, steak, sushi data\n",
        "    with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
        "        request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
        "        print(\"Downloading pizza, steak, sushi data...\")\n",
        "        f.write(request.content)\n",
        "\n",
        "    # Unzip pizza, steak, sushi data\n",
        "    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
        "        print(\"Unzipping pizza, steak, sushi data...\") \n",
        "        zip_ref.extractall(image_path)"
      ],
      "metadata": {
        "id": "oFhLuUtBwknw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# extra\n",
        "# 2. Become one with the data (data preparation)"
      ],
      "metadata": {
        "id": "MGA7IC10yVow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_path"
      ],
      "metadata": {
        "id": "wPYmTCQMyOD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# extra\n"
      ],
      "metadata": {
        "id": "MAl3yiQt30v6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def walk_through_dir(dir_path):\n",
        "  \"\"\"\n",
        "  Walks through dir_path returning its contents.\n",
        "  Args:\n",
        "    dir_path (str or pathlib.Path): target directory\n",
        "  \n",
        "  Returns:\n",
        "    A print out of:\n",
        "      number of subdiretories in dir_path\n",
        "      number of images (files) in each subdirectory\n",
        "      name of each subdirectory\n",
        "  \"\"\"\n",
        "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
        "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
      ],
      "metadata": {
        "id": "Di14PalXyLXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "walk_through_dir(image_path)"
      ],
      "metadata": {
        "id": "CFryJsxG4ric"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup train and testing paths\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\"\n",
        "\n",
        "train_dir, test_dir"
      ],
      "metadata": {
        "id": "dapLCQOZ4vi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1 Visualize an image"
      ],
      "metadata": {
        "id": "Nk2ANy7po8pq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "# Set seed\n",
        "# random.seed(42) # <- try changing this and see what happens\n",
        "\n",
        "# 1. Get all image paths (* means \"any combination\")\n",
        "\n",
        "image_path_list = list(image_path.glob(\"*/*/*.jpg\")) + list(image_path.glob(\"*/*/*.png\"))\n",
        "\n",
        "\n",
        "# 2. Get random image path\n",
        "random_image_path = random.choice(image_path_list)\n",
        "\n",
        "# 3. Get image class from path name (the image class is the name of the directory where the image is stored)\n",
        "image_class = random_image_path.parent.stem\n",
        "\n",
        "# 4. Open image\n",
        "img = Image.open(random_image_path)\n",
        "\n",
        "# 5. Print metadata\n",
        "print(f\"Random image path: {random_image_path}\")\n",
        "print(f\"Image class: {image_class}\")\n",
        "print(f\"Image height: {img.height}\") \n",
        "\n",
        "print(f\"Image width: {img.width}\")\n",
        "img"
      ],
      "metadata": {
        "id": "oQ1ro4Be414z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "img_as_array = np.asarray(img)\n",
        "\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.imshow(img_as_array)\n",
        "plt.title(f\"image class:{image_class} | image shape: {img_as_array.shape}\")\n",
        "plt.axis(False)"
      ],
      "metadata": {
        "id": "gKaeMdMxx-jA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets,transforms"
      ],
      "metadata": {
        "id": "vbQSyPlM1i_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tranforming data into tensor\n",
        "Vision:\ttorchvision.datasets\n",
        "\n",
        "\n",
        "Audio:\ttorchaudio.datasets\n",
        "\n",
        "\n",
        "Text:\ttorchtext.datasets\n",
        "\n",
        "\n",
        "Recommendation system:\ttorchrec.datasets"
      ],
      "metadata": {
        "id": "F7nuqwxP13o-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize(size=(224,224)),\n",
        "    \n",
        "    transforms.ToTensor()\n",
        "])\n",
        "data_transform_test = transforms.Compose([\n",
        "    transforms.Resize(size=(224,224)),\n",
        "    \n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "zs6MvCHgSsfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.1 Transforming data with torchvision.transforms"
      ],
      "metadata": {
        "id": "hAgWvmN8Sqs4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# just for plot"
      ],
      "metadata": {
        "id": "OAduWJHX72Rh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_transformed_images(image_path_list, transformed_data, n, seed=32):\n",
        "    \"\"\"Plots a series of random images from image_paths.\n",
        "\n",
        "    Will open n image paths from image_paths, transform them\n",
        "    with transform and plot them side by side.\n",
        "\n",
        "    Args:\n",
        "        image_paths (list): List of target image paths. \n",
        "        transform (PyTorch Transforms): Transforms to apply to images.\n",
        "        n (int, optional): Number of images to plot. Defaults to 3.\n",
        "        seed (int, optional): Random seed for the random generator. Defaults to 42.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    random_image_paths = random.sample(image_path_list, k=n)\n",
        "    for image_path in random_image_paths:\n",
        "        with Image.open(image_path) as f:\n",
        "            fig, ax = plt.subplots(1, 2)\n",
        "            ax[0].imshow(f) \n",
        "            ax[0].set_title(f\"Original \\nSize: {f.size}\")\n",
        "            ax[0].axis(\"off\")\n",
        "\n",
        "            # Transform and plot image\n",
        "            # Note: permute() will change shape of image to suit matplotlib \n",
        "            # (PyTorch default is [C, H, W] but Matplotlib is [H, W, C])\n",
        "            transformed_image = transformed_data(f).permute(1, 2, 0) \n",
        "            ax[1].imshow(transformed_image) \n",
        "            ax[1].set_title(f\"Transformed \\nSize: {transformed_image.shape}\")\n",
        "            ax[1].axis(\"off\")\n",
        "\n",
        "            fig.suptitle(f\"Class: {image_path.parent.stem}\", fontsize=16)\n"
      ],
      "metadata": {
        "id": "m63GPANtTREw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. option 1: Loading image "
      ],
      "metadata": {
        "id": "p88xt2sCGDei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_transformed_images(image_path_list,data_transform,n=2)"
      ],
      "metadata": {
        "id": "cc_zxYfzV6jD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "train_data = datasets.ImageFolder(root=train_dir,transform=data_transform,target_transform=None)\n",
        "test_data = datasets.ImageFolder(root=test_dir,transform=data_transform)\n",
        "print(f\"Train data:\\n{train_data}\\n{test_data}\")\n"
      ],
      "metadata": {
        "id": "31IL_Hv1t1S9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_data.classes"
      ],
      "metadata": {
        "id": "KPhQv8jGNB58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names"
      ],
      "metadata": {
        "id": "OpBgfghVNKUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_dict = train_data.class_to_idx\n",
        "class_dict"
      ],
      "metadata": {
        "id": "j4oY2t7QNMLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data),len(test_data)"
      ],
      "metadata": {
        "id": "WaMPet8WNUhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img,label = train_data[200]\n",
        "print(f\"image tensor:\\n{img}\")\n",
        "print(f\"image shape:{img.shape}\")\n",
        "print(f\"image datatype:{img.dtype}\")\n",
        "print(f\"image label: {label}\")\n",
        "print(f\"label datatype:{type(label)}\")"
      ],
      "metadata": {
        "id": "MiEaxCTdNaEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_permute = img.permute(1,2,0)\n",
        "print(f\"Original shape:{img.shape}\")\n",
        "print(f\"image permute shape:{img_permute.shape}\")\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.imshow(img.permute(1,2,0))\n",
        "plt.axis('off')\n",
        "plt.title(class_names[label],fontsize=14)"
      ],
      "metadata": {
        "id": "8DA3KopnOSax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.1 Turn loaded images into DataLoader's"
      ],
      "metadata": {
        "id": "Bwg3yNjPR9EI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "train_dataloader = DataLoader(dataset=train_data,batch_size=1,num_workers=1,shuffle=False)\n",
        "test_dataloader = DataLoader(dataset=test_data,batch_size=1,num_workers=1,shuffle=False)\n",
        "train_dataloader,test_dataloader"
      ],
      "metadata": {
        "id": "UAYRkhxkPZ_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img , label = next(iter(train_dataloader))\n",
        "print(f\"image shape:{img.shape}\")\n",
        "print(f\"label shape: {label.shape}\")"
      ],
      "metadata": {
        "id": "OSIl2O5Dl4WP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from typing import Tuple, Dict,List\n"
      ],
      "metadata": {
        "id": "0lQ0StciNWAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.classes,train_data.class_to_idx"
      ],
      "metadata": {
        "id": "gF20326zMp2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Creating a helper fuction to get class names\n",
        "we want a fuction to \n",
        "1. get the class names using os.scandir( to traverse a target directory (ideally) the directory is in standard image classification format).\n",
        "2. Rasie an error if the class names aren't fount (if this hapens,there might be something wrong with the directory structure.\n",
        "3. Turn the class names into a dict and a list and return them.\n"
      ],
      "metadata": {
        "id": "JqgBqMz-M9XQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup path for target directory\n",
        "target_directory = train_dir\n",
        "print(f\"Target directory: {target_directory}\")\n",
        "\n",
        "# Get the class names from the target directory\n",
        "class_names_found = sorted([entry.name for entry in list(os.scandir(image_path / \"train\"))])\n",
        "print(f\"Class names found: {class_names_found}\")\n"
      ],
      "metadata": {
        "id": "RLCO6GmEITYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_name_found = sorted(entry.name for entry in list(os.scandir(train_dir)))\n",
        "class_name_found"
      ],
      "metadata": {
        "id": "-Hp8NrIOM735"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_classes(directory: str) -> Tuple[List[str], Dict[str, int]]:\n",
        "    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
        "    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
        "    return classes, class_to_idx"
      ],
      "metadata": {
        "id": "P3KVx9BvOimO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_classes(train_dir)"
      ],
      "metadata": {
        "id": "y3GW1iKoPqWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.2 create a custom dataset to replicate 'imagefolder'\n",
        "To create our own custom dataset, we want to \n",
        "1. subclass 'torch.utils.data.Dataset'\n",
        "2. init our subclass with a target directory (the directory we'd like to get daa from( as well as a transform if we'd like to transform our data.\n",
        "3. create several attributes:\n",
        "* paths - paths of our images\n",
        "* transform - the transform we'd like to use\n",
        "* clalssed - a list of the target classess\n",
        "* class_to_idx = a dict of the traget class mapped to integer lablels\n",
        "4. creeate a fuction to 'load_images()' , this fuction will open an image\n",
        "5. operwrite the __len()__ method to return the leangth of or dataset\n",
        "6. overwrite the __getitem()__ method to return a given sample when passed an index\n"
      ],
      "metadata": {
        "id": "vD0PDg_4U42k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "# 1. subclass\n",
        "class ImageFolderCustom(Dataset):\n",
        "    # 2. Initialize custom dataset\n",
        "    def __init__(self, targ_dir: str, transform=None):\n",
        "        # 3. create class attributes\n",
        "        # Get all of the image paths\n",
        "        self.paths = list(pathlib.Path(targ_dir).glob('*/*.jpg'))\n",
        "        self.transform = transform\n",
        "        self.classes, self.class_to_idx = find_classes(targ_dir)\n",
        "\n",
        "    # 4. create a function to load images\n",
        "    def load_image(self, index: int) -> Image.Image:\n",
        "        image_path = self.paths[index]\n",
        "        return Image.open(image_path)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.paths)\n",
        "\n",
        "    # 6 overwrite __getitem__() method to return a particular sample\n",
        "    def __getitem__(self, index: int) -> Tuple[torch.tensor, int]:\n",
        "        img = self.load_image(index)\n",
        "        class_name = self.paths[index].parent.name\n",
        "        class_idx = self.class_to_idx[class_name]\n",
        "\n",
        "        # transform if necessary\n",
        "        if self.transform:\n",
        "            return self.transform(img), class_idx\n",
        "        else:\n",
        "            return img, class_idx\n",
        "\n"
      ],
      "metadata": {
        "id": "aGeuetwjP_Vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Augment train data\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Don't augment test data, only reshape\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "QrsRoHTCIt5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_custom = ImageFolderCustom(train_dir,data_transform)\n",
        "test_data_custom = ImageFolderCustom(test_dir,data_transform_test)\n",
        "train_data_custom,test_data_custom"
      ],
      "metadata": {
        "id": "52OoAtsn4FOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_custom.classes, train_data_custom.class_to_idx"
      ],
      "metadata": {
        "id": "8GRhtXelOjeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for equality amongst our custom Dataset and ImageFolder Dataset\n",
        "print((len(train_data_custom) == len(train_data)) & (len(test_data_custom) == len(test_data)))\n",
        "print(train_data_custom.classes == train_data.classes)\n",
        "print(train_data_custom.class_to_idx == train_data.class_to_idx)"
      ],
      "metadata": {
        "id": "y_0bLCSOJdi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.3 create a funtion to display random images\n",
        "\n",
        "1. take in a 'dataset' and a number of other parameters such as class names and how many images to visualize.\n",
        "2. To prevent the display getting out of hand let's cap the number of images to see at 10\n",
        "3. Set the random seed for reproducbility\n",
        "4. Get a list of random sample indexes from the target dataset\n",
        "5. setup a matplotlilb plot\n",
        "6. Loop through the random sample images and plot them with matplotlib\n",
        "7. Make sure the dimesions of our images line up with amtplotlib (hwc)\n"
      ],
      "metadata": {
        "id": "kovKIdm7SCUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Take in a Dataset as well as a list of class names\n",
        "def display_random_images(dataset: torch.utils.data.dataset.Dataset,\n",
        "                          classes: List[str] = None,\n",
        "                          n: int = 10,\n",
        "                          display_shape: bool = True,\n",
        "                          seed: int = None):\n",
        "    \n",
        "    # 2. Adjust display if n too high\n",
        "    if n > 10:\n",
        "        n = 10\n",
        "        display_shape = False\n",
        "        print(f\"For display purposes, n shouldn't be larger than 10, setting to 10 and removing shape display.\")\n",
        "    \n",
        "    # 3. Set random seed\n",
        "    if seed:\n",
        "        random.seed(seed)\n",
        "\n",
        "    # 4. Get random sample indexes\n",
        "    random_samples_idx = random.sample(range(len(dataset)), k=n)\n",
        "\n",
        "    # 5. Setup plot\n",
        "    plt.figure(figsize=(16, 8))\n",
        "\n",
        "    # 6. Loop through samples and display random samples \n",
        "    for i, targ_sample in enumerate(random_samples_idx):\n",
        "        targ_image, targ_label = dataset[targ_sample][0], dataset[targ_sample][1]\n",
        "\n",
        "        # 7. Adjust image tensor shape for plotting: [color_channels, height, width] -> [color_channels, height, width]\n",
        "        targ_image_adjust = targ_image.permute(1, 2, 0)\n",
        "\n",
        "        # Plot adjusted samples\n",
        "        plt.subplot(1, n, i+1)\n",
        "        plt.imshow(targ_image_adjust)\n",
        "        plt.axis(\"off\")\n",
        "        if classes:\n",
        "            title = f\"class: {classes[targ_label]}\"\n",
        "            if display_shape:\n",
        "                title = title + f\"\\n: {targ_image_adjust.shape}\"\n",
        "        plt.title(title)\n"
      ],
      "metadata": {
        "id": "JtMXwmNJQtYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display random images from  the dataset\n",
        "display_random_images(train_data,n=10,classes=class_names,seed=None)"
      ],
      "metadata": {
        "id": "KIeeGBtLVSia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display random images from ImageFolder created Dataset\n",
        "display_random_images(train_data, \n",
        "                      n=5, \n",
        "                      classes=class_names,\n",
        "                      seed=None)"
      ],
      "metadata": {
        "id": "yLM7bHIqb7pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.4 Turn custom loaded images into DataLoader's"
      ],
      "metadata": {
        "id": "DCPeu7HTfD6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn train and test custom Dataset's into DataLoader's\n",
        "from torch.utils.data import DataLoader\n",
        "train_dataloader_custom = DataLoader(dataset=train_data_custom, # use custom created train Dataset\n",
        "                                     batch_size=1, # how many samples per batch?\n",
        "                                     num_workers=0, # how many subprocesses to use for data loading? (higher = more)\n",
        "                                     shuffle=True) # shuffle the data?\n",
        "\n",
        "test_dataloader_custom = DataLoader(dataset=test_data_custom, # use custom created test Dataset\n",
        "                                    batch_size=1, \n",
        "                                    num_workers=0, \n",
        "                                    shuffle=False) # don't usually need to shuffle testing data\n",
        "\n",
        "train_dataloader_custom, test_dataloader_custom"
      ],
      "metadata": {
        "id": "oqmnOegCexqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_custom,label_custom = next(iter(train_dataloader_custom))\n",
        "print(f\"image shape:{img_custom.shape}\")\n",
        "print(f\"lalbel shape:{label_custom.shape}\")"
      ],
      "metadata": {
        "id": "xo7B24EFfuTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Other forms of transforms (data augemtation)\n"
      ],
      "metadata": {
        "id": "iCzmojhUgqEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_transformed_images(image_path_list,train_transforms,n=3)"
      ],
      "metadata": {
        "id": "ZWgjc_VDjR4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.TrivialAugmentWide(num_magnitude_bins=31), # how intense \n",
        "    transforms.ToTensor() # use ToTensor() last to get everything between 0 & 1\n",
        "])\n",
        "\n",
        "# Don't need to perform augmentation on the test data\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224,224)), \n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "1ydZWRREgjEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get image and label from custom DataLoader\n",
        "img_custom, label_custom = next(iter(train_dataloader_custom))\n",
        "\n",
        "# Batch size will now be 1, try changing the batch_size parameter above and see what happens\n",
        "print(f\"Image shape: {img_custom.shape} -> [batch_size, color_channels, height, width]\")\n",
        "print(f\"Label shape: {label_custom.shape}\")"
      ],
      "metadata": {
        "id": "Tx7C0NX7NmX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Model 0: TinyVGG without dataaugmentation"
      ],
      "metadata": {
        "id": "cnkq2RGajn17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.cpu_count()"
      ],
      "metadata": {
        "id": "iI9Ok1tYjjYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create simple transform\n",
        "simple_transform = transforms.Compose([ \n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "ZXOwuDAlthn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load and transform data\n",
        "from torchvision import datasets\n",
        "train_data_simple = datasets.ImageFolder(root=train_dir, transform=simple_transform)\n",
        "test_data_simple = datasets.ImageFolder(root=test_dir, transform=simple_transform)\n",
        "\n",
        "# 2. Turn data into DataLoaders\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Setup batch size and number of workers \n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "print(f\"Creating DataLoader's with batch size {BATCH_SIZE} and {NUM_WORKERS} workers.\")\n",
        "\n",
        "# Create DataLoader's\n",
        "train_dataloader_simple = DataLoader(train_data_simple, \n",
        "                                     batch_size=BATCH_SIZE, \n",
        "                                     shuffle=True, \n",
        "                                     num_workers=NUM_WORKERS)\n",
        "\n",
        "test_dataloader_simple = DataLoader(test_data_simple, \n",
        "                                    batch_size=BATCH_SIZE, \n",
        "                                    shuffle=False, \n",
        "                                    num_workers=NUM_WORKERS)\n",
        "\n",
        "train_dataloader_simple, test_dataloader_simple"
      ],
      "metadata": {
        "id": "GegvbB7dtH1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DahwinTinyVGG(nn.Module):\n",
        "    \"\"\"\n",
        "    Model architecture copying TinyVGG from: \n",
        "    https://poloclub.github.io/cnn-explainer/\n",
        "    \"\"\"\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
        "        super().__init__()\n",
        "        self.conv_block_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_shape, \n",
        "                      out_channels=hidden_units, \n",
        "                      kernel_size=3, # how big is the square that's going over the image?\n",
        "                      stride=1, # default\n",
        "                      padding=1), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_units, \n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2,\n",
        "                         stride=2) # default stride value is same as kernel_size\n",
        "        )\n",
        "        self.conv_block_2 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            # Where did this in_features shape come from? \n",
        "            # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
        "            nn.Linear(in_features=hidden_units*56*56,\n",
        "                      out_features=output_shape)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.conv_block_1(x)\n",
        "        # print(x.shape)\n",
        "        x = self.conv_block_2(x)\n",
        "        print(x.shape)\n",
        "        x = self.classifier(x)\n",
        "        # print(x.shape)\n",
        "        return x\n",
        "        # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # <- leverage the benefits of operator fusion\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model_0 = DahwinTinyVGG(input_shape=3, # number of color channels (3 for RGB) \n",
        "                  hidden_units=10, \n",
        "                  output_shape=len(train_data.classes)).to(device)\n",
        "model_0"
      ],
      "metadata": {
        "id": "eNv3XbWvR3dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image,l= next(iter(train_dataloader_custom))\n",
        "\n",
        "image.shape, l.shape"
      ],
      "metadata": {
        "id": "qzWvYuYjffgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0(image.to(device))"
      ],
      "metadata": {
        "id": "LP_idnyn8VGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  pip install torchinfo"
      ],
      "metadata": {
        "id": "1_kaZKQ29RED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "summary(model_0,input_size = [1,3,224,224])"
      ],
      "metadata": {
        "id": "IVAHAxCQBgoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.5 Create train & test loop functions"
      ],
      "metadata": {
        "id": "HdR7DPSwDdSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model:torch.nn.Module,dataloader:torch.utils.data.DataLoader,loss_fn:torch.nn.Module,optimizer:torch.optim.Optimizer):\n",
        "  model.train()\n",
        "  train_loss,train_acc = 0,0\n",
        "  for batch ,(x,y) in enumerate(dataloader):\n",
        "    x,y = x.to(device),y.to(device)\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred,y)\n",
        "    train_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    y_pred_class = torch.argmax(torch.softmax(y_pred,dim=1),dim=1)\n",
        "    train_acc += (y_pred_class==y).sum().item()/len(y_pred)\n",
        "\n",
        "  train_loss = train_loss/len(dataloader)\n",
        "  train_acc = train_acc/len(dataloader)\n",
        "  return train_loss,train_acc\n"
      ],
      "metadata": {
        "id": "LEqEDuwlD6ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(model:torch.nn.Module,\n",
        "              dataloader:torch.utils.data.DataLoader,\n",
        "              loss_fn:torch.nn.Module):\n",
        "  model.eval()\n",
        "  test_loss,test_acc =0,0\n",
        "  with torch.inference_mode():\n",
        "    for batch ,(X,y) in enumerate(dataloader):\n",
        "      X,y = X.to(device),y.to(device)\n",
        "      test_pred_logits = model(X)\n",
        "      loss = loss_fn(test_pred_logits,y)\n",
        "      test_loss += loss.item()\n",
        "      test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "      test_acc += ((test_pred_labels==y).sum().item()/len(test_pred_labels))\n",
        "  test_loss = test_loss/len(dataloader)\n",
        "  test_acc = test_acc/len(dataloader)\n",
        "  return test_loss,test_acc\n",
        "     \n"
      ],
      "metadata": {
        "id": "1khsIyy1eKZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "def train(model:torch.nn.Module,train_dataloader:torch.utils.data.DataLoader,test_dataloader:torch.utils.data.DataLoader,optimizer:torch.optim.Optimizer,loss_fn:torch.nn.Module=nn.CrossEntropyLoss(),epochs:int=5):\n",
        "  results = {'train_loss':[],\n",
        "             'train_acc':[],\n",
        "             'test_loss':[],\n",
        "             'test_acc':[]}\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "    train_loss ,train_acc = train_step(model=model,dataloader=train_dataloader,loss_fn=loss_fn,optimizer=optimizer)\n",
        "    test_loss,test_acc = test_step(model=model,dataloader=test_dataloader,loss_fn=loss_fn)\n",
        "    print(f'{epoch} | Train loss:{train_loss:.4f} |Train acc:{train_acc:.4f} | Test loss: {test_loss:.4f} | Test acc: {test_acc:.4f}')\n",
        "    results['train_loss'].append(train_loss)\n",
        "    results['train_acc'].append(train_acc)\n",
        "    results['test_loss'].append(test_loss)\n",
        "    results['test_acc'].append(test_acc)\n",
        "  return results"
      ],
      "metadata": {
        "id": "5wCEe1BSfZZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds\n",
        "torch.manual_seed(42) \n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Set number of epochs\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# Recreate an instance of TinyVGG\n",
        "model_0 = DahwinTinyVGG(input_shape=3, # number of color channels (3 for RGB) \n",
        "                  hidden_units=10, \n",
        "                  \n",
        "                  output_shape=len(train_data.classes)).to(device)\n",
        "\n",
        "# Setup loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.001)\n",
        "\n",
        "# Start the timer\n",
        "from timeit import default_timer as timer \n",
        "start_time = timer()\n",
        "\n",
        "# Train model_0 \n",
        "model_0_results = train(model=model_0, \n",
        "                        train_dataloader=train_dataloader_simple,\n",
        "                        test_dataloader=test_dataloader_simple,\n",
        "                        optimizer=optimizer,\n",
        "                        loss_fn=loss_fn, \n",
        "                        epochs=NUM_EPOCHS,\n",
        "                        )\n",
        "\n",
        "# End the timer and print out how long it took\n",
        "end_time = timer()\n",
        "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
      ],
      "metadata": {
        "id": "hWwsHjyGmha8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "for batch_idx, (data, target) in enumerate(train_dataloader_simple):\n",
        "    # Reshape target tensor to be 1D\n",
        "    target = target.view(-1)\n",
        "\n",
        "    # Forward pass\n",
        "    output = model_0(data.to(device))\n",
        "    \n",
        "    # Calculate loss\n",
        "    loss = F.cross_entropy(output.to(device), target.to(device))\n",
        "    \n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "id": "KF3DWNf7qQob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.8 Plot loss curves of Model 0"
      ],
      "metadata": {
        "id": "aDCX0MileUDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_0_results.keys()"
      ],
      "metadata": {
        "id": "DE6b-6EO7Mhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss_curves(results: Dict[str, List[float]]):\n",
        "    \"\"\"Plots training curves of a results dictionary.\n",
        "\n",
        "    Args:\n",
        "        results (dict): dictionary containing list of values, e.g.\n",
        "            {\"train_loss\": [...],\n",
        "             \"train_acc\": [...],\n",
        "             \"test_loss\": [...],\n",
        "             \"test_acc\": [...]}\n",
        "    \"\"\"\n",
        "    \n",
        "    # Get the loss values of the results dictionary (training and test)\n",
        "    loss = results['train_loss']\n",
        "    test_loss = results['test_loss']\n",
        "\n",
        "    # Get the accuracy values of the results dictionary (training and test)\n",
        "    accuracy = results['train_acc']\n",
        "    test_accuracy = results['test_acc']\n",
        "\n",
        "    # Figure out how many epochs there were\n",
        "    epochs = range(len(results['train_loss']))\n",
        "\n",
        "    # Setup a plot \n",
        "    plt.figure(figsize=(15, 7))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, loss, label='train_loss')\n",
        "    plt.plot(epochs, test_loss, label='test_loss')\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, accuracy, label='train_accuracy')\n",
        "    plt.plot(epochs, test_accuracy, label='test_accuracy')\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend();"
      ],
      "metadata": {
        "id": "Q_7uM3VUed_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(model_0_results)"
      ],
      "metadata": {
        "id": "L3OyIJbjgyxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Model 1: TinyVGG with Data Augmentation"
      ],
      "metadata": {
        "id": "xZo4GRfPjWk_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9.1 Create transform with data augmentation"
      ],
      "metadata": {
        "id": "HWrqcTPIjanf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform_trivial_augment = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.TrivialAugmentWide(num_magnitude_bins=31),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([transforms.Resize((224,224)),\n",
        "                                      transforms.ToTensor()]) \n"
      ],
      "metadata": {
        "id": "I1lm1iSPg2dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9.2 Create train and test Dataset's and DataLoader's"
      ],
      "metadata": {
        "id": "NZCO_EI4ky8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_augmented = datasets.ImageFolder(train_dir,transform= train_transform_trivial_augment)\n",
        "test_data_simple = datasets.ImageFolder(test_dir,transform=test_transforms)\n",
        "train_data_augmented,test_data_simple"
      ],
      "metadata": {
        "id": "naq2UFpFkswo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn Datasets into DataLoader's\n",
        "import os\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "torch.manual_seed(42)\n",
        "train_dataloader_augmented = DataLoader(train_data_augmented, \n",
        "                                        batch_size=BATCH_SIZE, \n",
        "                                        shuffle=True,\n",
        "                                        num_workers=NUM_WORKERS)\n",
        "\n",
        "test_dataloader_simple = DataLoader(test_data_simple, \n",
        "                                    batch_size=BATCH_SIZE, \n",
        "                                    shuffle=False, \n",
        "                                    num_workers=NUM_WORKERS)\n",
        "\n",
        "train_dataloader_augmented, test_dataloader"
      ],
      "metadata": {
        "id": "fKGkOUSLYmmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9.3 Construct and train Model 1"
      ],
      "metadata": {
        "id": "4D-pea6SY0gI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "model_1 = DahwinTinyVGG(\n",
        "    input_shape=3,\n",
        "    hidden_units=10,\n",
        "    output_shape=len(train_data_augmented.classes)\n",
        ").to(device)\n",
        "model_1"
      ],
      "metadata": {
        "id": "qfgKo1WJYuoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "NUM_EPOCHS = 5\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model_1.parameters(),lr=0.001)\n",
        "s = time.time()\n",
        "model_1_resultes = train(model=model_1,\n",
        "                         train_dataloader=train_dataloader_augmented,\n",
        "                         test_dataloader=test_dataloader_simple,\n",
        "                         optimizer=optimizer,\n",
        "                         loss_fn=loss_fn,\n",
        "                         epochs=NUM_EPOCHS)\n",
        "e = time.time()\n",
        "print(e-s)"
      ],
      "metadata": {
        "id": "WlhOZV1VZT0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(model_1_resultes)"
      ],
      "metadata": {
        "id": "r4UCHkuG-b8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10. Compare model results**"
      ],
      "metadata": {
        "id": "2Uf0AYYtDf6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "model_0_df = pd.DataFrame(model_0_results)\n",
        "model_1_df = pd.DataFrame(model_1_resultes)\n",
        "model_0_df"
      ],
      "metadata": {
        "id": "gDTIWHtKDVFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup a plot \n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Get number of epochs\n",
        "epochs = range(len(model_0_df))\n",
        "\n",
        "# Plot train loss\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(epochs, model_0_df[\"train_loss\"], label=\"Model 0\")\n",
        "plt.plot(epochs, model_1_df[\"train_loss\"], label=\"Model 1\")\n",
        "plt.title(\"Train Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend()\n",
        "\n",
        "# Plot test loss\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(epochs, model_0_df[\"test_loss\"], label=\"Model 0\")\n",
        "plt.plot(epochs, model_1_df[\"test_loss\"], label=\"Model 1\")\n",
        "plt.title(\"Test Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend()\n",
        "\n",
        "# Plot train accuracy\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(epochs, model_0_df[\"train_acc\"], label=\"Model 0\")\n",
        "plt.plot(epochs, model_1_df[\"train_acc\"], label=\"Model 1\")\n",
        "plt.title(\"Train Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend()\n",
        "\n",
        "# Plot test accuracy\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.plot(epochs, model_0_df[\"test_acc\"], label=\"Model 0\")\n",
        "plt.plot(epochs, model_1_df[\"test_acc\"], label=\"Model 1\")\n",
        "plt.title(\"Test Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "4dOuPOp9Dkcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Make a prediction on a custom image"
      ],
      "metadata": {
        "id": "MeOQ0p0uD0z9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download custom image\n",
        "import requests\n",
        "\n",
        "# Setup custom image path\n",
        "custom_image_path = data_path / \"dahyun_pizza.png\"\n",
        "\n",
        "# Download the image if it doesn't already exist\n",
        "if not custom_image_path.is_file():\n",
        "    with open(custom_image_path, \"wb\") as f:\n",
        "        # When downloading from GitHub, need to use the \"raw\" file link\n",
        "        request = requests.get(\"https://thumbs.gfycat.com/AcademicCalmClam-mobile.jpg\")\n",
        "        print(f\"Downloading {custom_image_path}...\")\n",
        "        f.write(request.content)\n",
        "else:\n",
        "    print(f\"{custom_image_path} already exists, skipping download.\")\n"
      ],
      "metadata": {
        "id": "SaCkQqNUDwu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "custom_image_uint8 = torchvision.io.read_image(str(custom_image_path)).type(torch.float32)\n",
        "custom_image_uint8 = custom_image_uint8 / 255. \n",
        "# Print out image data\n",
        "print(f\"Custom image tensor:\\n{custom_image_uint8}\\n\")\n",
        "print(f\"Custom image shape: {custom_image_uint8.shape}\\n\")\n",
        "print(f\"Custom image dtype: {custom_image_uint8.dtype}\")"
      ],
      "metadata": {
        "id": "o5f-RMN5EHZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in custom image and convert the tensor values to float32\n",
        "custom_image = torchvision.io.read_image(str(custom_image_path)).type(torch.float32)\n",
        "\n",
        "# Divide the image pixel values by 255 to get them between [0, 1]\n",
        "custom_image = custom_image / 255. \n",
        "\n",
        "# Print out image data\n",
        "print(f\"Custom image tensor:\\n{custom_image}\\n\")\n",
        "print(f\"Custom image shape: {custom_image.shape}\\n\")\n",
        "print(f\"Custom image dtype: {custom_image.dtype}\")"
      ],
      "metadata": {
        "id": "XZQKTyJggznA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(custom_image.permute(1, 2, 0))"
      ],
      "metadata": {
        "id": "VsgGr_SkE4EV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_image_transform =  transforms.Compose([\n",
        "    transforms.Resize((224,224))\n",
        "])\n",
        "custom_image_transformed = custom_image_transform(custom_image)\n",
        "print(f\"Original shape:{custom_image.shape}\")\n",
        "print(f\"New shape:{custom_image_transformed.shape}\")"
      ],
      "metadata": {
        "id": "M4KWOJSRmNwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(custom_image_transformed.permute(1, 2, 0))"
      ],
      "metadata": {
        "id": "1mtgwlSQyzPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is error no batch size\n",
        "model_1.eval()\n",
        "with torch.inference_mode():\n",
        "  custom_image_pred = model_1(custom_image_transformed.to(device))"
      ],
      "metadata": {
        "id": "L0G7yOX2dGE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Note to make a predictions on a custom image we had to:\n",
        "* Load the images and turn it into a tensor\n",
        "* Make sure the image was the same datatype as the model\n",
        "* make sure the image was the same shape as the data the model was trained on (3,64,64) with a batch size (1,3,64,64)\n",
        "* Make sure the image was on the same device as our model "
      ],
      "metadata": {
        "id": "VNoZMdDVlpgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.eval()\n",
        "with torch.inference_mode():\n",
        "  custom_image_pred = model_1(custom_image_transformed.unsqueeze(0).to(device))\n",
        "custom_image_pred"
      ],
      "metadata": {
        "id": "bZkuYkrldrUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  print(f\"Custom image transformed shape: {custom_image_transformed.shape}\")\n",
        "  print(f\"Unsqueezed custom image shape: {custom_image_pred.shape}\")"
      ],
      "metadata": {
        "id": "RX7Pmqd0lUnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_image_pred.shape"
      ],
      "metadata": {
        "id": "-m2jFarEmtF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out prediction logits\n",
        "print(f\"Prediction logits: {custom_image_pred}\")\n",
        "\n",
        "# Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
        "custom_image_pred_probs = torch.softmax(custom_image_pred, dim=1)\n",
        "print(f\"Prediction probabilities: {custom_image_pred_probs}\")\n",
        "\n",
        "# Convert prediction probabilities -> prediction labels\n",
        "custom_image_pred_label = torch.argmax(custom_image_pred_probs, dim=1)\n",
        "print(f\"Prediction label: {custom_image_pred_label}\")"
      ],
      "metadata": {
        "id": "ahz7CMQjolsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the predicted label\n",
        "custom_image_pred_class = class_names[custom_image_pred_label.cpu()] # put pred label to CPU, otherwise will error\n",
        "custom_image_pred_class"
      ],
      "metadata": {
        "id": "4867L3QypOWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The values of the prediction probabilities are quite similar\n",
        "custom_image_pred_probs"
      ],
      "metadata": {
        "id": "CK7WWce8pTFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11.3 Putting custom image prediction together: building a function"
      ],
      "metadata": {
        "id": "6zamJU7Npaop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pred_and_plot_image(model: torch.nn.Module, \n",
        "                        image_path: str, \n",
        "                        class_names: List[str] = None, \n",
        "                        transform=None,\n",
        "                        device: torch.device = device):\n",
        "    \"\"\"Makes a prediction on a target image and plots the image with its prediction.\"\"\"\n",
        "    \n",
        "    # 1. Load in image and convert the tensor values to float32\n",
        "    target_image = torchvision.io.read_image(str(image_path)).type(torch.float32)\n",
        "    \n",
        "    # 2. Divide the image pixel values by 255 to get them between [0, 1]\n",
        "    target_image = target_image / 255. \n",
        "    \n",
        "    # 3. Transform if necessary\n",
        "    if transform:\n",
        "        target_image = transform(target_image)\n",
        "    \n",
        "    # 4. Make sure the model is on the target device\n",
        "    model.to(device)\n",
        "    \n",
        "    # 5. Turn on model evaluation mode and inference mode\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        # Add an extra dimension to the image\n",
        "        target_image = target_image.unsqueeze(dim=0)\n",
        "    \n",
        "        # Make a prediction on image with an extra dimension and send it to the target device\n",
        "        target_image_pred = model(target_image.to(device))\n",
        "        \n",
        "    # 6. Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
        "    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n",
        "\n",
        "    # 7. Convert prediction probabilities -> prediction labels\n",
        "    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n",
        "    \n",
        "    # 8. Plot the image alongside the prediction and prediction probability\n",
        "    plt.imshow(target_image.squeeze().permute(1, 2, 0)) # make sure it's the right size for matplotlib\n",
        "    if class_names:\n",
        "        title = f\"Pred: {class_names[target_image_pred_label.cpu()]} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n",
        "    else: \n",
        "        title = f\"Pred: {target_image_pred_label} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n",
        "    plt.title(title)\n",
        "    plt.axis(False);\n"
      ],
      "metadata": {
        "id": "px8CxNL2pW27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pred on our custom image\n",
        "pred_and_plot_image(model=model_1,\n",
        "                    image_path=custom_image_path,\n",
        "                    class_names=class_names,\n",
        "                    transform=custom_image_transform,\n",
        "                    device=device)"
      ],
      "metadata": {
        "id": "7WjtYpA6pgUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YkkubSGVpjuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hz6Cfjbhqo15"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}